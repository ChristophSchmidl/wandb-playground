{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ab80abe",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3d953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import wandb\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb5073b0",
   "metadata": {},
   "source": [
    "# Set up Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6ebc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcschmidl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "179ff594",
   "metadata": {},
   "source": [
    "# Training function for single environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeff40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_single_env(config):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a gym environment\n",
    "    env_name = \"CartPole-v1\"\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Wrap the environment in a VecEnv\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "    # Set up evaluation environment\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "    # configure the PPo agent\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env, \n",
    "        verbose=1,\n",
    "        n_steps=config[\"n_steps\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        ent_coef=config[\"ent_coef\"],\n",
    "        clip_range=config[\"clip_range\"],\n",
    "        n_epochs=config[\"n_epochs\"],\n",
    "        gae_lambda=config[\"gae_lambda\"],\n",
    "        max_grad_norm=config[\"max_grad_norm\"]\n",
    "    )\n",
    "\n",
    "    # Set up an evaluation callback\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=\"./models/\",\n",
    "        log_path=\"./logs/\",\n",
    "        eval_freq=500,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    # Train the PPo agent\n",
    "    model.learn(total_timesteps=config[\"total_timesteps\"], callback=eval_callback)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Single environment training took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Evaluate the trained agent\n",
    "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e649342e",
   "metadata": {},
   "source": [
    "# Training function for multi environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe73d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed):\n",
    "    def _init():\n",
    "        env = gym.make(env_id)\n",
    "        env.seed(seed)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da66c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2848719473  263985200 2543817249 3150862783]\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "seeds = np.random.randint(0, 2**32 - 1, 4)\n",
    "print(seeds)\n",
    "\n",
    "for seed in seeds:\n",
    "    print(type(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4511ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent_multi_env(config, n_envs=4):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a gym environment\n",
    "    env_name = \"CartPole-v1\"\n",
    "    n_envs = n_envs,\n",
    "    #seeds = np.random.randint(0, int(1e6), n_envs)\n",
    "    seeds = np.random.randint(0, 2**32 - 1, n_envs)\n",
    "\n",
    "    envs = [make_env(env_name, int(seed)) for seed in seeds]\n",
    "    env = SubprocVecEnv(envs)\n",
    "\n",
    "    # Set up evaluation environment\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "    # Configure the PPO agent\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        n_steps=config[\"n_steps\"],\n",
    "        gamma=config[\"gamma\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        ent_coef=config[\"ent_coef\"],\n",
    "        clip_range=config[\"clip_range\"],\n",
    "        n_epochs=config[\"n_epochs\"],\n",
    "        gae_lambda=config[\"gae_lambda\"],\n",
    "        max_grad_norm=config[\"max_grad_norm\"],\n",
    "    )\n",
    "\n",
    "    # Set up an evaluation callback\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env, \n",
    "        best_model_save_path=\"./models/\",\n",
    "        log_path=\"./logs/\",\n",
    "        eval_freq=500,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    # Train the PPO agent\n",
    "    model.learn(total_timesteps=config[\"total_timesteps\"], callback=eval_callback)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Multi-environment training took {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Evaluate the trained agent\n",
    "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c7cf98c",
   "metadata": {},
   "source": [
    "# Set up Weights & Biases sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0405f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: a390ceqc\n",
      "Sweep URL: https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/a390ceqc\n",
      "Create sweep with ID: b30md251\n",
      "Sweep URL: https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251\n"
     ]
    }
   ],
   "source": [
    "sweep_config_single = {\n",
    "    \"name\": \"ppo_hyperparameter_tuning_single\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval/mean_reward\"},\n",
    "    \"parameters\": {\n",
    "        \"n_steps\": {\"min\": 64, \"max\": 2048, \"distribution\": \"int_uniform\"},\n",
    "        \"gamma\": {\"min\": 0.9, \"max\": 0.999, \"distribution\": \"uniform\"},\n",
    "        \"learning_rate\": {\"min\": 1e-5, \"max\": 1e-2, \"distribution\": \"uniform\"},\n",
    "        \"ent_coef\": {\"min\": 1e-6, \"max\": 1e-2, \"distribution\": \"uniform\"},\n",
    "        \"clip_range\": {\"min\": 0.1, \"max\": 0.3, \"distribution\": \"uniform\"},\n",
    "        \"n_epochs\": {\"min\": 1, \"max\": 10, \"distribution\": \"int_uniform\"},\n",
    "        \"gae_lambda\": {\"min\": 0.9, \"max\": 1.0, \"distribution\": \"uniform\"},\n",
    "        \"max_grad_norm\": {\"min\": 0.1, \"max\": 10, \"distribution\": \"uniform\"},\n",
    "        \"total_timesteps\": {\"min\": 10_000, \"max\": 100_000, \"distribution\": \"int_uniform\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config_multi = {\n",
    "    \"name\": \"ppo_hyperparameter_tuning_multi\",\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"eval/mean_reward\"},\n",
    "    \"parameters\": {\n",
    "        \"n_steps\": {\"min\": 64, \"max\": 2048, \"distribution\": \"int_uniform\"},\n",
    "        \"gamma\": {\"min\": 0.9, \"max\": 0.999, \"distribution\": \"uniform\"},\n",
    "        \"learning_rate\": {\"min\": 1e-5, \"max\": 1e-2, \"distribution\": \"uniform\"},\n",
    "        \"ent_coef\": {\"min\": 1e-6, \"max\": 1e-2, \"distribution\": \"uniform\"},\n",
    "        \"clip_range\": {\"min\": 0.1, \"max\": 0.3, \"distribution\": \"uniform\"},\n",
    "        \"n_epochs\": {\"min\": 1, \"max\": 10, \"distribution\": \"int_uniform\"},\n",
    "        \"gae_lambda\": {\"min\": 0.9, \"max\": 1.0, \"distribution\": \"uniform\"},\n",
    "        \"max_grad_norm\": {\"min\": 0.1, \"max\": 10, \"distribution\": \"uniform\"},\n",
    "        \"total_timesteps\": {\"min\": 10_000, \"max\": 100_000, \"distribution\": \"int_uniform\"}\n",
    "    },\n",
    "}\n",
    "\n",
    "single_sweep_id = wandb.sweep(sweep_config_single, project=\"ppo_hyperparamter_tuning\")\n",
    "multi_sweep_id = wandb.sweep(sweep_config_multi, project=\"ppo_hyperparamter_tuning\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b45da780",
   "metadata": {},
   "source": [
    "# Define the sweep function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92b07aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_agent_single():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "        mean_reward = train_agent_single_env(config)\n",
    "        wandb.log({\"eval/mean_reward\": mean_reward})\n",
    "\n",
    "def sweep_agent_multi():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "        mean_reward = train_agent_multi_env(config, n_envs=4)\n",
    "        wandb.log({\"eval/mean_reward\": mean_reward})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38497831",
   "metadata": {},
   "source": [
    "# Run the sweep: Single env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd12e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ec1zlvay with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.24801498317259676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.005259804550102741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9872416499332052\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9187271553667176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004104321568970994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 1.96005489016267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 535\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 24494\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_131529-ec1zlvay</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ec1zlvay' target=\"_blank\">devout-sweep-1</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ec1zlvay' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ec1zlvay</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 535`, after every 8 untruncated mini-batches, there will be a truncated mini-batch of size 23\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=535 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=9.40 +/- 0.80\n",
      "Episode length: 9.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1336 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 535  |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=158.00 +/- 171.13\n",
      "Episode length: 158.00 +/- 171.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 158         |\n",
      "|    mean_reward          | 158         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016432984 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.675      |\n",
      "|    explained_variance   | -0.0104     |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 14.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 759  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1070 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=245.20 +/- 67.81\n",
      "Episode length: 245.20 +/- 67.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 245         |\n",
      "|    mean_reward          | 245         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025983727 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | -0.0461     |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 4.77        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 527  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 1605 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=315.60 +/- 67.95\n",
      "Episode length: 315.60 +/- 67.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | 316         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019843854 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 498  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2140 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=311.20 +/- 86.32\n",
      "Episode length: 311.20 +/- 86.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 311        |\n",
      "|    mean_reward          | 311        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06653693 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.248      |\n",
      "|    entropy_loss         | -0.534     |\n",
      "|    explained_variance   | 0.347      |\n",
      "|    learning_rate        | 0.0041     |\n",
      "|    loss                 | 0.372      |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    value_loss           | 1.66       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 495  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2675 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=461.40 +/- 77.20\n",
      "Episode length: 461.40 +/- 77.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 461         |\n",
      "|    mean_reward          | 461         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012874671 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0424      |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | -0.00942    |\n",
      "|    value_loss           | 0.0507      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 472  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 3210 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=472.40 +/- 55.20\n",
      "Episode length: 472.40 +/- 55.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 472         |\n",
      "|    mean_reward          | 472         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010514552 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | -0.00661    |\n",
      "|    value_loss           | 0.583       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 447  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 3745 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=422.40 +/- 98.85\n",
      "Episode length: 422.40 +/- 98.85\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 422        |\n",
      "|    mean_reward          | 422        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00957346 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.248      |\n",
      "|    entropy_loss         | -0.477     |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 0.0041     |\n",
      "|    loss                 | 0.158      |\n",
      "|    n_updates            | 63         |\n",
      "|    policy_gradient_loss | -0.0076    |\n",
      "|    value_loss           | 0.236      |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 436  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4280 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=436.00 +/- 78.64\n",
      "Episode length: 436.00 +/- 78.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 436         |\n",
      "|    mean_reward          | 436         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008450892 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    value_loss           | 0.0964      |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 427  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 4815 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=472.00 +/- 56.00\n",
      "Episode length: 472.00 +/- 56.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 472         |\n",
      "|    mean_reward          | 472         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009895691 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.0196     |\n",
      "|    n_updates            | 81          |\n",
      "|    policy_gradient_loss | -0.00826    |\n",
      "|    value_loss           | 0.0364      |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 418  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 5350 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=456.60 +/- 55.73\n",
      "Episode length: 456.60 +/- 55.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 457         |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012173773 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.00101     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00779     |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 414  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 5885 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=424.40 +/- 92.81\n",
      "Episode length: 424.40 +/- 92.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 424         |\n",
      "|    mean_reward          | 424         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035678554 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 99          |\n",
      "|    policy_gradient_loss | 0.00759     |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 413  |\n",
      "|    iterations      | 12   |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 6420 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=432.00 +/- 55.87\n",
      "Episode length: 432.00 +/- 55.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 432         |\n",
      "|    mean_reward          | 432         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011821361 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.000663    |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 413  |\n",
      "|    iterations      | 13   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 6955 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=415.00 +/- 104.12\n",
      "Episode length: 415.00 +/- 104.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 415         |\n",
      "|    mean_reward          | 415         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008997078 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 117         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 3.36        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 413  |\n",
      "|    iterations      | 14   |\n",
      "|    time_elapsed    | 18   |\n",
      "|    total_timesteps | 7490 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=421.00 +/- 97.62\n",
      "Episode length: 421.00 +/- 97.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 421         |\n",
      "|    mean_reward          | 421         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011481612 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.412       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.907       |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.55        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=497.00 +/- 6.00\n",
      "Episode length: 497.00 +/- 6.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 497      |\n",
      "|    mean_reward     | 497      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 391  |\n",
      "|    iterations      | 15   |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 8025 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011382087 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.00769    |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 389  |\n",
      "|    iterations      | 16   |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 8560 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=458.60 +/- 82.80\n",
      "Episode length: 458.60 +/- 82.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 459         |\n",
      "|    mean_reward          | 459         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008250286 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.00807    |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | 0.00621     |\n",
      "|    value_loss           | 0.0532      |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 389  |\n",
      "|    iterations      | 17   |\n",
      "|    time_elapsed    | 23   |\n",
      "|    total_timesteps | 9095 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=345.40 +/- 79.26\n",
      "Episode length: 345.40 +/- 79.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 345          |\n",
      "|    mean_reward          | 345          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083181225 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 153          |\n",
      "|    policy_gradient_loss | -0.000715    |\n",
      "|    value_loss           | 0.0598       |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 391  |\n",
      "|    iterations      | 18   |\n",
      "|    time_elapsed    | 24   |\n",
      "|    total_timesteps | 9630 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=436.20 +/- 80.80\n",
      "Episode length: 436.20 +/- 80.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 436         |\n",
      "|    mean_reward          | 436         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014564384 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 162         |\n",
      "|    policy_gradient_loss | 0.00185     |\n",
      "|    value_loss           | 0.0681      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 391   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 10165 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=463.20 +/- 72.11\n",
      "Episode length: 463.20 +/- 72.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 463         |\n",
      "|    mean_reward          | 463         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017099198 |\n",
      "|    clip_fraction        | 0.0837      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.4        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0162      |\n",
      "|    n_updates            | 171         |\n",
      "|    policy_gradient_loss | -0.00726    |\n",
      "|    value_loss           | 0.0538      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 392   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 10700 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=398.00 +/- 127.97\n",
      "Episode length: 398.00 +/- 127.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 398          |\n",
      "|    mean_reward          | 398          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049854647 |\n",
      "|    clip_fraction        | 0.0516       |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | -0.0309      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    value_loss           | 0.0101       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 392   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 11235 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=378.80 +/- 107.12\n",
      "Episode length: 378.80 +/- 107.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 379         |\n",
      "|    mean_reward          | 379         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012614396 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.0279     |\n",
      "|    n_updates            | 189         |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    value_loss           | 0.0021      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 391   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 11770 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=308.60 +/- 126.10\n",
      "Episode length: 308.60 +/- 126.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 309          |\n",
      "|    mean_reward          | 309          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049784034 |\n",
      "|    clip_fraction        | 0.0517       |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | 0.0797       |\n",
      "|    n_updates            | 198          |\n",
      "|    policy_gradient_loss | 0.00215      |\n",
      "|    value_loss           | 0.0593       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 394   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 12305 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=451.00 +/- 98.00\n",
      "Episode length: 451.00 +/- 98.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 451         |\n",
      "|    mean_reward          | 451         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004034535 |\n",
      "|    clip_fraction        | 0.0431      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.00411     |\n",
      "|    n_updates            | 207         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 0.0627      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 394   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 12840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=438.40 +/- 53.91\n",
      "Episode length: 438.40 +/- 53.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 438         |\n",
      "|    mean_reward          | 438         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006980116 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0135      |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.00351    |\n",
      "|    value_loss           | 0.0242      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 393   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 13375 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=435.00 +/- 86.01\n",
      "Episode length: 435.00 +/- 86.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 435         |\n",
      "|    mean_reward          | 435         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008819493 |\n",
      "|    clip_fraction        | 0.068       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.025      |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 0.000464    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 392   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 13910 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=433.60 +/- 112.10\n",
      "Episode length: 433.60 +/- 112.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 434         |\n",
      "|    mean_reward          | 434         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024783798 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 234         |\n",
      "|    policy_gradient_loss | -0.00311    |\n",
      "|    value_loss           | 0.0938      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 391   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 14445 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=452.60 +/- 94.80\n",
      "Episode length: 452.60 +/- 94.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 453        |\n",
      "|    mean_reward          | 453        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01291788 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.248      |\n",
      "|    entropy_loss         | -0.457     |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0041     |\n",
      "|    loss                 | -0.00592   |\n",
      "|    n_updates            | 243        |\n",
      "|    policy_gradient_loss | 0.0022     |\n",
      "|    value_loss           | 0.0506     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 387   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 14980 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=333.00 +/- 160.02\n",
      "Episode length: 333.00 +/- 160.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 333         |\n",
      "|    mean_reward          | 333         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009216427 |\n",
      "|    clip_fraction        | 0.0886      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.00648    |\n",
      "|    n_updates            | 252         |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    value_loss           | 0.043       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=362.20 +/- 168.77\n",
      "Episode length: 362.20 +/- 168.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 362      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 381   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 15515 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=189.80 +/- 133.79\n",
      "Episode length: 189.80 +/- 133.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 190         |\n",
      "|    mean_reward          | 190         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015970994 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 261         |\n",
      "|    policy_gradient_loss | -0.000562   |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 385   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 16050 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=128.40 +/- 11.53\n",
      "Episode length: 128.40 +/- 11.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 128         |\n",
      "|    mean_reward          | 128         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013145756 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.19        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00605    |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 390   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 16585 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=211.40 +/- 145.73\n",
      "Episode length: 211.40 +/- 145.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 211         |\n",
      "|    mean_reward          | 211         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015089755 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 279         |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 0.607       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 393   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 17120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=144.00 +/- 23.05\n",
      "Episode length: 144.00 +/- 23.05\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 144        |\n",
      "|    mean_reward          | 144        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03565119 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.248      |\n",
      "|    entropy_loss         | -0.398     |\n",
      "|    explained_variance   | 0.498      |\n",
      "|    learning_rate        | 0.0041     |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 288        |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    value_loss           | 0.838      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 397   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 17655 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=192.60 +/- 79.37\n",
      "Episode length: 192.60 +/- 79.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 193          |\n",
      "|    mean_reward          | 193          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074778474 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | -0.0182      |\n",
      "|    n_updates            | 297          |\n",
      "|    policy_gradient_loss | 0.00198      |\n",
      "|    value_loss           | 0.379        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 398   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 18190 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=136.20 +/- 6.01\n",
      "Episode length: 136.20 +/- 6.01\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 136          |\n",
      "|    mean_reward          | 136          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063799433 |\n",
      "|    clip_fraction        | 0.0956       |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | -0.0381      |\n",
      "|    n_updates            | 306          |\n",
      "|    policy_gradient_loss | -0.0159      |\n",
      "|    value_loss           | 0.0522       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 401   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 18725 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=224.80 +/- 139.27\n",
      "Episode length: 224.80 +/- 139.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 225          |\n",
      "|    mean_reward          | 225          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114186965 |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.409       |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | -0.0203      |\n",
      "|    n_updates            | 315          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    value_loss           | 0.0331       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 404   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 19260 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=305.60 +/- 158.76\n",
      "Episode length: 305.60 +/- 158.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 306         |\n",
      "|    mean_reward          | 306         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025704337 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 324         |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    value_loss           | 0.0597      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 405   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 19795 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=225.60 +/- 91.55\n",
      "Episode length: 225.60 +/- 91.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 226          |\n",
      "|    mean_reward          | 226          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027162724 |\n",
      "|    clip_fraction        | 0.0303       |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | -0.00501     |\n",
      "|    n_updates            | 333          |\n",
      "|    policy_gradient_loss | -0.000969    |\n",
      "|    value_loss           | 0.0304       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 405   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 20330 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=303.40 +/- 160.59\n",
      "Episode length: 303.40 +/- 160.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 303          |\n",
      "|    mean_reward          | 303          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0100275045 |\n",
      "|    clip_fraction        | 0.0896       |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | 0.0101       |\n",
      "|    n_updates            | 342          |\n",
      "|    policy_gradient_loss | 0.00786      |\n",
      "|    value_loss           | 0.0337       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 20865 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=262.60 +/- 119.36\n",
      "Episode length: 262.60 +/- 119.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 263         |\n",
      "|    mean_reward          | 263         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006552395 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 351         |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    value_loss           | 0.0438      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 409   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 21400 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=261.60 +/- 121.79\n",
      "Episode length: 261.60 +/- 121.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 262        |\n",
      "|    mean_reward          | 262        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 21500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01636833 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.248      |\n",
      "|    entropy_loss         | -0.28      |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0041     |\n",
      "|    loss                 | 0.000193   |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | 0.00408    |\n",
      "|    value_loss           | 0.00798    |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 409   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 53    |\n",
      "|    total_timesteps | 21935 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=303.40 +/- 152.37\n",
      "Episode length: 303.40 +/- 152.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 303         |\n",
      "|    mean_reward          | 303         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024317922 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.243      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 369         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 410   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 22470 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=394.80 +/- 130.14\n",
      "Episode length: 394.80 +/- 130.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 395         |\n",
      "|    mean_reward          | 395         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015156938 |\n",
      "|    clip_fraction        | 0.0794      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0441      |\n",
      "|    n_updates            | 378         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 0.0305      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=286.20 +/- 112.97\n",
      "Episode length: 286.20 +/- 112.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 286      |\n",
      "|    mean_reward     | 286      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 406   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 23005 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=284.60 +/- 111.95\n",
      "Episode length: 284.60 +/- 111.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 285          |\n",
      "|    mean_reward          | 285          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012435033 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.248        |\n",
      "|    entropy_loss         | -0.26        |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0041       |\n",
      "|    loss                 | 0.0168       |\n",
      "|    n_updates            | 387          |\n",
      "|    policy_gradient_loss | 0.00563      |\n",
      "|    value_loss           | 0.0518       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 408   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 23540 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=327.60 +/- 142.63\n",
      "Episode length: 327.60 +/- 142.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 328         |\n",
      "|    mean_reward          | 328         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020104028 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.058       |\n",
      "|    n_updates            | 396         |\n",
      "|    policy_gradient_loss | -0.00162    |\n",
      "|    value_loss           | 0.0376      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 409   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 24075 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=212.60 +/- 20.57\n",
      "Episode length: 212.60 +/- 20.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 213         |\n",
      "|    mean_reward          | 213         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010635425 |\n",
      "|    clip_fraction        | 0.0676      |\n",
      "|    clip_range           | 0.248       |\n",
      "|    entropy_loss         | -0.36       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0041      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 405         |\n",
      "|    policy_gradient_loss | -0.0078     |\n",
      "|    value_loss           | 0.0372      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 412   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 24610 |\n",
      "------------------------------\n",
      "Single environment training took 59.91 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>405.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-sweep-1</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ec1zlvay' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ec1zlvay</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_131529-ec1zlvay/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ltmd9ibv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.13868505930966096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.004785396253998942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9424297839457756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9926226867607312\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001385788292958073\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 6.710536429049587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 49397\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_131644-ltmd9ibv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ltmd9ibv' target=\"_blank\">fiery-sweep-2</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ltmd9ibv' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ltmd9ibv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1218`, after every 19 untruncated mini-batches, there will be a truncated mini-batch of size 2\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1218 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=93.20 +/- 15.32\n",
      "Episode length: 93.20 +/- 15.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 93.2     |\n",
      "|    mean_reward     | 93.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=96.00 +/- 9.90\n",
      "Episode length: 96.00 +/- 9.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 96       |\n",
      "|    mean_reward     | 96       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 970  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1218 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=164.60 +/- 117.23\n",
      "Episode length: 164.60 +/- 117.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 165          |\n",
      "|    mean_reward          | 165          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049763797 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.689       |\n",
      "|    explained_variance   | 0.00916      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 9.44         |\n",
      "|    n_updates            | 5            |\n",
      "|    policy_gradient_loss | -0.00847     |\n",
      "|    value_loss           | 42.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=119.00 +/- 44.43\n",
      "Episode length: 119.00 +/- 44.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 119      |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 805  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2436 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=75.20 +/- 8.28\n",
      "Episode length: 75.20 +/- 8.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 75.2        |\n",
      "|    mean_reward          | 75.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007655546 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 27.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=105.60 +/- 25.62\n",
      "Episode length: 105.60 +/- 25.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 106      |\n",
      "|    mean_reward     | 106      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=93.80 +/- 19.51\n",
      "Episode length: 93.80 +/- 19.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 93.8     |\n",
      "|    mean_reward     | 93.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 811  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 3654 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=191.20 +/- 154.62\n",
      "Episode length: 191.20 +/- 154.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 191          |\n",
      "|    mean_reward          | 191          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067278324 |\n",
      "|    clip_fraction        | 0.181        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | 0.264        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 1.51         |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.0183      |\n",
      "|    value_loss           | 44.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=202.80 +/- 126.58\n",
      "Episode length: 202.80 +/- 126.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 203      |\n",
      "|    mean_reward     | 203      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 795  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 4872 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=182.20 +/- 34.86\n",
      "Episode length: 182.20 +/- 34.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 182          |\n",
      "|    mean_reward          | 182          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072707324 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.631       |\n",
      "|    explained_variance   | 0.407        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 12.8         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0164      |\n",
      "|    value_loss           | 54.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=234.80 +/- 149.48\n",
      "Episode length: 234.80 +/- 149.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 235      |\n",
      "|    mean_reward     | 235      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=291.20 +/- 120.77\n",
      "Episode length: 291.20 +/- 120.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 291      |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 721  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 6090 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=266.60 +/- 109.31\n",
      "Episode length: 266.60 +/- 109.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 267          |\n",
      "|    mean_reward          | 267          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051005734 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.61        |\n",
      "|    explained_variance   | 0.549        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 37.6         |\n",
      "|    n_updates            | 25           |\n",
      "|    policy_gradient_loss | -0.0171      |\n",
      "|    value_loss           | 59.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=273.20 +/- 125.90\n",
      "Episode length: 273.20 +/- 125.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 273      |\n",
      "|    mean_reward     | 273      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 696  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 7308 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=404.40 +/- 69.38\n",
      "Episode length: 404.40 +/- 69.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 404          |\n",
      "|    mean_reward          | 404          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065083476 |\n",
      "|    clip_fraction        | 0.207        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.592       |\n",
      "|    explained_variance   | 0.623        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 24           |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0165      |\n",
      "|    value_loss           | 56.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=354.80 +/- 87.72\n",
      "Episode length: 354.80 +/- 87.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 355      |\n",
      "|    mean_reward     | 355      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=357.60 +/- 114.38\n",
      "Episode length: 357.60 +/- 114.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 358      |\n",
      "|    mean_reward     | 358      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 614  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 8526 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=379.00 +/- 90.81\n",
      "Episode length: 379.00 +/- 90.81\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 379        |\n",
      "|    mean_reward          | 379        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00486954 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.139      |\n",
      "|    entropy_loss         | -0.6       |\n",
      "|    explained_variance   | 0.776      |\n",
      "|    learning_rate        | 0.00139    |\n",
      "|    loss                 | 0.156      |\n",
      "|    n_updates            | 35         |\n",
      "|    policy_gradient_loss | -0.0092    |\n",
      "|    value_loss           | 32.6       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=370.80 +/- 70.20\n",
      "Episode length: 370.80 +/- 70.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 371      |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 582  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 9744 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=281.80 +/- 30.80\n",
      "Episode length: 281.80 +/- 30.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 282          |\n",
      "|    mean_reward          | 282          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049525434 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.581       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 16.4         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00698     |\n",
      "|    value_loss           | 45.8         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=397.40 +/- 61.56\n",
      "Episode length: 397.40 +/- 61.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 397      |\n",
      "|    mean_reward     | 397      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 564   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 10962 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=476.40 +/- 32.08\n",
      "Episode length: 476.40 +/- 32.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 476          |\n",
      "|    mean_reward          | 476          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066429945 |\n",
      "|    clip_fraction        | 0.175        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.588       |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 1.12         |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | -0.0146      |\n",
      "|    value_loss           | 23.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11500, episode_reward=443.40 +/- 60.18\n",
      "Episode length: 443.40 +/- 60.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 443      |\n",
      "|    mean_reward     | 443      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=443.20 +/- 71.07\n",
      "Episode length: 443.20 +/- 71.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 443      |\n",
      "|    mean_reward     | 443      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 519   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 12180 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=487.80 +/- 17.21\n",
      "Episode length: 487.80 +/- 17.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 488          |\n",
      "|    mean_reward          | 488          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073982524 |\n",
      "|    clip_fraction        | 0.146        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.591       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 223          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 32.9         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 13398 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011799912 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 1.47        |\n",
      "|    n_updates            | 55          |\n",
      "|    policy_gradient_loss | -0.00937    |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 471   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 14616 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007629019 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 1.94        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 8.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 467   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 15834 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066015506 |\n",
      "|    clip_fraction        | 0.0998       |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.544       |\n",
      "|    explained_variance   | -0.0193      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.00302     |\n",
      "|    n_updates            | 65           |\n",
      "|    policy_gradient_loss | -0.000648    |\n",
      "|    value_loss           | 6.26         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 447   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 17052 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063917884 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 0.327        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.0854       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00395     |\n",
      "|    value_loss           | 4.8          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 442   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 18270 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002080993 |\n",
      "|    clip_fraction        | 0.0978      |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.00171     |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.0696     |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 3.61        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 439   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 19488 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00261461 |\n",
      "|    clip_fraction        | 0.0847     |\n",
      "|    clip_range           | 0.139      |\n",
      "|    entropy_loss         | -0.544     |\n",
      "|    explained_variance   | -0.00201   |\n",
      "|    learning_rate        | 0.00139    |\n",
      "|    loss                 | 0.262      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.000491  |\n",
      "|    value_loss           | 2.81       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 427   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 20706 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002968076 |\n",
      "|    clip_fraction        | 0.077       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.00418     |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.108       |\n",
      "|    n_updates            | 85          |\n",
      "|    policy_gradient_loss | 0.00222     |\n",
      "|    value_loss           | 2.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 427   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 21924 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003610329 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.00381     |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    value_loss           | 1.61        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 417   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 23142 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044573946 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.54        |\n",
      "|    explained_variance   | 0.369        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.0133      |\n",
      "|    n_updates            | 95           |\n",
      "|    policy_gradient_loss | -0.0063      |\n",
      "|    value_loss           | 1.17         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 417   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 24360 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004905465 |\n",
      "|    clip_fraction        | 0.0892      |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000612   |\n",
      "|    value_loss           | 22.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 405   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 25578 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020079066 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.534       |\n",
      "|    explained_variance   | 0.606        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.0255       |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | -0.00368     |\n",
      "|    value_loss           | 1.13         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 406   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 26796 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00403164 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.139      |\n",
      "|    entropy_loss         | -0.508     |\n",
      "|    explained_variance   | 0.376      |\n",
      "|    learning_rate        | 0.00139    |\n",
      "|    loss                 | 0.0668     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.000468  |\n",
      "|    value_loss           | 2.75       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 391   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 28014 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005308129 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.509      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.0487      |\n",
      "|    n_updates            | 115         |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    value_loss           | 0.356       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 387   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 29232 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011858344 |\n",
      "|    clip_fraction        | 0.0706       |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.523       |\n",
      "|    explained_variance   | 0.00762      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.0498       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.00178      |\n",
      "|    value_loss           | 0.261        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 388   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 30450 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002052941 |\n",
      "|    clip_fraction        | 0.0828      |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.0216      |\n",
      "|    n_updates            | 125         |\n",
      "|    policy_gradient_loss | 0.00128     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 385   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 31668 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021292465 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | -0.0625      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.0152       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | 0.00259      |\n",
      "|    value_loss           | 0.149        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 386   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 32886 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020203837 |\n",
      "|    clip_fraction        | 0.0808       |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | -0.0298      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.00645      |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | -0.000374    |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 383   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 88    |\n",
      "|    total_timesteps | 34104 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005030885 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.52        |\n",
      "|    explained_variance   | -0.0182      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.0136      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | 0.00169      |\n",
      "|    value_loss           | 0.0851       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 382   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 92    |\n",
      "|    total_timesteps | 35322 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051977355 |\n",
      "|    clip_fraction        | 0.163        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | -0.0265      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.00213     |\n",
      "|    n_updates            | 145          |\n",
      "|    policy_gradient_loss | -0.00356     |\n",
      "|    value_loss           | 0.0666       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 378   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 96    |\n",
      "|    total_timesteps | 36540 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010125034 |\n",
      "|    clip_fraction        | 0.0706       |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.501       |\n",
      "|    explained_variance   | 0.0149       |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.0306      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | 2.41e-05     |\n",
      "|    value_loss           | 0.0539       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 380   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 37758 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006005437 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.336       |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 155         |\n",
      "|    policy_gradient_loss | -0.00427    |\n",
      "|    value_loss           | 0.0405      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 379   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 102   |\n",
      "|    total_timesteps | 38976 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046737147 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | -0.017       |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | 0.0615       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    value_loss           | 0.0334       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 374   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 107   |\n",
      "|    total_timesteps | 40194 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030980934 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.47        |\n",
      "|    explained_variance   | 0.141        |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.0431      |\n",
      "|    n_updates            | 165          |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    value_loss           | 0.0253       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 374   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 110   |\n",
      "|    total_timesteps | 41412 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006440596 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | -0.0182     |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.000128    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 1.15e-06    |\n",
      "|    value_loss           | 0.0203      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 371   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 114   |\n",
      "|    total_timesteps | 42630 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025387746 |\n",
      "|    clip_fraction        | 0.0775       |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | -0.0131      |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.00153     |\n",
      "|    n_updates            | 175          |\n",
      "|    policy_gradient_loss | 0.000959     |\n",
      "|    value_loss           | 0.0157       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 371   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 117   |\n",
      "|    total_timesteps | 43848 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001843282 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | -0.000837   |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.0168      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.000467   |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 371   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 121   |\n",
      "|    total_timesteps | 45066 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002580798 |\n",
      "|    clip_fraction        | 0.0802      |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 185         |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 0.00937     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 373   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 124   |\n",
      "|    total_timesteps | 46284 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008026784 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.00805     |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | -0.0613     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.000268    |\n",
      "|    value_loss           | 0.00738     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 372   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 127   |\n",
      "|    total_timesteps | 47502 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004208791 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.139       |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | -0.0152     |\n",
      "|    learning_rate        | 0.00139     |\n",
      "|    loss                 | 0.0442      |\n",
      "|    n_updates            | 195         |\n",
      "|    policy_gradient_loss | 0.00164     |\n",
      "|    value_loss           | 0.00607     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 374   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 130   |\n",
      "|    total_timesteps | 48720 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025582446 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.139        |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.0536       |\n",
      "|    learning_rate        | 0.00139      |\n",
      "|    loss                 | -0.00825     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000815    |\n",
      "|    value_loss           | 0.00482      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 375   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 132   |\n",
      "|    total_timesteps | 49938 |\n",
      "------------------------------\n",
      "Single environment training took 133.13 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-sweep-2</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ltmd9ibv' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ltmd9ibv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_131644-ltmd9ibv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: maf5t0pd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.1762925903999336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.009679510913082995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9375607636666896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9701260016674802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004246935904103854\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 1.9158946434987156\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 460\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 44797\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_131913-maf5t0pd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/maf5t0pd' target=\"_blank\">sleek-sweep-3</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/maf5t0pd' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/maf5t0pd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 460`, after every 7 untruncated mini-batches, there will be a truncated mini-batch of size 12\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=460 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1671 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 460  |\n",
      "-----------------------------\n",
      "Eval num_timesteps=500, episode_reward=57.80 +/- 6.76\n",
      "Episode length: 57.80 +/- 6.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 57.8        |\n",
      "|    mean_reward          | 57.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009606323 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.0126     |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1309 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 920  |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=282.80 +/- 29.67\n",
      "Episode length: 282.80 +/- 29.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 283         |\n",
      "|    mean_reward          | 283         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014602743 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | 0.0609      |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | 0.000191    |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 886  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1380 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=499.00 +/- 2.00\n",
      "Episode length: 499.00 +/- 2.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 499          |\n",
      "|    mean_reward          | 499          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063863453 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.681       |\n",
      "|    explained_variance   | 0.0748       |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 7.34         |\n",
      "|    n_updates            | 9            |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 25.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 660  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 1840 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=428.80 +/- 93.88\n",
      "Episode length: 428.80 +/- 93.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 429          |\n",
      "|    mean_reward          | 429          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016541177 |\n",
      "|    clip_fraction        | 0.166        |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.624       |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 7.2          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    value_loss           | 24.3         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 605  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2300 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=307.40 +/- 88.39\n",
      "Episode length: 307.40 +/- 88.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 307         |\n",
      "|    mean_reward          | 307         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010407688 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.623      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 34.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 596  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2760 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=415.00 +/- 74.64\n",
      "Episode length: 415.00 +/- 74.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 415        |\n",
      "|    mean_reward          | 415        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00891927 |\n",
      "|    clip_fraction        | 0.0473     |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.642     |\n",
      "|    explained_variance   | 0.21       |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 7.02       |\n",
      "|    n_updates            | 18         |\n",
      "|    policy_gradient_loss | -0.00319   |\n",
      "|    value_loss           | 17.8       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 572  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 3220 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=250.20 +/- 107.96\n",
      "Episode length: 250.20 +/- 107.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 250         |\n",
      "|    mean_reward          | 250         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015840923 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | -1.6        |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | -0.00363    |\n",
      "|    value_loss           | 8.22        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 571  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 3680 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=421.60 +/- 96.43\n",
      "Episode length: 421.60 +/- 96.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 422         |\n",
      "|    mean_reward          | 422         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008538091 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | 0.732       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 3.57        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00353    |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 526  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 4140 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=371.20 +/- 105.89\n",
      "Episode length: 371.20 +/- 105.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 371         |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017003926 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    value_loss           | 8.89        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 523  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 4600 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=385.40 +/- 74.51\n",
      "Episode length: 385.40 +/- 74.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 385         |\n",
      "|    mean_reward          | 385         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011158455 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.411       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00252    |\n",
      "|    value_loss           | 2.55        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 518  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 5060 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=236.20 +/- 15.39\n",
      "Episode length: 236.20 +/- 15.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 236         |\n",
      "|    mean_reward          | 236         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032457914 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 2.84        |\n",
      "|    n_updates            | 33          |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 13.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 528  |\n",
      "|    iterations      | 12   |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 5520 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 556         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 5980        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026682515 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 2.64        |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.0064      |\n",
      "|    value_loss           | 7.52        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=215.60 +/- 15.45\n",
      "Episode length: 215.60 +/- 15.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 216          |\n",
      "|    mean_reward          | 216          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046455064 |\n",
      "|    clip_fraction        | 0.218        |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.573       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.407        |\n",
      "|    n_updates            | 39           |\n",
      "|    policy_gradient_loss | -0.0125      |\n",
      "|    value_loss           | 2.19         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 564  |\n",
      "|    iterations      | 14   |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 6440 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=314.80 +/- 151.23\n",
      "Episode length: 314.80 +/- 151.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 315        |\n",
      "|    mean_reward          | 315        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01833519 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.514     |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 0.453      |\n",
      "|    n_updates            | 42         |\n",
      "|    policy_gradient_loss | -0.00454   |\n",
      "|    value_loss           | 3.12       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 562  |\n",
      "|    iterations      | 15   |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 6900 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=459.80 +/- 80.40\n",
      "Episode length: 459.80 +/- 80.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 460        |\n",
      "|    mean_reward          | 460        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01576034 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.522     |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 1.07       |\n",
      "|    n_updates            | 45         |\n",
      "|    policy_gradient_loss | 0.0192     |\n",
      "|    value_loss           | 1.16       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 549  |\n",
      "|    iterations      | 16   |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 7360 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=413.00 +/- 112.59\n",
      "Episode length: 413.00 +/- 112.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 413         |\n",
      "|    mean_reward          | 413         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007470345 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.174       |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | 0.00611     |\n",
      "|    value_loss           | 0.477       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 543  |\n",
      "|    iterations      | 17   |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 7820 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=311.80 +/- 153.81\n",
      "Episode length: 311.80 +/- 153.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 312         |\n",
      "|    mean_reward          | 312         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006094229 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.523      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0726      |\n",
      "|    n_updates            | 51          |\n",
      "|    policy_gradient_loss | 0.00121     |\n",
      "|    value_loss           | 0.64        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 543  |\n",
      "|    iterations      | 18   |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 8280 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01385035 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.505     |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 0.653      |\n",
      "|    n_updates            | 54         |\n",
      "|    policy_gradient_loss | -0.00227   |\n",
      "|    value_loss           | 0.885      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 530  |\n",
      "|    iterations      | 19   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 8740 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=361.60 +/- 119.21\n",
      "Episode length: 361.60 +/- 119.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 362         |\n",
      "|    mean_reward          | 362         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014561746 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.243       |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | 0.00809     |\n",
      "|    value_loss           | 0.296       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 528  |\n",
      "|    iterations      | 20   |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 9200 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=494.40 +/- 11.20\n",
      "Episode length: 494.40 +/- 11.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 494         |\n",
      "|    mean_reward          | 494         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025331795 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.433      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0117      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00345     |\n",
      "|    value_loss           | 0.368       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 520  |\n",
      "|    iterations      | 21   |\n",
      "|    time_elapsed    | 18   |\n",
      "|    total_timesteps | 9660 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=425.80 +/- 91.17\n",
      "Episode length: 425.80 +/- 91.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 426          |\n",
      "|    mean_reward          | 426          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068972777 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.429       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.00463      |\n",
      "|    n_updates            | 63           |\n",
      "|    policy_gradient_loss | 0.0116       |\n",
      "|    value_loss           | 0.111        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 10120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=408.60 +/- 59.14\n",
      "Episode length: 408.60 +/- 59.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 409          |\n",
      "|    mean_reward          | 409          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050424505 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0197       |\n",
      "|    n_updates            | 66           |\n",
      "|    policy_gradient_loss | -0.00348     |\n",
      "|    value_loss           | 0.354        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 514   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 10580 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=407.00 +/- 95.49\n",
      "Episode length: 407.00 +/- 95.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 407          |\n",
      "|    mean_reward          | 407          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062411777 |\n",
      "|    clip_fraction        | 0.0632       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.344       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0458       |\n",
      "|    n_updates            | 69           |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    value_loss           | 0.189        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 11040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=396.80 +/- 114.25\n",
      "Episode length: 396.80 +/- 114.25\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 397        |\n",
      "|    mean_reward          | 397        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04477521 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.321     |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 0.0301     |\n",
      "|    n_updates            | 72         |\n",
      "|    policy_gradient_loss | 0.0355     |\n",
      "|    value_loss           | 0.665      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 508   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 11500 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 11960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021248674 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.00587     |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=440.40 +/- 119.20\n",
      "Episode length: 440.40 +/- 119.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 440         |\n",
      "|    mean_reward          | 440         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003444689 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.00997     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 518   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 12420 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=360.20 +/- 120.53\n",
      "Episode length: 360.20 +/- 120.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 360         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011316572 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 81          |\n",
      "|    policy_gradient_loss | 0.00729     |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 12880 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=309.80 +/- 142.84\n",
      "Episode length: 309.80 +/- 142.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 310         |\n",
      "|    mean_reward          | 310         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064893864 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.206      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0798      |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.00472    |\n",
      "|    value_loss           | 0.317       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 13340 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=275.00 +/- 130.35\n",
      "Episode length: 275.00 +/- 130.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 275         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007973928 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 87          |\n",
      "|    policy_gradient_loss | 0.000227    |\n",
      "|    value_loss           | 0.145       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 509   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 13800 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=396.60 +/- 93.39\n",
      "Episode length: 396.60 +/- 93.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 397         |\n",
      "|    mean_reward          | 397         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002375422 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00368     |\n",
      "|    value_loss           | 0.0945      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 500   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 14260 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=469.60 +/- 60.80\n",
      "Episode length: 469.60 +/- 60.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 470         |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003020567 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.000486   |\n",
      "|    n_updates            | 93          |\n",
      "|    policy_gradient_loss | 0.00161     |\n",
      "|    value_loss           | 0.0938      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 487   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 14720 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=445.80 +/- 108.40\n",
      "Episode length: 445.80 +/- 108.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 446         |\n",
      "|    mean_reward          | 446         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006047999 |\n",
      "|    clip_fraction        | 0.0551      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0605      |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | 0.0046      |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 481   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 15180 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=423.40 +/- 126.64\n",
      "Episode length: 423.40 +/- 126.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 423         |\n",
      "|    mean_reward          | 423         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009366432 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.174      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.152       |\n",
      "|    n_updates            | 99          |\n",
      "|    policy_gradient_loss | 0.0042      |\n",
      "|    value_loss           | 0.217       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 15640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=316.40 +/- 120.39\n",
      "Episode length: 316.40 +/- 120.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | 316         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007087718 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.218      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | -0.000776   |\n",
      "|    value_loss           | 0.0524      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 16100 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=351.40 +/- 132.22\n",
      "Episode length: 351.40 +/- 132.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | 351          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059368145 |\n",
      "|    clip_fraction        | 0.0809       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.148       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.0277      |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 0.106        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 16560 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=424.40 +/- 92.59\n",
      "Episode length: 424.40 +/- 92.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 424        |\n",
      "|    mean_reward          | 424        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01931713 |\n",
      "|    clip_fraction        | 0.0938     |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.169     |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | -0.0492    |\n",
      "|    n_updates            | 108        |\n",
      "|    policy_gradient_loss | 0.00331    |\n",
      "|    value_loss           | 0.0854     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 467   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 17020 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 38         |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 17480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05036839 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.15      |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 0.0493     |\n",
      "|    n_updates            | 111        |\n",
      "|    policy_gradient_loss | 0.00643    |\n",
      "|    value_loss           | 0.071      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=443.80 +/- 48.10\n",
      "Episode length: 443.80 +/- 48.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 444          |\n",
      "|    mean_reward          | 444          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066767903 |\n",
      "|    clip_fraction        | 0.0792       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.0109      |\n",
      "|    n_updates            | 114          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    value_loss           | 0.0473       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 464   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 17940 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=370.00 +/- 113.62\n",
      "Episode length: 370.00 +/- 113.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 370         |\n",
      "|    mean_reward          | 370         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013039607 |\n",
      "|    clip_fraction        | 0.0803      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 117         |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    value_loss           | 0.0811      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 464   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 18400 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008154634 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.00962     |\n",
      "|    value_loss           | 0.0564      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 459   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 18860 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=404.60 +/- 118.75\n",
      "Episode length: 404.60 +/- 118.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 405         |\n",
      "|    mean_reward          | 405         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015955945 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0579      |\n",
      "|    n_updates            | 123         |\n",
      "|    policy_gradient_loss | 0.00912     |\n",
      "|    value_loss           | 0.15        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 452   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 19320 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=438.60 +/- 122.80\n",
      "Episode length: 438.60 +/- 122.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 439          |\n",
      "|    mean_reward          | 439          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067762965 |\n",
      "|    clip_fraction        | 0.0757       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.00911      |\n",
      "|    n_updates            | 126          |\n",
      "|    policy_gradient_loss | 0.00126      |\n",
      "|    value_loss           | 0.0636       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 450   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 19780 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=457.80 +/- 84.40\n",
      "Episode length: 457.80 +/- 84.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 458          |\n",
      "|    mean_reward          | 458          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036329364 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.00969      |\n",
      "|    n_updates            | 129          |\n",
      "|    policy_gradient_loss | 0.00181      |\n",
      "|    value_loss           | 0.0465       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 445   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 20240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=461.60 +/- 76.80\n",
      "Episode length: 461.60 +/- 76.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 462         |\n",
      "|    mean_reward          | 462         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019940399 |\n",
      "|    clip_fraction        | 0.0586      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.0415     |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | 0.00332     |\n",
      "|    value_loss           | 0.0511      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 440   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 20700 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=428.00 +/- 59.38\n",
      "Episode length: 428.00 +/- 59.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 428         |\n",
      "|    mean_reward          | 428         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014213735 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.182      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | -0.00441    |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 438   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 21160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=464.80 +/- 62.25\n",
      "Episode length: 464.80 +/- 62.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 465         |\n",
      "|    mean_reward          | 465         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001890711 |\n",
      "|    clip_fraction        | 0.0104      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.00712     |\n",
      "|    n_updates            | 138         |\n",
      "|    policy_gradient_loss | 0.00115     |\n",
      "|    value_loss           | 0.469       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 437   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 21620 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=494.20 +/- 11.60\n",
      "Episode length: 494.20 +/- 11.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | 494          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035982628 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.184        |\n",
      "|    n_updates            | 141          |\n",
      "|    policy_gradient_loss | 0.00258      |\n",
      "|    value_loss           | 0.272        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 435   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 22080 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=370.60 +/- 93.94\n",
      "Episode length: 370.60 +/- 93.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 371         |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007947136 |\n",
      "|    clip_fraction        | 0.0703      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.149      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | 0.00317     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 435   |\n",
      "|    iterations      | 49    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 22540 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=495.60 +/- 8.80\n",
      "Episode length: 495.60 +/- 8.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | 496         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004776539 |\n",
      "|    clip_fraction        | 0.0438      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.126      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 147         |\n",
      "|    policy_gradient_loss | 0.00188     |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 433   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 23000 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 440         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 23460       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004738788 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.19       |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0102      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    value_loss           | 0.0508      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041655406 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 153         |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 438   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 23920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=490.40 +/- 18.70\n",
      "Episode length: 490.40 +/- 18.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 490          |\n",
      "|    mean_reward          | 490          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032202606 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.161       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.0132      |\n",
      "|    n_updates            | 156          |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 0.272        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 437   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 24380 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006476499 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.166      |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 159         |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    value_loss           | 1.03        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 435   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 24840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=472.40 +/- 43.78\n",
      "Episode length: 472.40 +/- 43.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 472          |\n",
      "|    mean_reward          | 472          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020149224 |\n",
      "|    clip_fraction        | 0.036        |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.158       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.00544     |\n",
      "|    n_updates            | 162          |\n",
      "|    policy_gradient_loss | 0.00115      |\n",
      "|    value_loss           | 0.00116      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 434   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 25300 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014384008 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.184      |\n",
      "|    explained_variance   | 0.891       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.103       |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | 0.00919     |\n",
      "|    value_loss           | 0.877       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 433   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 25760 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=458.00 +/- 78.62\n",
      "Episode length: 458.00 +/- 78.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 458          |\n",
      "|    mean_reward          | 458          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031857493 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.197       |\n",
      "|    explained_variance   | 0.953        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 168          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 0.41         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 432   |\n",
      "|    iterations      | 57    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 26220 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=476.20 +/- 47.60\n",
      "Episode length: 476.20 +/- 47.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 476         |\n",
      "|    mean_reward          | 476         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008740428 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.244      |\n",
      "|    explained_variance   | -0.0358     |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.00709     |\n",
      "|    n_updates            | 171         |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    value_loss           | 0.000944    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 431   |\n",
      "|    iterations      | 58    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 26680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=447.00 +/- 82.11\n",
      "Episode length: 447.00 +/- 82.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 447          |\n",
      "|    mean_reward          | 447          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 27000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052062273 |\n",
      "|    clip_fraction        | 0.0414       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.181       |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0402       |\n",
      "|    n_updates            | 174          |\n",
      "|    policy_gradient_loss | -0.00357     |\n",
      "|    value_loss           | 0.292        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 59    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 27140 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=419.00 +/- 82.54\n",
      "Episode length: 419.00 +/- 82.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 419         |\n",
      "|    mean_reward          | 419         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022982266 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.00171     |\n",
      "|    n_updates            | 177         |\n",
      "|    policy_gradient_loss | 0.00685     |\n",
      "|    value_loss           | 0.285       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 431   |\n",
      "|    iterations      | 60    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 27600 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=479.40 +/- 41.20\n",
      "Episode length: 479.40 +/- 41.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 479         |\n",
      "|    mean_reward          | 479         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015514932 |\n",
      "|    clip_fraction        | 0.0916      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0917      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00214     |\n",
      "|    value_loss           | 0.101       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 61    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 28060 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=455.80 +/- 88.40\n",
      "Episode length: 455.80 +/- 88.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 456         |\n",
      "|    mean_reward          | 456         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004268614 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.145      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0365      |\n",
      "|    n_updates            | 183         |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    value_loss           | 0.0152      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 429   |\n",
      "|    iterations      | 62    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 28520 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 28980        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025579722 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.131       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 186          |\n",
      "|    policy_gradient_loss | 0.000739     |\n",
      "|    value_loss           | 0.232        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067232833 |\n",
      "|    clip_fraction        | 0.0829       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.167       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0266       |\n",
      "|    n_updates            | 189          |\n",
      "|    policy_gradient_loss | -0.0043      |\n",
      "|    value_loss           | 0.0682       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 433   |\n",
      "|    iterations      | 64    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 29440 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009685826 |\n",
      "|    clip_fraction        | 0.0608      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.154       |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.000513   |\n",
      "|    value_loss           | 0.647       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 432   |\n",
      "|    iterations      | 65    |\n",
      "|    time_elapsed    | 69    |\n",
      "|    total_timesteps | 29900 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031653643 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.132       |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.488        |\n",
      "|    n_updates            | 195          |\n",
      "|    policy_gradient_loss | -0.00553     |\n",
      "|    value_loss           | 0.303        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 431   |\n",
      "|    iterations      | 66    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 30360 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=475.20 +/- 49.60\n",
      "Episode length: 475.20 +/- 49.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 475         |\n",
      "|    mean_reward          | 475         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023367353 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0914      |\n",
      "|    n_updates            | 198         |\n",
      "|    policy_gradient_loss | 0.00785     |\n",
      "|    value_loss           | 0.702       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 67    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 30820 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027253614 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.00146     |\n",
      "|    n_updates            | 201          |\n",
      "|    policy_gradient_loss | -0.00528     |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 426   |\n",
      "|    iterations      | 68    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 31280 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011402456 |\n",
      "|    clip_fraction        | 0.0747      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.205      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.00871     |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | 0.00268     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 425   |\n",
      "|    iterations      | 69    |\n",
      "|    time_elapsed    | 74    |\n",
      "|    total_timesteps | 31740 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011942564 |\n",
      "|    clip_fraction        | 0.0933      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.165      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.00967    |\n",
      "|    n_updates            | 207         |\n",
      "|    policy_gradient_loss | 0.00158     |\n",
      "|    value_loss           | 0.00251     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 70    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 32200 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053710965 |\n",
      "|    clip_fraction        | 0.0464       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.155       |\n",
      "|    explained_variance   | 0.923        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0253       |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | 0.00402      |\n",
      "|    value_loss           | 0.00336      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 71    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 32660 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001279329 |\n",
      "|    clip_fraction        | 0.0395      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.148      |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.004      |\n",
      "|    n_updates            | 213         |\n",
      "|    policy_gradient_loss | 0.000432    |\n",
      "|    value_loss           | 0.000884    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 72    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 33120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016776267 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.188      |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.000624    |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | 0.0034      |\n",
      "|    value_loss           | 0.0606      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 73    |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 33580 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=474.40 +/- 40.29\n",
      "Episode length: 474.40 +/- 40.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 474         |\n",
      "|    mean_reward          | 474         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002790704 |\n",
      "|    clip_fraction        | 0.0773      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.186      |\n",
      "|    explained_variance   | 0.887       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.00126    |\n",
      "|    n_updates            | 219         |\n",
      "|    policy_gradient_loss | 0.00035     |\n",
      "|    value_loss           | 0.000699    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 74    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 34040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=410.80 +/- 64.79\n",
      "Episode length: 410.80 +/- 64.79\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 411       |\n",
      "|    mean_reward          | 411       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 34500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0065623 |\n",
      "|    clip_fraction        | 0.0508    |\n",
      "|    clip_range           | 0.176     |\n",
      "|    entropy_loss         | -0.127    |\n",
      "|    explained_variance   | 0.898     |\n",
      "|    learning_rate        | 0.00425   |\n",
      "|    loss                 | 0.0032    |\n",
      "|    n_updates            | 222       |\n",
      "|    policy_gradient_loss | 0.0019    |\n",
      "|    value_loss           | 0.914     |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 75    |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 34500 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 428          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 34960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005810945 |\n",
      "|    clip_fraction        | 0.0026       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.103       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 2.05         |\n",
      "|    n_updates            | 225          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 1.86         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=485.80 +/- 28.40\n",
      "Episode length: 485.80 +/- 28.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 486         |\n",
      "|    mean_reward          | 486         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013744548 |\n",
      "|    clip_fraction        | 0.0276      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.206      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.513       |\n",
      "|    n_updates            | 228         |\n",
      "|    policy_gradient_loss | 0.0151      |\n",
      "|    value_loss           | 0.832       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 428   |\n",
      "|    iterations      | 77    |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 35420 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=491.20 +/- 17.60\n",
      "Episode length: 491.20 +/- 17.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 491          |\n",
      "|    mean_reward          | 491          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014821521 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.122       |\n",
      "|    explained_variance   | 0.647        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.00767     |\n",
      "|    n_updates            | 231          |\n",
      "|    policy_gradient_loss | 0.000899     |\n",
      "|    value_loss           | 0.00935      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 428   |\n",
      "|    iterations      | 78    |\n",
      "|    time_elapsed    | 83    |\n",
      "|    total_timesteps | 35880 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=495.80 +/- 8.40\n",
      "Episode length: 495.80 +/- 8.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | 496         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010637924 |\n",
      "|    clip_fraction        | 0.0653      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 234         |\n",
      "|    policy_gradient_loss | -0.000519   |\n",
      "|    value_loss           | 0.00179     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 426   |\n",
      "|    iterations      | 79    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 36340 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=458.40 +/- 51.20\n",
      "Episode length: 458.40 +/- 51.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 458          |\n",
      "|    mean_reward          | 458          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 36500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024810228 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.139       |\n",
      "|    explained_variance   | 0.597        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.0125      |\n",
      "|    n_updates            | 237          |\n",
      "|    policy_gradient_loss | -0.000239    |\n",
      "|    value_loss           | 0.00195      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 425   |\n",
      "|    iterations      | 80    |\n",
      "|    time_elapsed    | 86    |\n",
      "|    total_timesteps | 36800 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=374.00 +/- 70.98\n",
      "Episode length: 374.00 +/- 70.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 374          |\n",
      "|    mean_reward          | 374          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021700019 |\n",
      "|    clip_fraction        | 0.0137       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.112       |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0714       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | 0.00393      |\n",
      "|    value_loss           | 0.658        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 81    |\n",
      "|    time_elapsed    | 87    |\n",
      "|    total_timesteps | 37260 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=445.00 +/- 30.02\n",
      "Episode length: 445.00 +/- 30.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 445         |\n",
      "|    mean_reward          | 445         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005881752 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.0299      |\n",
      "|    n_updates            | 243         |\n",
      "|    policy_gradient_loss | 0.00287     |\n",
      "|    value_loss           | 0.267       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 82    |\n",
      "|    time_elapsed    | 89    |\n",
      "|    total_timesteps | 37720 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=401.00 +/- 25.18\n",
      "Episode length: 401.00 +/- 25.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 401          |\n",
      "|    mean_reward          | 401          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011184507 |\n",
      "|    clip_fraction        | 0.00977      |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.12        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0148       |\n",
      "|    n_updates            | 246          |\n",
      "|    policy_gradient_loss | 0.00031      |\n",
      "|    value_loss           | 0.153        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 83    |\n",
      "|    time_elapsed    | 90    |\n",
      "|    total_timesteps | 38180 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 38500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02702533 |\n",
      "|    clip_fraction        | 0.0799     |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.0989    |\n",
      "|    explained_variance   | 0.809      |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 0.075      |\n",
      "|    n_updates            | 249        |\n",
      "|    policy_gradient_loss | 0.00214    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 422   |\n",
      "|    iterations      | 84    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 38640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003787477 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.0784      |\n",
      "|    explained_variance   | 0.585        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.00391     |\n",
      "|    n_updates            | 252          |\n",
      "|    policy_gradient_loss | -0.000325    |\n",
      "|    value_loss           | 0.0653       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 421   |\n",
      "|    iterations      | 85    |\n",
      "|    time_elapsed    | 92    |\n",
      "|    total_timesteps | 39100 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=462.40 +/- 20.69\n",
      "Episode length: 462.40 +/- 20.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 462          |\n",
      "|    mean_reward          | 462          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058903317 |\n",
      "|    clip_fraction        | 0.0295       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.0714      |\n",
      "|    explained_variance   | 0.567        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.00712      |\n",
      "|    n_updates            | 255          |\n",
      "|    policy_gradient_loss | 0.00257      |\n",
      "|    value_loss           | 0.00726      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 420   |\n",
      "|    iterations      | 86    |\n",
      "|    time_elapsed    | 93    |\n",
      "|    total_timesteps | 39560 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01052513 |\n",
      "|    clip_fraction        | 0.0397     |\n",
      "|    clip_range           | 0.176      |\n",
      "|    entropy_loss         | -0.0741    |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 0.00425    |\n",
      "|    loss                 | 0.0548     |\n",
      "|    n_updates            | 258        |\n",
      "|    policy_gradient_loss | 0.0025     |\n",
      "|    value_loss           | 0.0618     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 420   |\n",
      "|    iterations      | 87    |\n",
      "|    time_elapsed    | 95    |\n",
      "|    total_timesteps | 40020 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 424         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 40480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014210618 |\n",
      "|    clip_fraction        | 0.051       |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.0906     |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.00136     |\n",
      "|    n_updates            | 261         |\n",
      "|    policy_gradient_loss | 0.00262     |\n",
      "|    value_loss           | 0.0136      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020352334 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.074      |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 264         |\n",
      "|    policy_gradient_loss | 0.000923    |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 89    |\n",
      "|    time_elapsed    | 96    |\n",
      "|    total_timesteps | 40940 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=489.20 +/- 21.60\n",
      "Episode length: 489.20 +/- 21.60\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 489           |\n",
      "|    mean_reward          | 489           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 41000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090049766 |\n",
      "|    clip_fraction        | 0.0104        |\n",
      "|    clip_range           | 0.176         |\n",
      "|    entropy_loss         | -0.091        |\n",
      "|    explained_variance   | 0.771         |\n",
      "|    learning_rate        | 0.00425       |\n",
      "|    loss                 | -0.000697     |\n",
      "|    n_updates            | 267           |\n",
      "|    policy_gradient_loss | -0.00116      |\n",
      "|    value_loss           | 0.000308      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 90    |\n",
      "|    time_elapsed    | 97    |\n",
      "|    total_timesteps | 41400 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=441.40 +/- 52.91\n",
      "Episode length: 441.40 +/- 52.91\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 441           |\n",
      "|    mean_reward          | 441           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 41500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085787557 |\n",
      "|    clip_fraction        | 0.0237        |\n",
      "|    clip_range           | 0.176         |\n",
      "|    entropy_loss         | -0.0919       |\n",
      "|    explained_variance   | 0.446         |\n",
      "|    learning_rate        | 0.00425       |\n",
      "|    loss                 | 0.00132       |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | 0.000738      |\n",
      "|    value_loss           | 0.000771      |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 91    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 41860 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=377.00 +/- 45.46\n",
      "Episode length: 377.00 +/- 45.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 377         |\n",
      "|    mean_reward          | 377         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011361612 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.0875     |\n",
      "|    explained_variance   | 0.295       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | -0.00414    |\n",
      "|    n_updates            | 273         |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    value_loss           | 0.00574     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 92    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 42320 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=476.80 +/- 28.42\n",
      "Episode length: 476.80 +/- 28.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 477         |\n",
      "|    mean_reward          | 477         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004114656 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.176       |\n",
      "|    entropy_loss         | -0.0935     |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.00425     |\n",
      "|    loss                 | 0.481       |\n",
      "|    n_updates            | 276         |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    value_loss           | 1.73        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 93    |\n",
      "|    time_elapsed    | 100   |\n",
      "|    total_timesteps | 42780 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=414.60 +/- 44.79\n",
      "Episode length: 414.60 +/- 44.79\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 415          |\n",
      "|    mean_reward          | 415          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034325542 |\n",
      "|    clip_fraction        | 0.0215       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.0867      |\n",
      "|    explained_variance   | -0.0586      |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 4.67         |\n",
      "|    n_updates            | 279          |\n",
      "|    policy_gradient_loss | -0.000338    |\n",
      "|    value_loss           | 11           |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 422   |\n",
      "|    iterations      | 94    |\n",
      "|    time_elapsed    | 102   |\n",
      "|    total_timesteps | 43240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=458.40 +/- 49.30\n",
      "Episode length: 458.40 +/- 49.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 458          |\n",
      "|    mean_reward          | 458          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007785544 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.108       |\n",
      "|    explained_variance   | 0.602        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.0187       |\n",
      "|    n_updates            | 282          |\n",
      "|    policy_gradient_loss | 0.00261      |\n",
      "|    value_loss           | 0.068        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 422   |\n",
      "|    iterations      | 95    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 43700 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=492.60 +/- 14.80\n",
      "Episode length: 492.60 +/- 14.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 493          |\n",
      "|    mean_reward          | 493          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043883333 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.136       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.0138      |\n",
      "|    n_updates            | 285          |\n",
      "|    policy_gradient_loss | 0.00265      |\n",
      "|    value_loss           | 0.03         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 421   |\n",
      "|    iterations      | 96    |\n",
      "|    time_elapsed    | 104   |\n",
      "|    total_timesteps | 44160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032007988 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.29         |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | 0.00615      |\n",
      "|    n_updates            | 288          |\n",
      "|    policy_gradient_loss | 0.000948     |\n",
      "|    value_loss           | 0.00977      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 421   |\n",
      "|    iterations      | 97    |\n",
      "|    time_elapsed    | 105   |\n",
      "|    total_timesteps | 44620 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062495964 |\n",
      "|    clip_fraction        | 0.069        |\n",
      "|    clip_range           | 0.176        |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.00425      |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 291          |\n",
      "|    policy_gradient_loss | 0.00368      |\n",
      "|    value_loss           | 0.0213       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 419   |\n",
      "|    iterations      | 98    |\n",
      "|    time_elapsed    | 107   |\n",
      "|    total_timesteps | 45080 |\n",
      "------------------------------\n",
      "Single environment training took 107.50 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-sweep-3</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/maf5t0pd' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/maf5t0pd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_131913-maf5t0pd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: izrwlfhc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2892569697834375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.006980804007713124\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9924020261966444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.921687619532182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005588395986800764\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.37108214483521473\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 997\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 55757\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_132113-izrwlfhc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/izrwlfhc' target=\"_blank\">lilac-sweep-4</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/izrwlfhc' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/izrwlfhc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 997`, after every 15 untruncated mini-batches, there will be a truncated mini-batch of size 37\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=997 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=9.20 +/- 0.75\n",
      "Episode length: 9.20 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 9.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1874 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 997  |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=255.80 +/- 149.41\n",
      "Episode length: 255.80 +/- 149.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 256         |\n",
      "|    mean_reward          | 256         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029687017 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | -0.0083     |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 2.82        |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=146.60 +/- 86.95\n",
      "Episode length: 146.60 +/- 86.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 147      |\n",
      "|    mean_reward     | 147      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1020 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1994 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=157.40 +/- 108.46\n",
      "Episode length: 157.40 +/- 108.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 157         |\n",
      "|    mean_reward          | 157         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015551074 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.096       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.861       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.00794    |\n",
      "|    value_loss           | 6           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=115.40 +/- 20.93\n",
      "Episode length: 115.40 +/- 20.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 115      |\n",
      "|    mean_reward     | 115      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 949  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2991 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=132.00 +/- 30.10\n",
      "Episode length: 132.00 +/- 30.10\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 132        |\n",
      "|    mean_reward          | 132        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02276147 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.55      |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | 1.19       |\n",
      "|    n_updates            | 9          |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    value_loss           | 3.49       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=121.20 +/- 10.50\n",
      "Episode length: 121.20 +/- 10.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 121      |\n",
      "|    mean_reward     | 121      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 937  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 3988 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=466.00 +/- 68.00\n",
      "Episode length: 466.00 +/- 68.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 466         |\n",
      "|    mean_reward          | 466         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031613618 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.283       |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.00882    |\n",
      "|    value_loss           | 0.863       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=468.00 +/- 64.00\n",
      "Episode length: 468.00 +/- 64.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 468      |\n",
      "|    mean_reward     | 468      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 757  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 4985 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=221.80 +/- 15.92\n",
      "Episode length: 221.80 +/- 15.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 222         |\n",
      "|    mean_reward          | 222         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023843613 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.183       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.00834    |\n",
      "|    value_loss           | 0.912       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=311.00 +/- 93.91\n",
      "Episode length: 311.00 +/- 93.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 311      |\n",
      "|    mean_reward     | 311      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 733  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 5982 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=404.20 +/- 21.52\n",
      "Episode length: 404.20 +/- 21.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 404         |\n",
      "|    mean_reward          | 404         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017487703 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.172       |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.000103   |\n",
      "|    value_loss           | 0.5         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=439.60 +/- 44.88\n",
      "Episode length: 439.60 +/- 44.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 440      |\n",
      "|    mean_reward     | 440      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 681  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 6979 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=268.80 +/- 18.63\n",
      "Episode length: 268.80 +/- 18.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 269         |\n",
      "|    mean_reward          | 269         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009569775 |\n",
      "|    clip_fraction        | 0.0849      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.0732      |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.32        |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | 0.00742     |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=267.00 +/- 22.30\n",
      "Episode length: 267.00 +/- 22.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 267      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 671  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 7976 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=231.80 +/- 14.61\n",
      "Episode length: 231.80 +/- 14.61\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 232       |\n",
      "|    mean_reward          | 232       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 8000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0648121 |\n",
      "|    clip_fraction        | 0.2       |\n",
      "|    clip_range           | 0.289     |\n",
      "|    entropy_loss         | -0.517    |\n",
      "|    explained_variance   | 0.324     |\n",
      "|    learning_rate        | 0.00559   |\n",
      "|    loss                 | 1.05      |\n",
      "|    n_updates            | 24        |\n",
      "|    policy_gradient_loss | 0.00453   |\n",
      "|    value_loss           | 0.997     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=205.40 +/- 2.73\n",
      "Episode length: 205.40 +/- 2.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 205      |\n",
      "|    mean_reward     | 205      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 675  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 8973 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015650898 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.426       |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 2.43        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 625  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 9970 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=498.00 +/- 4.00\n",
      "Episode length: 498.00 +/- 4.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 498         |\n",
      "|    mean_reward          | 498         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018690413 |\n",
      "|    clip_fraction        | 0.069       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.595       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.175       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00468    |\n",
      "|    value_loss           | 0.486       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 594   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 10967 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=443.80 +/- 112.40\n",
      "Episode length: 443.80 +/- 112.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 444         |\n",
      "|    mean_reward          | 444         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042268824 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.0231      |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0498      |\n",
      "|    n_updates            | 33          |\n",
      "|    policy_gradient_loss | 0.0123      |\n",
      "|    value_loss           | 0.662       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=476.00 +/- 39.20\n",
      "Episode length: 476.00 +/- 39.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 476      |\n",
      "|    mean_reward     | 476      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 576   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 11964 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142975915 |\n",
      "|    clip_fraction        | 0.0901       |\n",
      "|    clip_range           | 0.289        |\n",
      "|    entropy_loss         | -0.511       |\n",
      "|    explained_variance   | 0.521        |\n",
      "|    learning_rate        | 0.00559      |\n",
      "|    loss                 | 0.705        |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 1.51         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 558   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 12961 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016203325 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.615       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.546       |\n",
      "|    n_updates            | 39          |\n",
      "|    policy_gradient_loss | 0.00829     |\n",
      "|    value_loss           | 0.574       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=484.40 +/- 31.20\n",
      "Episode length: 484.40 +/- 31.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 484      |\n",
      "|    mean_reward     | 484      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 544   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 13958 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=346.20 +/- 126.68\n",
      "Episode length: 346.20 +/- 126.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 346        |\n",
      "|    mean_reward          | 346        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04818186 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.45      |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | -0.00335   |\n",
      "|    n_updates            | 42         |\n",
      "|    policy_gradient_loss | 0.0211     |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=373.80 +/- 136.61\n",
      "Episode length: 373.80 +/- 136.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 542   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 14955 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=344.40 +/- 129.48\n",
      "Episode length: 344.40 +/- 129.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 344         |\n",
      "|    mean_reward          | 344         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013795145 |\n",
      "|    clip_fraction        | 0.0537      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0629      |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.000146    |\n",
      "|    value_loss           | 0.418       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=256.20 +/- 108.53\n",
      "Episode length: 256.20 +/- 108.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 545   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 15952 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020936817 |\n",
      "|    clip_fraction        | 0.0772      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0786      |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | 0.0157      |\n",
      "|    value_loss           | 0.0283      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 529   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 16949 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=475.40 +/- 49.20\n",
      "Episode length: 475.40 +/- 49.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 475         |\n",
      "|    mean_reward          | 475         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017294997 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0393      |\n",
      "|    n_updates            | 51          |\n",
      "|    policy_gradient_loss | 0.00779     |\n",
      "|    value_loss           | 0.0709      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=464.60 +/- 46.48\n",
      "Episode length: 464.60 +/- 46.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 465      |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 17946 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=458.00 +/- 60.72\n",
      "Episode length: 458.00 +/- 60.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 458         |\n",
      "|    mean_reward          | 458         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014258064 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 0.852       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=448.40 +/- 66.96\n",
      "Episode length: 448.40 +/- 66.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 448      |\n",
      "|    mean_reward     | 448      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 506   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 18943 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007622419 |\n",
      "|    clip_fraction        | 0.0316      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.52       |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0202      |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | 0.00395     |\n",
      "|    value_loss           | 0.0934      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 498   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 19940 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01217229 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.531     |\n",
      "|    explained_variance   | 0.75       |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | 0.0418     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.00245   |\n",
      "|    value_loss           | 0.745      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=435.40 +/- 61.88\n",
      "Episode length: 435.40 +/- 61.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 435      |\n",
      "|    mean_reward     | 435      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 494   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 20937 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023978096 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.623       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.166       |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 490   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 21934 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=483.40 +/- 33.20\n",
      "Episode length: 483.40 +/- 33.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015409608 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.415      |\n",
      "|    explained_variance   | 0.0723      |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0517      |\n",
      "|    n_updates            | 66          |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    value_loss           | 0.801       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=470.20 +/- 37.63\n",
      "Episode length: 470.20 +/- 37.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 470      |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 488   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 22931 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=493.80 +/- 12.40\n",
      "Episode length: 493.80 +/- 12.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 494         |\n",
      "|    mean_reward          | 494         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021420509 |\n",
      "|    clip_fraction        | 0.0996      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.443      |\n",
      "|    explained_variance   | -23.6       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0223      |\n",
      "|    n_updates            | 69          |\n",
      "|    policy_gradient_loss | 0.00202     |\n",
      "|    value_loss           | 0.00954     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=470.20 +/- 36.99\n",
      "Episode length: 470.20 +/- 36.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 470      |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 23928 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=352.40 +/- 32.42\n",
      "Episode length: 352.40 +/- 32.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 352        |\n",
      "|    mean_reward          | 352        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05257391 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.395     |\n",
      "|    explained_variance   | -11.4      |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | -0.0421    |\n",
      "|    n_updates            | 72         |\n",
      "|    policy_gradient_loss | 0.00445    |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=348.60 +/- 55.03\n",
      "Episode length: 348.60 +/- 55.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 349      |\n",
      "|    mean_reward     | 349      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 487   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 24925 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=388.40 +/- 62.01\n",
      "Episode length: 388.40 +/- 62.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 388         |\n",
      "|    mean_reward          | 388         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030013882 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.424      |\n",
      "|    explained_variance   | -6.11       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 0.000367    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=375.80 +/- 74.74\n",
      "Episode length: 375.80 +/- 74.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 376      |\n",
      "|    mean_reward     | 376      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 488   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 53    |\n",
      "|    total_timesteps | 25922 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=269.80 +/- 41.28\n",
      "Episode length: 269.80 +/- 41.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | 270         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008701485 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.369      |\n",
      "|    explained_variance   | -26.4       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.00156     |\n",
      "|    value_loss           | 0.000105    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=288.20 +/- 37.03\n",
      "Episode length: 288.20 +/- 37.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 288      |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 493   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 26919 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=318.40 +/- 44.28\n",
      "Episode length: 318.40 +/- 44.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 318        |\n",
      "|    mean_reward          | 318        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01598242 |\n",
      "|    clip_fraction        | 0.0727     |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.428     |\n",
      "|    explained_variance   | -108       |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | 0.0139     |\n",
      "|    n_updates            | 81         |\n",
      "|    policy_gradient_loss | 0.00448    |\n",
      "|    value_loss           | 0.000369   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=301.60 +/- 30.18\n",
      "Episode length: 301.60 +/- 30.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 302      |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 27916 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=296.80 +/- 27.47\n",
      "Episode length: 296.80 +/- 27.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 297         |\n",
      "|    mean_reward          | 297         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011778725 |\n",
      "|    clip_fraction        | 0.082       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | -59.2       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.00435     |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | 0.00275     |\n",
      "|    value_loss           | 7.79e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=296.80 +/- 39.71\n",
      "Episode length: 296.80 +/- 39.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 297      |\n",
      "|    mean_reward     | 297      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 499   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 28913 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=330.40 +/- 34.34\n",
      "Episode length: 330.40 +/- 34.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 330         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013244494 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.0146      |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.992       |\n",
      "|    n_updates            | 87          |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    value_loss           | 0.529       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=331.00 +/- 41.20\n",
      "Episode length: 331.00 +/- 41.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 331      |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 501   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 29910 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=311.20 +/- 35.50\n",
      "Episode length: 311.20 +/- 35.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 311         |\n",
      "|    mean_reward          | 311         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013075698 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.309       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | -0.000821   |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00121    |\n",
      "|    value_loss           | 0.798       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=327.20 +/- 63.70\n",
      "Episode length: 327.20 +/- 63.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 327      |\n",
      "|    mean_reward     | 327      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 30907 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=320.80 +/- 31.03\n",
      "Episode length: 320.80 +/- 31.03\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 321          |\n",
      "|    mean_reward          | 321          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029050852 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.289        |\n",
      "|    entropy_loss         | -0.445       |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.00559      |\n",
      "|    loss                 | 0.0534       |\n",
      "|    n_updates            | 93           |\n",
      "|    policy_gradient_loss | -0.0058      |\n",
      "|    value_loss           | 0.155        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=305.60 +/- 31.17\n",
      "Episode length: 305.60 +/- 31.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 306      |\n",
      "|    mean_reward     | 306      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 506   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 31904 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=288.60 +/- 28.65\n",
      "Episode length: 288.60 +/- 28.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 289          |\n",
      "|    mean_reward          | 289          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017718138 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.289        |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | 0.929        |\n",
      "|    learning_rate        | 0.00559      |\n",
      "|    loss                 | 0.0414       |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | 0.0036       |\n",
      "|    value_loss           | 0.26         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=286.40 +/- 51.75\n",
      "Episode length: 286.40 +/- 51.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 286      |\n",
      "|    mean_reward     | 286      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 509   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 32901 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=286.80 +/- 20.96\n",
      "Episode length: 286.80 +/- 20.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 287          |\n",
      "|    mean_reward          | 287          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035327394 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.289        |\n",
      "|    entropy_loss         | -0.444       |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.00559      |\n",
      "|    loss                 | 0.0713       |\n",
      "|    n_updates            | 99           |\n",
      "|    policy_gradient_loss | 0.000182     |\n",
      "|    value_loss           | 0.145        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=301.40 +/- 36.70\n",
      "Episode length: 301.40 +/- 36.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 301      |\n",
      "|    mean_reward     | 301      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 33898 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=261.20 +/- 18.20\n",
      "Episode length: 261.20 +/- 18.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 261         |\n",
      "|    mean_reward          | 261         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022798892 |\n",
      "|    clip_fraction        | 0.0809      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | 0.00666     |\n",
      "|    value_loss           | 0.0681      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=279.00 +/- 33.56\n",
      "Episode length: 279.00 +/- 33.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 279      |\n",
      "|    mean_reward     | 279      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 514   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 34895 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=282.80 +/- 11.09\n",
      "Episode length: 282.80 +/- 11.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 283          |\n",
      "|    mean_reward          | 283          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032915755 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.289        |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00559      |\n",
      "|    loss                 | 0.0097       |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | 0.00271      |\n",
      "|    value_loss           | 0.031        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=304.60 +/- 52.21\n",
      "Episode length: 304.60 +/- 52.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 305      |\n",
      "|    mean_reward     | 305      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 69    |\n",
      "|    total_timesteps | 35892 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=354.40 +/- 73.27\n",
      "Episode length: 354.40 +/- 73.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 354         |\n",
      "|    mean_reward          | 354         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019006077 |\n",
      "|    clip_fraction        | 0.0717      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | -0.00473    |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    value_loss           | 0.0449      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=400.80 +/- 68.91\n",
      "Episode length: 400.80 +/- 68.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 401      |\n",
      "|    mean_reward     | 401      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 517   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 36889 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=398.00 +/- 90.05\n",
      "Episode length: 398.00 +/- 90.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 398         |\n",
      "|    mean_reward          | 398         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011731446 |\n",
      "|    clip_fraction        | 0.0764      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 111         |\n",
      "|    policy_gradient_loss | 0.00625     |\n",
      "|    value_loss           | 0.0578      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=434.20 +/- 80.64\n",
      "Episode length: 434.20 +/- 80.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 434      |\n",
      "|    mean_reward     | 434      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 37886 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=471.80 +/- 56.40\n",
      "Episode length: 471.80 +/- 56.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 472         |\n",
      "|    mean_reward          | 472         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013221655 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.00423     |\n",
      "|    n_updates            | 114         |\n",
      "|    policy_gradient_loss | 0.000753    |\n",
      "|    value_loss           | 0.0533      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 513   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 38883 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009670049 |\n",
      "|    clip_fraction        | 0.0543      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.00036     |\n",
      "|    n_updates            | 117         |\n",
      "|    policy_gradient_loss | 0.00747     |\n",
      "|    value_loss           | 0.0342      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 39880 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01340094 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.39      |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.00902    |\n",
      "|    value_loss           | 0.0413     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 508   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 40877 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012446232 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | -0.0593     |\n",
      "|    n_updates            | 123         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 0.0478      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=496.60 +/- 6.80\n",
      "Episode length: 496.60 +/- 6.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 497      |\n",
      "|    mean_reward     | 497      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 506   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 41874 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017805303 |\n",
      "|    clip_fraction        | 0.052       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.352      |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.00518     |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | 0.00158     |\n",
      "|    value_loss           | 0.0394      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 42871 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 43000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008079022 |\n",
      "|    clip_fraction        | 0.0569      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.344      |\n",
      "|    explained_variance   | -0.303      |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0436      |\n",
      "|    n_updates            | 129         |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 0.656       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 498   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 87    |\n",
      "|    total_timesteps | 43868 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 44000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03258336 |\n",
      "|    clip_fraction        | 0.0369     |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.316     |\n",
      "|    explained_variance   | -40.5      |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | 0.00132    |\n",
      "|    n_updates            | 132        |\n",
      "|    policy_gradient_loss | -0.000166  |\n",
      "|    value_loss           | 9.86e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 496   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 90    |\n",
      "|    total_timesteps | 44865 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=465.80 +/- 68.40\n",
      "Episode length: 465.80 +/- 68.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 466         |\n",
      "|    mean_reward          | 466         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030291613 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.47        |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 492   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 93    |\n",
      "|    total_timesteps | 45862 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=457.40 +/- 81.26\n",
      "Episode length: 457.40 +/- 81.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 457         |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039861567 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.798       |\n",
      "|    n_updates            | 138         |\n",
      "|    policy_gradient_loss | -0.00795    |\n",
      "|    value_loss           | 1.63        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=494.60 +/- 10.80\n",
      "Episode length: 494.60 +/- 10.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 495      |\n",
      "|    mean_reward     | 495      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 488   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 95    |\n",
      "|    total_timesteps | 46859 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=320.60 +/- 146.74\n",
      "Episode length: 320.60 +/- 146.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 321         |\n",
      "|    mean_reward          | 321         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035185654 |\n",
      "|    clip_fraction        | 0.0943      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 141         |\n",
      "|    policy_gradient_loss | 0.00393     |\n",
      "|    value_loss           | 0.146       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=264.00 +/- 118.11\n",
      "Episode length: 264.00 +/- 118.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 264      |\n",
      "|    mean_reward     | 264      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 488   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 47856 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=365.80 +/- 157.83\n",
      "Episode length: 365.80 +/- 157.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 366         |\n",
      "|    mean_reward          | 366         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011024257 |\n",
      "|    clip_fraction        | 0.0548      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | -0.00463    |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | 0.000178    |\n",
      "|    value_loss           | 0.054       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=291.60 +/- 150.59\n",
      "Episode length: 291.60 +/- 150.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 292      |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 488   |\n",
      "|    iterations      | 49    |\n",
      "|    time_elapsed    | 100   |\n",
      "|    total_timesteps | 48853 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=243.40 +/- 129.66\n",
      "Episode length: 243.40 +/- 129.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 243          |\n",
      "|    mean_reward          | 243          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077563147 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.289        |\n",
      "|    entropy_loss         | -0.37        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00559      |\n",
      "|    loss                 | 0.00632      |\n",
      "|    n_updates            | 147          |\n",
      "|    policy_gradient_loss | 0.00466      |\n",
      "|    value_loss           | 0.0125       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=262.00 +/- 127.63\n",
      "Episode length: 262.00 +/- 127.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 262      |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 490   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 101   |\n",
      "|    total_timesteps | 49850 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=212.20 +/- 130.44\n",
      "Episode length: 212.20 +/- 130.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 212         |\n",
      "|    mean_reward          | 212         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005562982 |\n",
      "|    clip_fraction        | 0.0908      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.311      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.258       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00513     |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=253.60 +/- 106.69\n",
      "Episode length: 253.60 +/- 106.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 254      |\n",
      "|    mean_reward     | 254      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 493   |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 50847 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=228.60 +/- 135.95\n",
      "Episode length: 228.60 +/- 135.95\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 229         |\n",
      "|    mean_reward          | 229         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028532768 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.315      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 153         |\n",
      "|    policy_gradient_loss | 0.00285     |\n",
      "|    value_loss           | 0.417       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=158.40 +/- 14.29\n",
      "Episode length: 158.40 +/- 14.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 158      |\n",
      "|    mean_reward     | 158      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 496   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 104   |\n",
      "|    total_timesteps | 51844 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=285.00 +/- 161.18\n",
      "Episode length: 285.00 +/- 161.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 285        |\n",
      "|    mean_reward          | 285        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 52000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01767888 |\n",
      "|    clip_fraction        | 0.0637     |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.346     |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.00559    |\n",
      "|    loss                 | 0.0774     |\n",
      "|    n_updates            | 156        |\n",
      "|    policy_gradient_loss | 0.000622   |\n",
      "|    value_loss           | 0.0671     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=374.20 +/- 156.40\n",
      "Episode length: 374.20 +/- 156.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 496   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 106   |\n",
      "|    total_timesteps | 52841 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=372.40 +/- 156.53\n",
      "Episode length: 372.40 +/- 156.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 372         |\n",
      "|    mean_reward          | 372         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020233382 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 159         |\n",
      "|    policy_gradient_loss | -0.00747    |\n",
      "|    value_loss           | 0.212       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=298.20 +/- 164.79\n",
      "Episode length: 298.20 +/- 164.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 298      |\n",
      "|    mean_reward     | 298      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 497   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 108   |\n",
      "|    total_timesteps | 53838 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=250.00 +/- 126.56\n",
      "Episode length: 250.00 +/- 126.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 250         |\n",
      "|    mean_reward          | 250         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008939324 |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.28       |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.00559     |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 162         |\n",
      "|    policy_gradient_loss | 0.00377     |\n",
      "|    value_loss           | 0.0363      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=313.20 +/- 153.45\n",
      "Episode length: 313.20 +/- 153.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 313      |\n",
      "|    mean_reward     | 313      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 498   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 110   |\n",
      "|    total_timesteps | 54835 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=297.40 +/- 146.55\n",
      "Episode length: 297.40 +/- 146.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 297          |\n",
      "|    mean_reward          | 297          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 55000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151739735 |\n",
      "|    clip_fraction        | 0.0623       |\n",
      "|    clip_range           | 0.289        |\n",
      "|    entropy_loss         | -0.355       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00559      |\n",
      "|    loss                 | 0.0062       |\n",
      "|    n_updates            | 165          |\n",
      "|    policy_gradient_loss | 0.0077       |\n",
      "|    value_loss           | 0.0523       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=433.80 +/- 132.40\n",
      "Episode length: 433.80 +/- 132.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 434      |\n",
      "|    mean_reward     | 434      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 497   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 112   |\n",
      "|    total_timesteps | 55832 |\n",
      "------------------------------\n",
      "Single environment training took 112.30 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>417.3</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-4</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/izrwlfhc' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/izrwlfhc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_132113-izrwlfhc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 384yu8dq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2697110407148269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.008182937435302151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9884067073825544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9851558093610108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005911299597872144\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 9.629033075280216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 684\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 16671\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_132318-384yu8dq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/384yu8dq' target=\"_blank\">quiet-sweep-5</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/384yu8dq' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/384yu8dq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 684`, after every 10 untruncated mini-batches, there will be a truncated mini-batch of size 44\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=684 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=279.80 +/- 103.05\n",
      "Episode length: 279.80 +/- 103.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 786 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 0   |\n",
      "|    total_timesteps | 684 |\n",
      "----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=428.60 +/- 142.80\n",
      "Episode length: 428.60 +/- 142.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 429        |\n",
      "|    mean_reward          | 429        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03761311 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.27       |\n",
      "|    entropy_loss         | -0.668     |\n",
      "|    explained_variance   | -0.0206    |\n",
      "|    learning_rate        | 0.00591    |\n",
      "|    loss                 | 17.5       |\n",
      "|    n_updates            | 7          |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    value_loss           | 72.2       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 621  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 1368 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=379.00 +/- 64.73\n",
      "Episode length: 379.00 +/- 64.73\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 379        |\n",
      "|    mean_reward          | 379        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03181901 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.27       |\n",
      "|    entropy_loss         | -0.606     |\n",
      "|    explained_variance   | 0.145      |\n",
      "|    learning_rate        | 0.00591    |\n",
      "|    loss                 | 23.1       |\n",
      "|    n_updates            | 14         |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 63.3       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=473.60 +/- 52.30\n",
      "Episode length: 473.60 +/- 52.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 474      |\n",
      "|    mean_reward     | 474      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 483  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2052 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013044214 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 61.6        |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    value_loss           | 184         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 483  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2736 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=465.00 +/- 57.04\n",
      "Episode length: 465.00 +/- 57.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 465         |\n",
      "|    mean_reward          | 465         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016631078 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.523      |\n",
      "|    explained_variance   | -0.315      |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 9.17        |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | 0.000814    |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 489  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 3420 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=471.80 +/- 37.60\n",
      "Episode length: 471.80 +/- 37.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 472         |\n",
      "|    mean_reward          | 472         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018212331 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.289       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 9.08        |\n",
      "|    n_updates            | 35          |\n",
      "|    policy_gradient_loss | 0.00467     |\n",
      "|    value_loss           | 59.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 445  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4104 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=479.40 +/- 25.39\n",
      "Episode length: 479.40 +/- 25.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 479          |\n",
      "|    mean_reward          | 479          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090816375 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.27         |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.352        |\n",
      "|    learning_rate        | 0.00591      |\n",
      "|    loss                 | 23.1         |\n",
      "|    n_updates            | 42           |\n",
      "|    policy_gradient_loss | 0.000959     |\n",
      "|    value_loss           | 44.4         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 447  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 4788 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=468.20 +/- 63.60\n",
      "Episode length: 468.20 +/- 63.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 468         |\n",
      "|    mean_reward          | 468         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017001493 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 1.28        |\n",
      "|    n_updates            | 49          |\n",
      "|    policy_gradient_loss | 0.00836     |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 444  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 5472 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014632825 |\n",
      "|    clip_fraction        | 0.0945      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.00872     |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 9.39        |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.00456    |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=485.60 +/- 28.80\n",
      "Episode length: 485.60 +/- 28.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 486      |\n",
      "|    mean_reward     | 486      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 420  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 6156 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017281167 |\n",
      "|    clip_fraction        | 0.0672      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.861       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 0.53        |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | -0.00714    |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 418  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 6840 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=419.80 +/- 115.31\n",
      "Episode length: 419.80 +/- 115.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 420         |\n",
      "|    mean_reward          | 420         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011000645 |\n",
      "|    clip_fraction        | 0.0782      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.723       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 2.02        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    value_loss           | 23.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=443.40 +/- 113.20\n",
      "Episode length: 443.40 +/- 113.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 443      |\n",
      "|    mean_reward     | 443      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 408  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 18   |\n",
      "|    total_timesteps | 7524 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=469.80 +/- 60.40\n",
      "Episode length: 469.80 +/- 60.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 470         |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007109809 |\n",
      "|    clip_fraction        | 0.0639      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 77          |\n",
      "|    policy_gradient_loss | -0.00866    |\n",
      "|    value_loss           | 6           |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 413  |\n",
      "|    iterations      | 12   |\n",
      "|    time_elapsed    | 19   |\n",
      "|    total_timesteps | 8208 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011558873 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.00636    |\n",
      "|    value_loss           | 5.92        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 418  |\n",
      "|    iterations      | 13   |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 8892 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006142196 |\n",
      "|    clip_fraction        | 0.0732      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 7.46        |\n",
      "|    n_updates            | 91          |\n",
      "|    policy_gradient_loss | -0.00769    |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 399  |\n",
      "|    iterations      | 14   |\n",
      "|    time_elapsed    | 23   |\n",
      "|    total_timesteps | 9576 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=483.20 +/- 33.60\n",
      "Episode length: 483.20 +/- 33.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022113128 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 4.03        |\n",
      "|    n_updates            | 98          |\n",
      "|    policy_gradient_loss | -0.00547    |\n",
      "|    value_loss           | 20.9        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 403   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 10260 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024331719 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | -0.00549    |\n",
      "|    value_loss           | 8.78        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 10944 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=467.00 +/- 66.00\n",
      "Episode length: 467.00 +/- 66.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 467         |\n",
      "|    mean_reward          | 467         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014440878 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 19.1        |\n",
      "|    n_updates            | 112         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 55.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 398   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 11628 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011351448 |\n",
      "|    clip_fraction        | 0.061       |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.0176      |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 119         |\n",
      "|    policy_gradient_loss | -0.00639    |\n",
      "|    value_loss           | 36.2        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 400   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 12312 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=263.80 +/- 22.01\n",
      "Episode length: 263.80 +/- 22.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 264         |\n",
      "|    mean_reward          | 264         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012182379 |\n",
      "|    clip_fraction        | 0.0879      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | -0.00454    |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 408   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 12996 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=448.20 +/- 53.40\n",
      "Episode length: 448.20 +/- 53.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 448         |\n",
      "|    mean_reward          | 448         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009595086 |\n",
      "|    clip_fraction        | 0.0694      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 6.61        |\n",
      "|    n_updates            | 133         |\n",
      "|    policy_gradient_loss | -0.0055     |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=382.20 +/- 52.02\n",
      "Episode length: 382.20 +/- 52.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 382      |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 404   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 13680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010579312 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 0.161       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 14364 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066864234 |\n",
      "|    clip_fraction        | 0.0673       |\n",
      "|    clip_range           | 0.27         |\n",
      "|    entropy_loss         | -0.423       |\n",
      "|    explained_variance   | -1.81        |\n",
      "|    learning_rate        | 0.00591      |\n",
      "|    loss                 | 0.172        |\n",
      "|    n_updates            | 147          |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    value_loss           | 0.984        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 400   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 15048 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037286803 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.27         |\n",
      "|    entropy_loss         | -0.399       |\n",
      "|    explained_variance   | -0.0651      |\n",
      "|    learning_rate        | 0.00591      |\n",
      "|    loss                 | 0.0539       |\n",
      "|    n_updates            | 154          |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    value_loss           | 0.215        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 401   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 15732 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016774334 |\n",
      "|    clip_fraction        | 0.0985      |\n",
      "|    clip_range           | 0.27        |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | -7.05       |\n",
      "|    learning_rate        | 0.00591     |\n",
      "|    loss                 | 0.00718     |\n",
      "|    n_updates            | 161         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 401   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 16416 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071313498 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.27         |\n",
      "|    entropy_loss         | -0.331       |\n",
      "|    explained_variance   | -2.65        |\n",
      "|    learning_rate        | 0.00591      |\n",
      "|    loss                 | 0.0237       |\n",
      "|    n_updates            | 168          |\n",
      "|    policy_gradient_loss | -0.00468     |\n",
      "|    value_loss           | 0.144        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 395   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 17100 |\n",
      "------------------------------\n",
      "Single environment training took 43.49 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">quiet-sweep-5</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/384yu8dq' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/384yu8dq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_132318-384yu8dq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: socqhpcv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.28807872877562746\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.002661551855965269\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.905870808126836\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9644004062571392\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0035659444257926266\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 6.450301574876193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1132\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 35969\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_132414-socqhpcv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/socqhpcv' target=\"_blank\">toasty-sweep-6</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/socqhpcv' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/socqhpcv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1132`, after every 17 untruncated mini-batches, there will be a truncated mini-batch of size 44\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1132 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=9.80 +/- 0.98\n",
      "Episode length: 9.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 9.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=9.80 +/- 0.40\n",
      "Episode length: 9.80 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 9.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1819 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1132 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=487.80 +/- 24.40\n",
      "Episode length: 487.80 +/- 24.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 488        |\n",
      "|    mean_reward          | 488        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01986767 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.288      |\n",
      "|    entropy_loss         | -0.674     |\n",
      "|    explained_variance   | -0.008     |\n",
      "|    learning_rate        | 0.00357    |\n",
      "|    loss                 | 1.4        |\n",
      "|    n_updates            | 4          |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    value_loss           | 9.95       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=482.60 +/- 34.80\n",
      "Episode length: 482.60 +/- 34.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 483      |\n",
      "|    mean_reward     | 483      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 730  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2264 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=349.60 +/- 106.99\n",
      "Episode length: 349.60 +/- 106.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 350         |\n",
      "|    mean_reward          | 350         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037500568 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.0506     |\n",
      "|    value_loss           | 6.88        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=371.40 +/- 157.53\n",
      "Episode length: 371.40 +/- 157.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 371      |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 649  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 3396 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=388.00 +/- 140.77\n",
      "Episode length: 388.00 +/- 140.77\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 388         |\n",
      "|    mean_reward          | 388         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055278763 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.646       |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=161.00 +/- 16.41\n",
      "Episode length: 161.00 +/- 16.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 161      |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=172.00 +/- 47.61\n",
      "Episode length: 172.00 +/- 47.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 172      |\n",
      "|    mean_reward     | 172      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 602  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 4528 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=233.00 +/- 52.79\n",
      "Episode length: 233.00 +/- 52.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 233         |\n",
      "|    mean_reward          | 233         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019845294 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.478      |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=177.40 +/- 12.64\n",
      "Episode length: 177.40 +/- 12.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 177      |\n",
      "|    mean_reward     | 177      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 623  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 5660 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=376.40 +/- 104.60\n",
      "Episode length: 376.40 +/- 104.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 376         |\n",
      "|    mean_reward          | 376         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031945795 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.644       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 1.67        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=339.00 +/- 90.64\n",
      "Episode length: 339.00 +/- 90.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 339      |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 606  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 6792 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=437.00 +/- 81.71\n",
      "Episode length: 437.00 +/- 81.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 437         |\n",
      "|    mean_reward          | 437         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016793702 |\n",
      "|    clip_fraction        | 0.0746      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.158       |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    value_loss           | 1.37        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 579  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 7924 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017112447 |\n",
      "|    clip_fraction        | 0.0624      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.355       |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.000972   |\n",
      "|    value_loss           | 0.927       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=456.20 +/- 87.60\n",
      "Episode length: 456.20 +/- 87.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 456      |\n",
      "|    mean_reward     | 456      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=419.20 +/- 98.96\n",
      "Episode length: 419.20 +/- 98.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 419      |\n",
      "|    mean_reward     | 419      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 534  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 9056 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=452.20 +/- 95.60\n",
      "Episode length: 452.20 +/- 95.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 452         |\n",
      "|    mean_reward          | 452         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010789769 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.0303      |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    value_loss           | 0.471       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=406.00 +/- 115.24\n",
      "Episode length: 406.00 +/- 115.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 406      |\n",
      "|    mean_reward     | 406      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 517   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 10188 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=396.00 +/- 110.24\n",
      "Episode length: 396.00 +/- 110.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 396         |\n",
      "|    mean_reward          | 396         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004414009 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.202       |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.00832     |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=427.00 +/- 83.11\n",
      "Episode length: 427.00 +/- 83.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 427      |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 506   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 11320 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=429.60 +/- 89.02\n",
      "Episode length: 429.60 +/- 89.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 430         |\n",
      "|    mean_reward          | 430         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011337633 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.729       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.441       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000558   |\n",
      "|    value_loss           | 1.22        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=412.40 +/- 114.45\n",
      "Episode length: 412.40 +/- 114.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 412      |\n",
      "|    mean_reward     | 412      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 12452 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005315009 |\n",
      "|    clip_fraction        | 0.0381      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.115       |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    value_loss           | 0.497       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=451.00 +/- 60.02\n",
      "Episode length: 451.00 +/- 60.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 451      |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=481.20 +/- 37.60\n",
      "Episode length: 481.20 +/- 37.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 481      |\n",
      "|    mean_reward     | 481      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 13584 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012367755 |\n",
      "|    clip_fraction        | 0.0629      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00503     |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 472   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 14716 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010341798 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.841       |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 0.62        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 15848 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02637572 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.288      |\n",
      "|    entropy_loss         | -0.412     |\n",
      "|    explained_variance   | 0.768      |\n",
      "|    learning_rate        | 0.00357    |\n",
      "|    loss                 | 0.00598    |\n",
      "|    n_updates            | 56         |\n",
      "|    policy_gradient_loss | -0.00606   |\n",
      "|    value_loss           | 0.0356     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 464   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 16980 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005036654 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00401     |\n",
      "|    value_loss           | 0.0417      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 453   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 18112 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008890705 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.768       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00625     |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    value_loss           | 0.0112      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 451   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 19244 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078106117 |\n",
      "|    clip_fraction        | 0.0565       |\n",
      "|    clip_range           | 0.288        |\n",
      "|    entropy_loss         | -0.378       |\n",
      "|    explained_variance   | 0.702        |\n",
      "|    learning_rate        | 0.00357      |\n",
      "|    loss                 | 0.00298      |\n",
      "|    n_updates            | 68           |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 0.00183      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 446   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 20376 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003597332 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.00311    |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.00334     |\n",
      "|    value_loss           | 0.00962     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 434   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 21508 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005601166 |\n",
      "|    clip_fraction        | 0.0385      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00584     |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | -0.000826   |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 435   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 22640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01459906 |\n",
      "|    clip_fraction        | 0.082      |\n",
      "|    clip_range           | 0.288      |\n",
      "|    entropy_loss         | -0.363     |\n",
      "|    explained_variance   | 0.0543     |\n",
      "|    learning_rate        | 0.00357    |\n",
      "|    loss                 | 1.2        |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | 0.000478   |\n",
      "|    value_loss           | 2.69       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 436   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 23772 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017066201 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | 0.00061     |\n",
      "|    value_loss           | 0.00903     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 435   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 24904 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013060527 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.00865    |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | 0.00118     |\n",
      "|    value_loss           | 0.00184     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 428   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 26036 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008264734 |\n",
      "|    clip_fraction        | 0.0643      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.392       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00989     |\n",
      "|    n_updates            | 92          |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 0.00067     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 426   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 27168 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014090346 |\n",
      "|    clip_fraction        | 0.0458      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.00126    |\n",
      "|    value_loss           | 0.000861    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 28300 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013647881 |\n",
      "|    clip_fraction        | 0.0601      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.334      |\n",
      "|    explained_variance   | 0.0754      |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | 0.00138     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000578   |\n",
      "|    value_loss           | 0.000176    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 419   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 29432 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020477375 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.311      |\n",
      "|    explained_variance   | 0.45        |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.00836    |\n",
      "|    n_updates            | 104         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    value_loss           | 8.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 413   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 30564 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008076635 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.14        |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.00813    |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | 0.00048     |\n",
      "|    value_loss           | 4.35e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 413   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 31696 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019800281 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.288       |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.00357     |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 112         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    value_loss           | 2.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 413   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 32828 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 33000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04177338 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.288      |\n",
      "|    entropy_loss         | -0.291     |\n",
      "|    explained_variance   | 0.15       |\n",
      "|    learning_rate        | 0.00357    |\n",
      "|    loss                 | 0.0561     |\n",
      "|    n_updates            | 116        |\n",
      "|    policy_gradient_loss | 0.00174    |\n",
      "|    value_loss           | 1.14e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 414   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 33960 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 34000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02441314 |\n",
      "|    clip_fraction        | 0.1        |\n",
      "|    clip_range           | 0.288      |\n",
      "|    entropy_loss         | -0.266     |\n",
      "|    explained_variance   | 0.537      |\n",
      "|    learning_rate        | 0.00357    |\n",
      "|    loss                 | -0.02      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.00226    |\n",
      "|    value_loss           | 7.53e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 410   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 35092 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01951913 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.288      |\n",
      "|    entropy_loss         | -0.237     |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.00357    |\n",
      "|    loss                 | -0.0228    |\n",
      "|    n_updates            | 124        |\n",
      "|    policy_gradient_loss | -0.00244   |\n",
      "|    value_loss           | 3.2e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 411   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 88    |\n",
      "|    total_timesteps | 36224 |\n",
      "------------------------------\n",
      "Single environment training took 88.29 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-sweep-6</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/socqhpcv' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/socqhpcv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_132414-socqhpcv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k1uhhekk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.1299608663676622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.00969745839950704\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.93800791038971\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9113254580941896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00952214721600086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 1.362056748161388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1803\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 79321\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_132559-k1uhhekk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/k1uhhekk' target=\"_blank\">cosmic-sweep-7</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/k1uhhekk' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/k1uhhekk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1803`, after every 28 untruncated mini-batches, there will be a truncated mini-batch of size 11\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1803 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=8.40 +/- 0.80\n",
      "Episode length: 8.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.4      |\n",
      "|    mean_reward     | 8.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=8.80 +/- 0.40\n",
      "Episode length: 8.80 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 8.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=9.40 +/- 0.49\n",
      "Episode length: 9.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1772 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1803 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=182.20 +/- 31.22\n",
      "Episode length: 182.20 +/- 31.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 182          |\n",
      "|    mean_reward          | 182          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074157645 |\n",
      "|    clip_fraction        | 0.325        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.687       |\n",
      "|    explained_variance   | -0.00323     |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0172      |\n",
      "|    value_loss           | 1.45         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=163.80 +/- 12.11\n",
      "Episode length: 163.80 +/- 12.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 164      |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=168.00 +/- 12.31\n",
      "Episode length: 168.00 +/- 12.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 168      |\n",
      "|    mean_reward     | 168      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=168.80 +/- 9.74\n",
      "Episode length: 168.80 +/- 9.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 169      |\n",
      "|    mean_reward     | 169      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 865  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 3606 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=319.00 +/- 98.28\n",
      "Episode length: 319.00 +/- 98.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 319         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009368884 |\n",
      "|    clip_fraction        | 0.406       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.661      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.48        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0301     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=344.00 +/- 84.50\n",
      "Episode length: 344.00 +/- 84.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 344      |\n",
      "|    mean_reward     | 344      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=258.00 +/- 70.49\n",
      "Episode length: 258.00 +/- 70.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 258      |\n",
      "|    mean_reward     | 258      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 710  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 5409 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=335.60 +/- 59.61\n",
      "Episode length: 335.60 +/- 59.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 336         |\n",
      "|    mean_reward          | 336         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010767453 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.637       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.697       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 1.67        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=319.80 +/- 53.63\n",
      "Episode length: 319.80 +/- 53.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 320      |\n",
      "|    mean_reward     | 320      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=402.40 +/- 95.13\n",
      "Episode length: 402.40 +/- 95.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 402      |\n",
      "|    mean_reward     | 402      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=346.20 +/- 89.94\n",
      "Episode length: 346.20 +/- 89.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 346      |\n",
      "|    mean_reward     | 346      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 604  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 7212 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=375.00 +/- 109.97\n",
      "Episode length: 375.00 +/- 109.97\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 375          |\n",
      "|    mean_reward          | 375          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0142168235 |\n",
      "|    clip_fraction        | 0.402        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.62         |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | 0.323        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0162      |\n",
      "|    value_loss           | 0.954        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=335.20 +/- 68.99\n",
      "Episode length: 335.20 +/- 68.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 335      |\n",
      "|    mean_reward     | 335      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=413.40 +/- 81.26\n",
      "Episode length: 413.40 +/- 81.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 413      |\n",
      "|    mean_reward     | 413      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=367.00 +/- 64.81\n",
      "Episode length: 367.00 +/- 64.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 367      |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 544  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 9015 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=368.40 +/- 69.49\n",
      "Episode length: 368.40 +/- 69.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 368         |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009312703 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | -0.0963     |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0992      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00251     |\n",
      "|    value_loss           | 0.44        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=425.20 +/- 55.26\n",
      "Episode length: 425.20 +/- 55.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 425      |\n",
      "|    mean_reward     | 425      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10500, episode_reward=398.80 +/- 72.10\n",
      "Episode length: 398.80 +/- 72.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 399      |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 519   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 10818 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=284.20 +/- 47.82\n",
      "Episode length: 284.20 +/- 47.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 284         |\n",
      "|    mean_reward          | 284         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007879742 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000946   |\n",
      "|    value_loss           | 0.485       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=319.40 +/- 56.77\n",
      "Episode length: 319.40 +/- 56.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 319      |\n",
      "|    mean_reward     | 319      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=317.40 +/- 31.36\n",
      "Episode length: 317.40 +/- 31.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 317      |\n",
      "|    mean_reward     | 317      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=293.80 +/- 25.52\n",
      "Episode length: 293.80 +/- 25.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 294      |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 500   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 12621 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=233.20 +/- 15.92\n",
      "Episode length: 233.20 +/- 15.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 233          |\n",
      "|    mean_reward          | 233          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096152965 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.625        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | 0.0427       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | 0.00164      |\n",
      "|    value_loss           | 0.177        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=244.20 +/- 22.66\n",
      "Episode length: 244.20 +/- 22.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 244      |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=250.20 +/- 9.93\n",
      "Episode length: 250.20 +/- 9.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 250      |\n",
      "|    mean_reward     | 250      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 507   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 14424 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=228.20 +/- 11.16\n",
      "Episode length: 228.20 +/- 11.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 228         |\n",
      "|    mean_reward          | 228         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008299625 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0258      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00471     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=228.60 +/- 7.61\n",
      "Episode length: 228.60 +/- 7.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=243.60 +/- 17.23\n",
      "Episode length: 243.60 +/- 17.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 244      |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=235.60 +/- 9.97\n",
      "Episode length: 235.60 +/- 9.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 16227 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=250.20 +/- 5.31\n",
      "Episode length: 250.20 +/- 5.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 250        |\n",
      "|    mean_reward          | 250        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00965363 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.13       |\n",
      "|    entropy_loss         | -0.571     |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.00952    |\n",
      "|    loss                 | 0.0448     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.00215    |\n",
      "|    value_loss           | 0.201      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=255.40 +/- 2.87\n",
      "Episode length: 255.40 +/- 2.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 255      |\n",
      "|    mean_reward     | 255      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=253.60 +/- 7.68\n",
      "Episode length: 253.60 +/- 7.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 254      |\n",
      "|    mean_reward     | 254      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=248.40 +/- 7.71\n",
      "Episode length: 248.40 +/- 7.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 248      |\n",
      "|    mean_reward     | 248      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 18030 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=246.00 +/- 10.18\n",
      "Episode length: 246.00 +/- 10.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 246         |\n",
      "|    mean_reward          | 246         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007273173 |\n",
      "|    clip_fraction        | 0.347       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.826       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=243.00 +/- 7.85\n",
      "Episode length: 243.00 +/- 7.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 243      |\n",
      "|    mean_reward     | 243      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=239.40 +/- 10.89\n",
      "Episode length: 239.40 +/- 10.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 239      |\n",
      "|    mean_reward     | 239      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 19833 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=268.60 +/- 14.04\n",
      "Episode length: 268.60 +/- 14.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 269         |\n",
      "|    mean_reward          | 269         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009526749 |\n",
      "|    clip_fraction        | 0.358       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 1.48        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=296.00 +/- 22.61\n",
      "Episode length: 296.00 +/- 22.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=286.40 +/- 18.87\n",
      "Episode length: 286.40 +/- 18.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 286      |\n",
      "|    mean_reward     | 286      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=281.80 +/- 16.34\n",
      "Episode length: 281.80 +/- 16.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 282      |\n",
      "|    mean_reward     | 282      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 507   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 21636 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=429.80 +/- 80.39\n",
      "Episode length: 429.80 +/- 80.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 430         |\n",
      "|    mean_reward          | 430         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007719259 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0613      |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 0.825       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22500, episode_reward=436.00 +/- 87.21\n",
      "Episode length: 436.00 +/- 87.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 436      |\n",
      "|    mean_reward     | 436      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=23000, episode_reward=405.20 +/- 51.24\n",
      "Episode length: 405.20 +/- 51.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 405      |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 502   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 23439 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=277.60 +/- 58.44\n",
      "Episode length: 277.60 +/- 58.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 278         |\n",
      "|    mean_reward          | 278         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008936335 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0237      |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 0.637       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=279.60 +/- 53.00\n",
      "Episode length: 279.60 +/- 53.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=264.40 +/- 21.04\n",
      "Episode length: 264.40 +/- 21.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 264      |\n",
      "|    mean_reward     | 264      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=294.40 +/- 73.51\n",
      "Episode length: 294.40 +/- 73.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 294      |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 500   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 25242 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=351.80 +/- 83.51\n",
      "Episode length: 351.80 +/- 83.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | 352         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008106785 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00359    |\n",
      "|    value_loss           | 0.442       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=389.00 +/- 92.73\n",
      "Episode length: 389.00 +/- 92.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 389      |\n",
      "|    mean_reward     | 389      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=340.80 +/- 57.08\n",
      "Episode length: 340.80 +/- 57.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 341      |\n",
      "|    mean_reward     | 341      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=350.80 +/- 101.98\n",
      "Episode length: 350.80 +/- 101.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 351      |\n",
      "|    mean_reward     | 351      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 491   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 27045 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=358.40 +/- 86.06\n",
      "Episode length: 358.40 +/- 86.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 358         |\n",
      "|    mean_reward          | 358         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016943987 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.00294     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.000732   |\n",
      "|    value_loss           | 0.092       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=440.40 +/- 95.35\n",
      "Episode length: 440.40 +/- 95.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 440      |\n",
      "|    mean_reward     | 440      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28500, episode_reward=470.20 +/- 59.60\n",
      "Episode length: 470.20 +/- 59.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 470      |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 28848 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=321.40 +/- 18.18\n",
      "Episode length: 321.40 +/- 18.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 321         |\n",
      "|    mean_reward          | 321         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005505254 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.00738     |\n",
      "|    value_loss           | 0.088       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=311.60 +/- 54.26\n",
      "Episode length: 311.60 +/- 54.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 312      |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=285.40 +/- 51.15\n",
      "Episode length: 285.40 +/- 51.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 285      |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=377.00 +/- 67.41\n",
      "Episode length: 377.00 +/- 67.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 30651 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=374.00 +/- 70.55\n",
      "Episode length: 374.00 +/- 70.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 374         |\n",
      "|    mean_reward          | 374         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007628183 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.00568     |\n",
      "|    value_loss           | 0.0411      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=301.40 +/- 40.44\n",
      "Episode length: 301.40 +/- 40.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 301      |\n",
      "|    mean_reward     | 301      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=354.00 +/- 63.80\n",
      "Episode length: 354.00 +/- 63.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 354      |\n",
      "|    mean_reward     | 354      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 32454 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=365.80 +/- 86.91\n",
      "Episode length: 365.80 +/- 86.91\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 366          |\n",
      "|    mean_reward          | 366          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076729944 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.571       |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | -0.0424      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.00141      |\n",
      "|    value_loss           | 0.22         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=400.40 +/- 89.72\n",
      "Episode length: 400.40 +/- 89.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 400      |\n",
      "|    mean_reward     | 400      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=388.40 +/- 68.07\n",
      "Episode length: 388.40 +/- 68.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 388      |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=334.00 +/- 85.72\n",
      "Episode length: 334.00 +/- 85.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 334      |\n",
      "|    mean_reward     | 334      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 34257 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=248.00 +/- 24.58\n",
      "Episode length: 248.00 +/- 24.58\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 248          |\n",
      "|    mean_reward          | 248          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055712718 |\n",
      "|    clip_fraction        | 0.177        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    value_loss           | 0.0262       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=245.60 +/- 21.69\n",
      "Episode length: 245.60 +/- 21.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 246      |\n",
      "|    mean_reward     | 246      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=243.60 +/- 21.57\n",
      "Episode length: 243.60 +/- 21.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 244      |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=235.20 +/- 14.82\n",
      "Episode length: 235.20 +/- 14.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 235      |\n",
      "|    mean_reward     | 235      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 74    |\n",
      "|    total_timesteps | 36060 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=184.40 +/- 10.01\n",
      "Episode length: 184.40 +/- 10.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 184         |\n",
      "|    mean_reward          | 184         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013433345 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | 0.00681     |\n",
      "|    value_loss           | 0.0561      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=191.80 +/- 6.91\n",
      "Episode length: 191.80 +/- 6.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 192      |\n",
      "|    mean_reward     | 192      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=187.40 +/- 11.77\n",
      "Episode length: 187.40 +/- 11.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 187      |\n",
      "|    mean_reward     | 187      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 480   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 37863 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=192.60 +/- 11.25\n",
      "Episode length: 192.60 +/- 11.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 193          |\n",
      "|    mean_reward          | 193          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068170424 |\n",
      "|    clip_fraction        | 0.241        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | -0.00965     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | 0.00281      |\n",
      "|    value_loss           | 0.0311       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=192.80 +/- 4.17\n",
      "Episode length: 192.80 +/- 4.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 193      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=194.60 +/- 8.11\n",
      "Episode length: 194.60 +/- 8.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 195      |\n",
      "|    mean_reward     | 195      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=189.40 +/- 8.75\n",
      "Episode length: 189.40 +/- 8.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 189      |\n",
      "|    mean_reward     | 189      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 464   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 39666 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=228.20 +/- 11.37\n",
      "Episode length: 228.20 +/- 11.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 228         |\n",
      "|    mean_reward          | 228         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012961076 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.000172    |\n",
      "|    value_loss           | 0.278       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=225.40 +/- 16.61\n",
      "Episode length: 225.40 +/- 16.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 225      |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=236.20 +/- 12.11\n",
      "Episode length: 236.20 +/- 12.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 456   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 90    |\n",
      "|    total_timesteps | 41469 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=210.20 +/- 8.18\n",
      "Episode length: 210.20 +/- 8.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 210          |\n",
      "|    mean_reward          | 210          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091780545 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | 0.0034       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | 0.000863     |\n",
      "|    value_loss           | 0.0884       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=209.20 +/- 9.26\n",
      "Episode length: 209.20 +/- 9.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 209      |\n",
      "|    mean_reward     | 209      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=200.80 +/- 7.30\n",
      "Episode length: 200.80 +/- 7.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=206.20 +/- 7.36\n",
      "Episode length: 206.20 +/- 7.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 206      |\n",
      "|    mean_reward     | 206      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 451   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 95    |\n",
      "|    total_timesteps | 43272 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=203.80 +/- 15.92\n",
      "Episode length: 203.80 +/- 15.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 204        |\n",
      "|    mean_reward          | 204        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 43500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00641786 |\n",
      "|    clip_fraction        | 0.246      |\n",
      "|    clip_range           | 0.13       |\n",
      "|    entropy_loss         | -0.558     |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.00952    |\n",
      "|    loss                 | 0.0535     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | 0.00621    |\n",
      "|    value_loss           | 0.0233     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=210.00 +/- 10.95\n",
      "Episode length: 210.00 +/- 10.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 210      |\n",
      "|    mean_reward     | 210      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=212.20 +/- 6.62\n",
      "Episode length: 212.20 +/- 6.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 212      |\n",
      "|    mean_reward     | 212      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=212.00 +/- 6.72\n",
      "Episode length: 212.00 +/- 6.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 212      |\n",
      "|    mean_reward     | 212      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 454   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 45075 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=210.00 +/- 5.93\n",
      "Episode length: 210.00 +/- 5.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 210          |\n",
      "|    mean_reward          | 210          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 45500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077502113 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | -0.0482      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | 0.00403      |\n",
      "|    value_loss           | 0.028        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=199.40 +/- 11.91\n",
      "Episode length: 199.40 +/- 11.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 199      |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=203.20 +/- 4.83\n",
      "Episode length: 203.20 +/- 4.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 203      |\n",
      "|    mean_reward     | 203      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 458   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 102   |\n",
      "|    total_timesteps | 46878 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=189.80 +/- 4.31\n",
      "Episode length: 189.80 +/- 4.31\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 190          |\n",
      "|    mean_reward          | 190          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 47000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066257413 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | -0.0261      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.000569     |\n",
      "|    value_loss           | 0.016        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=195.80 +/- 8.30\n",
      "Episode length: 195.80 +/- 8.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 196      |\n",
      "|    mean_reward     | 196      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=198.60 +/- 12.24\n",
      "Episode length: 198.60 +/- 12.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 199      |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=193.20 +/- 9.09\n",
      "Episode length: 193.20 +/- 9.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 193      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 460   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 105   |\n",
      "|    total_timesteps | 48681 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=212.80 +/- 8.08\n",
      "Episode length: 212.80 +/- 8.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 213         |\n",
      "|    mean_reward          | 213         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 49000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005953415 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.034      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00658    |\n",
      "|    value_loss           | 0.527       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=210.80 +/- 9.00\n",
      "Episode length: 210.80 +/- 9.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 211      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=209.00 +/- 4.05\n",
      "Episode length: 209.00 +/- 4.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 209      |\n",
      "|    mean_reward     | 209      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 462   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 109   |\n",
      "|    total_timesteps | 50484 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=226.40 +/- 18.10\n",
      "Episode length: 226.40 +/- 18.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 226         |\n",
      "|    mean_reward          | 226         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018073555 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.0351     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00481    |\n",
      "|    value_loss           | 0.0288      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=225.60 +/- 17.93\n",
      "Episode length: 225.60 +/- 17.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 226      |\n",
      "|    mean_reward     | 226      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=235.80 +/- 22.92\n",
      "Episode length: 235.80 +/- 22.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=225.80 +/- 16.46\n",
      "Episode length: 225.80 +/- 16.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 226      |\n",
      "|    mean_reward     | 226      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 464   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 112   |\n",
      "|    total_timesteps | 52287 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=233.60 +/- 18.18\n",
      "Episode length: 233.60 +/- 18.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 234         |\n",
      "|    mean_reward          | 234         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009160419 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.000881   |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.000498   |\n",
      "|    value_loss           | 0.015       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=209.60 +/- 8.62\n",
      "Episode length: 209.60 +/- 8.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 210      |\n",
      "|    mean_reward     | 210      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=222.40 +/- 16.01\n",
      "Episode length: 222.40 +/- 16.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 222      |\n",
      "|    mean_reward     | 222      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=217.20 +/- 5.81\n",
      "Episode length: 217.20 +/- 5.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 217      |\n",
      "|    mean_reward     | 217      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 465   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 116   |\n",
      "|    total_timesteps | 54090 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=304.60 +/- 64.62\n",
      "Episode length: 304.60 +/- 64.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | 305          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0126652755 |\n",
      "|    clip_fraction        | 0.225        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.557       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | 0.0132       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | 0.0039       |\n",
      "|    value_loss           | 0.0164       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=377.20 +/- 74.13\n",
      "Episode length: 377.20 +/- 74.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=343.80 +/- 92.00\n",
      "Episode length: 343.80 +/- 92.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 344      |\n",
      "|    mean_reward     | 344      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 462   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 120   |\n",
      "|    total_timesteps | 55893 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=160.00 +/- 8.37\n",
      "Episode length: 160.00 +/- 8.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 160         |\n",
      "|    mean_reward          | 160         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011703084 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0339      |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.00466     |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=160.80 +/- 2.86\n",
      "Episode length: 160.80 +/- 2.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 161      |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=162.80 +/- 8.08\n",
      "Episode length: 162.80 +/- 8.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 163      |\n",
      "|    mean_reward     | 163      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=160.60 +/- 5.54\n",
      "Episode length: 160.60 +/- 5.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 161      |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 465   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 123   |\n",
      "|    total_timesteps | 57696 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=195.20 +/- 10.30\n",
      "Episode length: 195.20 +/- 10.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 195         |\n",
      "|    mean_reward          | 195         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007384116 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.000681   |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=180.80 +/- 5.27\n",
      "Episode length: 180.80 +/- 5.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 181      |\n",
      "|    mean_reward     | 181      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=188.80 +/- 7.70\n",
      "Episode length: 188.80 +/- 7.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 189      |\n",
      "|    mean_reward     | 189      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 467   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 127   |\n",
      "|    total_timesteps | 59499 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=213.00 +/- 15.66\n",
      "Episode length: 213.00 +/- 15.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 213          |\n",
      "|    mean_reward          | 213          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 59500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074232905 |\n",
      "|    clip_fraction        | 0.208        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | -0.0366      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0027      |\n",
      "|    value_loss           | 0.0696       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=210.60 +/- 16.80\n",
      "Episode length: 210.60 +/- 16.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 211      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=216.00 +/- 18.62\n",
      "Episode length: 216.00 +/- 18.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 216      |\n",
      "|    mean_reward     | 216      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=237.20 +/- 21.54\n",
      "Episode length: 237.20 +/- 21.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 237      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 131   |\n",
      "|    total_timesteps | 61302 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=197.20 +/- 10.42\n",
      "Episode length: 197.20 +/- 10.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 197         |\n",
      "|    mean_reward          | 197         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 61500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007371388 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.00378     |\n",
      "|    value_loss           | 0.0443      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=192.60 +/- 2.58\n",
      "Episode length: 192.60 +/- 2.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 193      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=195.20 +/- 6.31\n",
      "Episode length: 195.20 +/- 6.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 195      |\n",
      "|    mean_reward     | 195      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=191.60 +/- 6.18\n",
      "Episode length: 191.60 +/- 6.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 192      |\n",
      "|    mean_reward     | 192      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 135   |\n",
      "|    total_timesteps | 63105 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=238.20 +/- 17.53\n",
      "Episode length: 238.20 +/- 17.53\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 238          |\n",
      "|    mean_reward          | 238          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 63500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066964836 |\n",
      "|    clip_fraction        | 0.26         |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.566       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | 0.0221       |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | 0.00606      |\n",
      "|    value_loss           | 0.0162       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=225.60 +/- 8.01\n",
      "Episode length: 225.60 +/- 8.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 226      |\n",
      "|    mean_reward     | 226      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=227.60 +/- 10.71\n",
      "Episode length: 227.60 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | 228      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 467   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 138   |\n",
      "|    total_timesteps | 64908 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=293.00 +/- 18.93\n",
      "Episode length: 293.00 +/- 18.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 293          |\n",
      "|    mean_reward          | 293          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 65000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075604934 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.13         |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00952      |\n",
      "|    loss                 | 0.0427       |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | 0.00306      |\n",
      "|    value_loss           | 0.0144       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=279.60 +/- 22.18\n",
      "Episode length: 279.60 +/- 22.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=277.00 +/- 15.40\n",
      "Episode length: 277.00 +/- 15.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 277      |\n",
      "|    mean_reward     | 277      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=275.60 +/- 16.43\n",
      "Episode length: 275.60 +/- 16.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 276      |\n",
      "|    mean_reward     | 276      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 143   |\n",
      "|    total_timesteps | 66711 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=323.80 +/- 32.01\n",
      "Episode length: 323.80 +/- 32.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | 324         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 67000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004515961 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.00616     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.00698     |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=305.40 +/- 27.80\n",
      "Episode length: 305.40 +/- 27.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 305      |\n",
      "|    mean_reward     | 305      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=287.60 +/- 17.10\n",
      "Episode length: 287.60 +/- 17.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 288      |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=300.40 +/- 38.30\n",
      "Episode length: 300.40 +/- 38.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 300      |\n",
      "|    mean_reward     | 300      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 458   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 149   |\n",
      "|    total_timesteps | 68514 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=282.20 +/- 18.14\n",
      "Episode length: 282.20 +/- 18.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 282         |\n",
      "|    mean_reward          | 282         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 69000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010164545 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.00201     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    value_loss           | 0.16        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=272.60 +/- 33.63\n",
      "Episode length: 272.60 +/- 33.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 273      |\n",
      "|    mean_reward     | 273      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=281.20 +/- 26.36\n",
      "Episode length: 281.20 +/- 26.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 281      |\n",
      "|    mean_reward     | 281      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 453   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 155   |\n",
      "|    total_timesteps | 70317 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=216.20 +/- 7.98\n",
      "Episode length: 216.20 +/- 7.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 216         |\n",
      "|    mean_reward          | 216         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005886157 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.0783      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.00276     |\n",
      "|    value_loss           | 0.0561      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=210.00 +/- 8.94\n",
      "Episode length: 210.00 +/- 8.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 210      |\n",
      "|    mean_reward     | 210      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=215.80 +/- 5.98\n",
      "Episode length: 215.80 +/- 5.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 216      |\n",
      "|    mean_reward     | 216      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=217.00 +/- 5.06\n",
      "Episode length: 217.00 +/- 5.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 217      |\n",
      "|    mean_reward     | 217      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 453   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 159   |\n",
      "|    total_timesteps | 72120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=193.40 +/- 9.05\n",
      "Episode length: 193.40 +/- 9.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 193         |\n",
      "|    mean_reward          | 193         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009029932 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.00765     |\n",
      "|    value_loss           | 0.0272      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=199.80 +/- 12.42\n",
      "Episode length: 199.80 +/- 12.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=195.60 +/- 9.13\n",
      "Episode length: 195.60 +/- 9.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 196      |\n",
      "|    mean_reward     | 196      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 452   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 163   |\n",
      "|    total_timesteps | 73923 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=201.80 +/- 16.05\n",
      "Episode length: 201.80 +/- 16.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 202         |\n",
      "|    mean_reward          | 202         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008390368 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.0207     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.00176     |\n",
      "|    value_loss           | 0.0112      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=202.20 +/- 12.94\n",
      "Episode length: 202.20 +/- 12.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 202      |\n",
      "|    mean_reward     | 202      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=193.20 +/- 9.87\n",
      "Episode length: 193.20 +/- 9.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 193      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=193.60 +/- 13.97\n",
      "Episode length: 193.60 +/- 13.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 194      |\n",
      "|    mean_reward     | 194      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 452   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 167   |\n",
      "|    total_timesteps | 75726 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=181.40 +/- 9.07\n",
      "Episode length: 181.40 +/- 9.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 181         |\n",
      "|    mean_reward          | 181         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 76000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010569681 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | 0.00477     |\n",
      "|    value_loss           | 0.00612     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=189.80 +/- 6.27\n",
      "Episode length: 189.80 +/- 6.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | 190      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=181.00 +/- 8.10\n",
      "Episode length: 181.00 +/- 8.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 181      |\n",
      "|    mean_reward     | 181      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=182.00 +/- 5.76\n",
      "Episode length: 182.00 +/- 5.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 182      |\n",
      "|    mean_reward     | 182      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 452   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 171   |\n",
      "|    total_timesteps | 77529 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=251.40 +/- 33.15\n",
      "Episode length: 251.40 +/- 33.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | 251         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 78000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012327151 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.13        |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.00952     |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.00181     |\n",
      "|    value_loss           | 0.185       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=357.00 +/- 110.74\n",
      "Episode length: 357.00 +/- 110.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 357      |\n",
      "|    mean_reward     | 357      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=241.80 +/- 18.05\n",
      "Episode length: 241.80 +/- 18.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 242      |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 452   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 175   |\n",
      "|    total_timesteps | 79332 |\n",
      "------------------------------\n",
      "Single environment training took 176.05 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>251.8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-sweep-7</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/k1uhhekk' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/k1uhhekk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_132559-k1uhhekk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cjyosavt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2473549928026329\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0002203437209263696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9164672597813148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9498650779535368\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009852906142059435\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 2.184831490047454\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 83769\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_132911-cjyosavt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cjyosavt' target=\"_blank\">rosy-sweep-8</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cjyosavt' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cjyosavt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1239`, after every 19 untruncated mini-batches, there will be a truncated mini-batch of size 23\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1239 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=8.60 +/- 0.49\n",
      "Episode length: 8.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | 8.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=9.00 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 9        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1402 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1239 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=104.00 +/- 7.40\n",
      "Episode length: 104.00 +/- 7.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 104         |\n",
      "|    mean_reward          | 104         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024947602 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | -0.0274     |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.425       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=111.80 +/- 15.89\n",
      "Episode length: 111.80 +/- 15.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 112      |\n",
      "|    mean_reward     | 112      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1028 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2478 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=405.40 +/- 85.21\n",
      "Episode length: 405.40 +/- 85.21\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 405        |\n",
      "|    mean_reward          | 405        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03294187 |\n",
      "|    clip_fraction        | 0.376      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.612     |\n",
      "|    explained_variance   | 0.567      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 1.11       |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | -0.0527    |\n",
      "|    value_loss           | 3.43       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=457.20 +/- 47.51\n",
      "Episode length: 457.20 +/- 47.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 457      |\n",
      "|    mean_reward     | 457      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=443.60 +/- 79.95\n",
      "Episode length: 443.60 +/- 79.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 444      |\n",
      "|    mean_reward     | 444      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 614  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 3717 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=403.00 +/- 88.42\n",
      "Episode length: 403.00 +/- 88.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 403        |\n",
      "|    mean_reward          | 403        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05005157 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.51      |\n",
      "|    explained_variance   | 0.544      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.293      |\n",
      "|    n_updates            | 18         |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    value_loss           | 2.4        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=434.00 +/- 72.02\n",
      "Episode length: 434.00 +/- 72.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 434      |\n",
      "|    mean_reward     | 434      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 561  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 4956 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=252.40 +/- 124.16\n",
      "Episode length: 252.40 +/- 124.16\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 252        |\n",
      "|    mean_reward          | 252        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13301538 |\n",
      "|    clip_fraction        | 0.405      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.366     |\n",
      "|    explained_variance   | 0.689      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.579      |\n",
      "|    n_updates            | 24         |\n",
      "|    policy_gradient_loss | 0.0417     |\n",
      "|    value_loss           | 1.02       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=267.20 +/- 71.62\n",
      "Episode length: 267.20 +/- 71.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 267      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=240.80 +/- 94.06\n",
      "Episode length: 240.80 +/- 94.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 241      |\n",
      "|    mean_reward     | 241      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 544  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 6195 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=174.20 +/- 6.37\n",
      "Episode length: 174.20 +/- 6.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 174         |\n",
      "|    mean_reward          | 174         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017263412 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    value_loss           | 0.468       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=348.80 +/- 147.04\n",
      "Episode length: 348.80 +/- 147.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 349      |\n",
      "|    mean_reward     | 349      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 480  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 7434 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=233.80 +/- 134.66\n",
      "Episode length: 233.80 +/- 134.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 234         |\n",
      "|    mean_reward          | 234         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009976304 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.00841     |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    value_loss           | 0.124       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=245.80 +/- 128.40\n",
      "Episode length: 245.80 +/- 128.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 246      |\n",
      "|    mean_reward     | 246      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=250.60 +/- 127.44\n",
      "Episode length: 250.60 +/- 127.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 468  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 18   |\n",
      "|    total_timesteps | 8673 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=251.00 +/- 125.07\n",
      "Episode length: 251.00 +/- 125.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | 251         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033572044 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.32       |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0572      |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | 0.0196      |\n",
      "|    value_loss           | 0.535       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=262.80 +/- 128.11\n",
      "Episode length: 262.80 +/- 128.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 263      |\n",
      "|    mean_reward     | 263      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 480  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 9912 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 500       |\n",
      "|    mean_reward          | 500       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 10000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3501453 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.23     |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.0328    |\n",
      "|    n_updates            | 48        |\n",
      "|    policy_gradient_loss | 0.0089    |\n",
      "|    value_loss           | 0.0617    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10500, episode_reward=466.20 +/- 67.60\n",
      "Episode length: 466.20 +/- 67.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 466      |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=421.20 +/- 109.56\n",
      "Episode length: 421.20 +/- 109.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 421      |\n",
      "|    mean_reward     | 421      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 419   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 11151 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=207.80 +/- 24.38\n",
      "Episode length: 207.80 +/- 24.38\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 208       |\n",
      "|    mean_reward          | 208       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 11500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1562534 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.134    |\n",
      "|    explained_variance   | 0.966     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | -0.0193   |\n",
      "|    n_updates            | 54        |\n",
      "|    policy_gradient_loss | 0.0146    |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=376.80 +/- 151.59\n",
      "Episode length: 376.80 +/- 151.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 414   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 12390 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=292.60 +/- 124.44\n",
      "Episode length: 292.60 +/- 124.44\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 293          |\n",
      "|    mean_reward          | 293          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067136013 |\n",
      "|    clip_fraction        | 0.0803       |\n",
      "|    clip_range           | 0.247        |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.00985      |\n",
      "|    loss                 | -0.00682     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.0153       |\n",
      "|    value_loss           | 0.0495       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=271.40 +/- 124.87\n",
      "Episode length: 271.40 +/- 124.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 271      |\n",
      "|    mean_reward     | 271      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=472.00 +/- 56.00\n",
      "Episode length: 472.00 +/- 56.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 472      |\n",
      "|    mean_reward     | 472      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 393   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 13629 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017102737 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.175      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0387      |\n",
      "|    n_updates            | 66          |\n",
      "|    policy_gradient_loss | 0.00673     |\n",
      "|    value_loss           | 0.093       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 384   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 14868 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=436.80 +/- 126.40\n",
      "Episode length: 436.80 +/- 126.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 437         |\n",
      "|    mean_reward          | 437         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005775141 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    value_loss           | 0.0629      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=401.80 +/- 134.30\n",
      "Episode length: 401.80 +/- 134.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 402      |\n",
      "|    mean_reward     | 402      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=403.60 +/- 121.31\n",
      "Episode length: 403.60 +/- 121.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 404      |\n",
      "|    mean_reward     | 404      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 376   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 16107 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=415.80 +/- 130.91\n",
      "Episode length: 415.80 +/- 130.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 416        |\n",
      "|    mean_reward          | 416        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02252328 |\n",
      "|    clip_fraction        | 0.0585     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0957    |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 78         |\n",
      "|    policy_gradient_loss | 0.00353    |\n",
      "|    value_loss           | 0.0322     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=302.60 +/- 161.92\n",
      "Episode length: 302.60 +/- 161.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 303      |\n",
      "|    mean_reward     | 303      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 375   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 17346 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=247.00 +/- 128.17\n",
      "Episode length: 247.00 +/- 128.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 247          |\n",
      "|    mean_reward          | 247          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066121845 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.247        |\n",
      "|    entropy_loss         | -0.11        |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00985      |\n",
      "|    loss                 | 0.00228      |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | 0.00395      |\n",
      "|    value_loss           | 0.045        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=348.20 +/- 145.22\n",
      "Episode length: 348.20 +/- 145.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 348      |\n",
      "|    mean_reward     | 348      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=256.20 +/- 134.43\n",
      "Episode length: 256.20 +/- 134.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 381   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 18585 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=432.00 +/- 136.00\n",
      "Episode length: 432.00 +/- 136.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 432         |\n",
      "|    mean_reward          | 432         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027032044 |\n",
      "|    clip_fraction        | 0.0481      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.136      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0432      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00144     |\n",
      "|    value_loss           | 0.0449      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=459.40 +/- 81.20\n",
      "Episode length: 459.40 +/- 81.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 459      |\n",
      "|    mean_reward     | 459      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 375   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 19824 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=381.40 +/- 63.02\n",
      "Episode length: 381.40 +/- 63.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 381         |\n",
      "|    mean_reward          | 381         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067538776 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.159      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.129       |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | 0.0144      |\n",
      "|    value_loss           | 0.237       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=398.00 +/- 131.51\n",
      "Episode length: 398.00 +/- 131.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 398      |\n",
      "|    mean_reward     | 398      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=320.40 +/- 113.85\n",
      "Episode length: 320.40 +/- 113.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 320      |\n",
      "|    mean_reward     | 320      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 377   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 21063 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=453.20 +/- 93.60\n",
      "Episode length: 453.20 +/- 93.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 453         |\n",
      "|    mean_reward          | 453         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029789463 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.00288     |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | 0.00471     |\n",
      "|    value_loss           | 0.063       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=311.00 +/- 154.46\n",
      "Episode length: 311.00 +/- 154.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 311      |\n",
      "|    mean_reward     | 311      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 369   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 22302 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=159.40 +/- 14.68\n",
      "Episode length: 159.40 +/- 14.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 159        |\n",
      "|    mean_reward          | 159        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30095595 |\n",
      "|    clip_fraction        | 0.0977     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0898     |\n",
      "|    n_updates            | 108        |\n",
      "|    policy_gradient_loss | 0.00946    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=237.00 +/- 133.72\n",
      "Episode length: 237.00 +/- 133.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 237      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=224.60 +/- 139.97\n",
      "Episode length: 224.60 +/- 139.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 225      |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 364   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 23541 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=377.40 +/- 147.91\n",
      "Episode length: 377.40 +/- 147.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 377        |\n",
      "|    mean_reward          | 377        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38249737 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0573    |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.000903   |\n",
      "|    n_updates            | 114        |\n",
      "|    policy_gradient_loss | 0.0301     |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=288.40 +/- 134.85\n",
      "Episode length: 288.40 +/- 134.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 288      |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 24780 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=367.40 +/- 162.54\n",
      "Episode length: 367.40 +/- 162.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 367        |\n",
      "|    mean_reward          | 367        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19638847 |\n",
      "|    clip_fraction        | 0.0995     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0685    |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0327     |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.00926    |\n",
      "|    value_loss           | 0.319      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=359.60 +/- 157.60\n",
      "Episode length: 359.60 +/- 157.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 360      |\n",
      "|    mean_reward     | 360      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=307.80 +/- 157.10\n",
      "Episode length: 307.80 +/- 157.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | 308      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 26019 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=312.00 +/- 154.50\n",
      "Episode length: 312.00 +/- 154.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 312        |\n",
      "|    mean_reward          | 312        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03901016 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.155     |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.036      |\n",
      "|    n_updates            | 126        |\n",
      "|    policy_gradient_loss | 0.00488    |\n",
      "|    value_loss           | 0.058      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=263.00 +/- 68.43\n",
      "Episode length: 263.00 +/- 68.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 263      |\n",
      "|    mean_reward     | 263      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 370   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 27258 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=255.40 +/- 107.01\n",
      "Episode length: 255.40 +/- 107.01\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 255        |\n",
      "|    mean_reward          | 255        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44319707 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0678    |\n",
      "|    explained_variance   | 0.787      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0755     |\n",
      "|    n_updates            | 132        |\n",
      "|    policy_gradient_loss | 0.0342     |\n",
      "|    value_loss           | 0.586      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=362.60 +/- 150.08\n",
      "Episode length: 362.60 +/- 150.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 363      |\n",
      "|    mean_reward     | 363      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 28497 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=316.60 +/- 150.15\n",
      "Episode length: 316.60 +/- 150.15\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 317        |\n",
      "|    mean_reward          | 317        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13912767 |\n",
      "|    clip_fraction        | 0.0741     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0452    |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.02       |\n",
      "|    n_updates            | 138        |\n",
      "|    policy_gradient_loss | 0.00578    |\n",
      "|    value_loss           | 0.0592     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=414.80 +/- 106.42\n",
      "Episode length: 414.80 +/- 106.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 415      |\n",
      "|    mean_reward     | 415      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=434.00 +/- 132.00\n",
      "Episode length: 434.00 +/- 132.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 434      |\n",
      "|    mean_reward     | 434      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 364   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 29736 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=260.40 +/- 120.47\n",
      "Episode length: 260.40 +/- 120.47\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 260        |\n",
      "|    mean_reward          | 260        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01747885 |\n",
      "|    clip_fraction        | 0.0527     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.063     |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0657     |\n",
      "|    n_updates            | 144        |\n",
      "|    policy_gradient_loss | 0.00651    |\n",
      "|    value_loss           | 0.0274     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=345.60 +/- 140.61\n",
      "Episode length: 345.60 +/- 140.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 346      |\n",
      "|    mean_reward     | 346      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 367   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 84    |\n",
      "|    total_timesteps | 30975 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=244.00 +/- 89.30\n",
      "Episode length: 244.00 +/- 89.30\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 244       |\n",
      "|    mean_reward          | 244       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 31000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7472421 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0595   |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.145     |\n",
      "|    n_updates            | 150       |\n",
      "|    policy_gradient_loss | 0.012     |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=336.40 +/- 99.08\n",
      "Episode length: 336.40 +/- 99.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 336      |\n",
      "|    mean_reward     | 336      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=383.00 +/- 134.56\n",
      "Episode length: 383.00 +/- 134.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 383      |\n",
      "|    mean_reward     | 383      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 87    |\n",
      "|    total_timesteps | 32214 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=241.40 +/- 96.18\n",
      "Episode length: 241.40 +/- 96.18\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 241       |\n",
      "|    mean_reward          | 241       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 32500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3675916 |\n",
      "|    clip_fraction        | 0.0958    |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0522   |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.00737   |\n",
      "|    n_updates            | 156       |\n",
      "|    policy_gradient_loss | 0.0205    |\n",
      "|    value_loss           | 0.0159    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=264.60 +/- 139.93\n",
      "Episode length: 264.60 +/- 139.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 265      |\n",
      "|    mean_reward     | 265      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 368   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 90    |\n",
      "|    total_timesteps | 33453 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=358.40 +/- 86.13\n",
      "Episode length: 358.40 +/- 86.13\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 358       |\n",
      "|    mean_reward          | 358       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 33500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1843149 |\n",
      "|    clip_fraction        | 0.0643    |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0245   |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.0733    |\n",
      "|    n_updates            | 162       |\n",
      "|    policy_gradient_loss | 0.00856   |\n",
      "|    value_loss           | 0.104     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=407.40 +/- 114.55\n",
      "Episode length: 407.40 +/- 114.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 407      |\n",
      "|    mean_reward     | 407      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 94    |\n",
      "|    total_timesteps | 34692 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=154.00 +/- 19.88\n",
      "Episode length: 154.00 +/- 19.88\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 154       |\n",
      "|    mean_reward          | 154       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 35000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3169147 |\n",
      "|    clip_fraction        | 0.09      |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0696   |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.00166   |\n",
      "|    n_updates            | 168       |\n",
      "|    policy_gradient_loss | -0.00129  |\n",
      "|    value_loss           | 0.0091    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=146.40 +/- 30.45\n",
      "Episode length: 146.40 +/- 30.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 146      |\n",
      "|    mean_reward     | 146      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 372   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 96    |\n",
      "|    total_timesteps | 35931 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=385.20 +/- 119.93\n",
      "Episode length: 385.20 +/- 119.93\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 385       |\n",
      "|    mean_reward          | 385       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 36000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9874946 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.968     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.252     |\n",
      "|    n_updates            | 174       |\n",
      "|    policy_gradient_loss | 0.036     |\n",
      "|    value_loss           | 0.228     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=496.00 +/- 8.00\n",
      "Episode length: 496.00 +/- 8.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 496      |\n",
      "|    mean_reward     | 496      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=349.20 +/- 135.35\n",
      "Episode length: 349.20 +/- 135.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 349      |\n",
      "|    mean_reward     | 349      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 372   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 37170 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=292.40 +/- 111.09\n",
      "Episode length: 292.40 +/- 111.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 292         |\n",
      "|    mean_reward          | 292         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059064798 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.0475     |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.00136     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.0153      |\n",
      "|    value_loss           | 0.0713      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=333.00 +/- 134.14\n",
      "Episode length: 333.00 +/- 134.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 333      |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 375   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 102   |\n",
      "|    total_timesteps | 38409 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=492.40 +/- 12.01\n",
      "Episode length: 492.40 +/- 12.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 492         |\n",
      "|    mean_reward          | 492         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049942095 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.0177     |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.083       |\n",
      "|    n_updates            | 186         |\n",
      "|    policy_gradient_loss | 0.00589     |\n",
      "|    value_loss           | 0.044       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=321.20 +/- 99.50\n",
      "Episode length: 321.20 +/- 99.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 321      |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=293.60 +/- 139.32\n",
      "Episode length: 293.60 +/- 139.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 294      |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 376   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 105   |\n",
      "|    total_timesteps | 39648 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=371.60 +/- 157.32\n",
      "Episode length: 371.60 +/- 157.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 372        |\n",
      "|    mean_reward          | 372        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05683461 |\n",
      "|    clip_fraction        | 0.0306     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0205    |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.00106    |\n",
      "|    n_updates            | 192        |\n",
      "|    policy_gradient_loss | 0.0201     |\n",
      "|    value_loss           | 0.019      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=366.80 +/- 130.85\n",
      "Episode length: 366.80 +/- 130.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 367      |\n",
      "|    mean_reward     | 367      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 378   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 107   |\n",
      "|    total_timesteps | 40887 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=368.00 +/- 122.04\n",
      "Episode length: 368.00 +/- 122.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 368         |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037483133 |\n",
      "|    clip_fraction        | 0.0328      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.0459     |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 198         |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=328.00 +/- 142.26\n",
      "Episode length: 328.00 +/- 142.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 328      |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=266.60 +/- 130.24\n",
      "Episode length: 266.60 +/- 130.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 267      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 379   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 110   |\n",
      "|    total_timesteps | 42126 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=311.40 +/- 67.05\n",
      "Episode length: 311.40 +/- 67.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 311         |\n",
      "|    mean_reward          | 311         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037901998 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.0472     |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0717      |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | 0.0056      |\n",
      "|    value_loss           | 0.0897      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=390.20 +/- 129.74\n",
      "Episode length: 390.20 +/- 129.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 390      |\n",
      "|    mean_reward     | 390      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 382   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 113   |\n",
      "|    total_timesteps | 43365 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=205.00 +/- 64.98\n",
      "Episode length: 205.00 +/- 64.98\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 205       |\n",
      "|    mean_reward          | 205       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 43500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2325292 |\n",
      "|    clip_fraction        | 0.103     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0304   |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.00598   |\n",
      "|    n_updates            | 210       |\n",
      "|    policy_gradient_loss | 0.00876   |\n",
      "|    value_loss           | 0.0609    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=183.40 +/- 23.43\n",
      "Episode length: 183.40 +/- 23.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 183      |\n",
      "|    mean_reward     | 183      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=175.40 +/- 46.37\n",
      "Episode length: 175.40 +/- 46.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 175      |\n",
      "|    mean_reward     | 175      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 385   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 115   |\n",
      "|    total_timesteps | 44604 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=230.80 +/- 74.80\n",
      "Episode length: 230.80 +/- 74.80\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 231       |\n",
      "|    mean_reward          | 231       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 45000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1583987 |\n",
      "|    clip_fraction        | 0.0258    |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.00826  |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.145     |\n",
      "|    n_updates            | 216       |\n",
      "|    policy_gradient_loss | 0.0101    |\n",
      "|    value_loss           | 0.0252    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=166.00 +/- 31.20\n",
      "Episode length: 166.00 +/- 31.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 166      |\n",
      "|    mean_reward     | 166      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 389   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 117   |\n",
      "|    total_timesteps | 45843 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=194.20 +/- 86.32\n",
      "Episode length: 194.20 +/- 86.32\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 194        |\n",
      "|    mean_reward          | 194        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 46000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10002875 |\n",
      "|    clip_fraction        | 0.0277     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0161    |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0795     |\n",
      "|    n_updates            | 222        |\n",
      "|    policy_gradient_loss | 0.0249     |\n",
      "|    value_loss           | 0.0593     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=253.80 +/- 130.52\n",
      "Episode length: 253.80 +/- 130.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 254      |\n",
      "|    mean_reward     | 254      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=184.20 +/- 56.20\n",
      "Episode length: 184.20 +/- 56.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 184      |\n",
      "|    mean_reward     | 184      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 391   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 120   |\n",
      "|    total_timesteps | 47082 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=230.00 +/- 72.42\n",
      "Episode length: 230.00 +/- 72.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 230        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 47500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11860434 |\n",
      "|    clip_fraction        | 0.019      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0147    |\n",
      "|    explained_variance   | 0.858      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.00228    |\n",
      "|    n_updates            | 228        |\n",
      "|    policy_gradient_loss | -0.00249   |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=360.60 +/- 145.52\n",
      "Episode length: 360.60 +/- 145.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 361      |\n",
      "|    mean_reward     | 361      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 394   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 122   |\n",
      "|    total_timesteps | 48321 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=261.40 +/- 110.83\n",
      "Episode length: 261.40 +/- 110.83\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 261       |\n",
      "|    mean_reward          | 261       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 48500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3370795 |\n",
      "|    clip_fraction        | 0.0661    |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.017    |\n",
      "|    explained_variance   | 0.962     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.0251    |\n",
      "|    n_updates            | 234       |\n",
      "|    policy_gradient_loss | 0.0302    |\n",
      "|    value_loss           | 0.058     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=276.60 +/- 91.66\n",
      "Episode length: 276.60 +/- 91.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 277      |\n",
      "|    mean_reward     | 277      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=176.20 +/- 13.06\n",
      "Episode length: 176.20 +/- 13.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 176      |\n",
      "|    mean_reward     | 176      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 395   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 125   |\n",
      "|    total_timesteps | 49560 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=373.60 +/- 127.25\n",
      "Episode length: 373.60 +/- 127.25\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 374       |\n",
      "|    mean_reward          | 374       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 50000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0445597 |\n",
      "|    clip_fraction        | 0.0227    |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0099   |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.00901   |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | 0.00246   |\n",
      "|    value_loss           | 0.073     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=272.60 +/- 137.15\n",
      "Episode length: 272.60 +/- 137.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 273      |\n",
      "|    mean_reward     | 273      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 396   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 128   |\n",
      "|    total_timesteps | 50799 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=262.40 +/- 126.82\n",
      "Episode length: 262.40 +/- 126.82\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 262        |\n",
      "|    mean_reward          | 262        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 51000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31068724 |\n",
      "|    clip_fraction        | 0.0577     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0178    |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0239     |\n",
      "|    n_updates            | 246        |\n",
      "|    policy_gradient_loss | 0.00956    |\n",
      "|    value_loss           | 0.0282     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=177.60 +/- 49.33\n",
      "Episode length: 177.60 +/- 49.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 178      |\n",
      "|    mean_reward     | 178      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=227.60 +/- 92.28\n",
      "Episode length: 227.60 +/- 92.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | 228      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 396   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 131   |\n",
      "|    total_timesteps | 52038 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=351.80 +/- 130.79\n",
      "Episode length: 351.80 +/- 130.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 352        |\n",
      "|    mean_reward          | 352        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 52500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26000175 |\n",
      "|    clip_fraction        | 0.0945     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0289    |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.00982    |\n",
      "|    n_updates            | 252        |\n",
      "|    policy_gradient_loss | 0.00663    |\n",
      "|    value_loss           | 0.0235     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=304.00 +/- 156.41\n",
      "Episode length: 304.00 +/- 156.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 304      |\n",
      "|    mean_reward     | 304      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 397   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 133   |\n",
      "|    total_timesteps | 53277 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=258.00 +/- 62.78\n",
      "Episode length: 258.00 +/- 62.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 258         |\n",
      "|    mean_reward          | 258         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057812758 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.0286     |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0189      |\n",
      "|    n_updates            | 258         |\n",
      "|    policy_gradient_loss | 0.00403     |\n",
      "|    value_loss           | 0.0228      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=372.00 +/- 117.43\n",
      "Episode length: 372.00 +/- 117.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 372      |\n",
      "|    mean_reward     | 372      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=401.80 +/- 116.67\n",
      "Episode length: 401.80 +/- 116.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 402      |\n",
      "|    mean_reward     | 402      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 393   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 138   |\n",
      "|    total_timesteps | 54516 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=348.00 +/- 88.53\n",
      "Episode length: 348.00 +/- 88.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 348        |\n",
      "|    mean_reward          | 348        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 55000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05680232 |\n",
      "|    clip_fraction        | 0.0853     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0393    |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | -0.00293   |\n",
      "|    n_updates            | 264        |\n",
      "|    policy_gradient_loss | 0.00807    |\n",
      "|    value_loss           | 0.0243     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=358.00 +/- 150.75\n",
      "Episode length: 358.00 +/- 150.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 358      |\n",
      "|    mean_reward     | 358      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 394   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 141   |\n",
      "|    total_timesteps | 55755 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=187.40 +/- 57.90\n",
      "Episode length: 187.40 +/- 57.90\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 187        |\n",
      "|    mean_reward          | 187        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 56000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47728053 |\n",
      "|    clip_fraction        | 0.0964     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0376    |\n",
      "|    explained_variance   | 0.998      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0179     |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | 0.0106     |\n",
      "|    value_loss           | 0.0425     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=194.80 +/- 92.28\n",
      "Episode length: 194.80 +/- 92.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 195      |\n",
      "|    mean_reward     | 195      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 396   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 143   |\n",
      "|    total_timesteps | 56994 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=203.20 +/- 19.34\n",
      "Episode length: 203.20 +/- 19.34\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 203       |\n",
      "|    mean_reward          | 203       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 57000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0948723 |\n",
      "|    clip_fraction        | 0.0475    |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0166   |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.0684    |\n",
      "|    n_updates            | 276       |\n",
      "|    policy_gradient_loss | -0.000605 |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=192.80 +/- 42.74\n",
      "Episode length: 192.80 +/- 42.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 193      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=189.00 +/- 51.78\n",
      "Episode length: 189.00 +/- 51.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 189      |\n",
      "|    mean_reward     | 189      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 397   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 146   |\n",
      "|    total_timesteps | 58233 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=266.80 +/- 78.96\n",
      "Episode length: 266.80 +/- 78.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 267        |\n",
      "|    mean_reward          | 267        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 58500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02832662 |\n",
      "|    clip_fraction        | 0.0299     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0232    |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | -0.00265   |\n",
      "|    n_updates            | 282        |\n",
      "|    policy_gradient_loss | -0.00297   |\n",
      "|    value_loss           | 0.0279     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=190.20 +/- 34.82\n",
      "Episode length: 190.20 +/- 34.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | 190      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 149   |\n",
      "|    total_timesteps | 59472 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=267.60 +/- 58.92\n",
      "Episode length: 267.60 +/- 58.92\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 268       |\n",
      "|    mean_reward          | 268       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 59500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9455487 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0297   |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.014     |\n",
      "|    n_updates            | 288       |\n",
      "|    policy_gradient_loss | 0.0191    |\n",
      "|    value_loss           | 0.065     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=249.00 +/- 99.71\n",
      "Episode length: 249.00 +/- 99.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 249      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=257.20 +/- 55.06\n",
      "Episode length: 257.20 +/- 55.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 257      |\n",
      "|    mean_reward     | 257      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 49    |\n",
      "|    time_elapsed    | 152   |\n",
      "|    total_timesteps | 60711 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=225.80 +/- 14.96\n",
      "Episode length: 225.80 +/- 14.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 226         |\n",
      "|    mean_reward          | 226         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 61000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008606819 |\n",
      "|    clip_fraction        | 0.0139      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.00962    |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 294         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    value_loss           | 0.625       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=191.80 +/- 17.31\n",
      "Episode length: 191.80 +/- 17.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 192      |\n",
      "|    mean_reward     | 192      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 401   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 154   |\n",
      "|    total_timesteps | 61950 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=250.20 +/- 102.50\n",
      "Episode length: 250.20 +/- 102.50\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 250       |\n",
      "|    mean_reward          | 250       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 62000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7427912 |\n",
      "|    clip_fraction        | 0.0629    |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0177   |\n",
      "|    explained_variance   | 0.921     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.00986   |\n",
      "|    n_updates            | 300       |\n",
      "|    policy_gradient_loss | -0.0049   |\n",
      "|    value_loss           | 0.142     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=336.40 +/- 137.30\n",
      "Episode length: 336.40 +/- 137.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 336      |\n",
      "|    mean_reward     | 336      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=379.80 +/- 147.39\n",
      "Episode length: 379.80 +/- 147.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 380      |\n",
      "|    mean_reward     | 380      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 398   |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 158   |\n",
      "|    total_timesteps | 63189 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=260.20 +/- 138.12\n",
      "Episode length: 260.20 +/- 138.12\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 260        |\n",
      "|    mean_reward          | 260        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 63500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53167033 |\n",
      "|    clip_fraction        | 0.0608     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0296    |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.0188     |\n",
      "|    n_updates            | 306        |\n",
      "|    policy_gradient_loss | 0.0025     |\n",
      "|    value_loss           | 0.25       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=247.60 +/- 67.21\n",
      "Episode length: 247.60 +/- 67.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 248      |\n",
      "|    mean_reward     | 248      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 161   |\n",
      "|    total_timesteps | 64428 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=266.00 +/- 139.92\n",
      "Episode length: 266.00 +/- 139.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 266        |\n",
      "|    mean_reward          | 266        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 64500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10717859 |\n",
      "|    clip_fraction        | 0.0363     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0161    |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.142      |\n",
      "|    n_updates            | 312        |\n",
      "|    policy_gradient_loss | 0.017      |\n",
      "|    value_loss           | 0.137      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=402.80 +/- 114.10\n",
      "Episode length: 402.80 +/- 114.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 403      |\n",
      "|    mean_reward     | 403      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=248.60 +/- 75.09\n",
      "Episode length: 248.60 +/- 75.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 249      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 397   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 165   |\n",
      "|    total_timesteps | 65667 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=301.00 +/- 126.45\n",
      "Episode length: 301.00 +/- 126.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 301        |\n",
      "|    mean_reward          | 301        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 66000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48899236 |\n",
      "|    clip_fraction        | 0.0696     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0233    |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 318        |\n",
      "|    policy_gradient_loss | 0.553      |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=290.80 +/- 90.52\n",
      "Episode length: 290.80 +/- 90.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 291      |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 398   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 168   |\n",
      "|    total_timesteps | 66906 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=177.20 +/- 32.18\n",
      "Episode length: 177.20 +/- 32.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 177         |\n",
      "|    mean_reward          | 177         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 67000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025423622 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.247       |\n",
      "|    entropy_loss         | -0.0389     |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.00985     |\n",
      "|    loss                 | 0.00503     |\n",
      "|    n_updates            | 324         |\n",
      "|    policy_gradient_loss | 0.00671     |\n",
      "|    value_loss           | 0.0994      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=269.00 +/- 118.96\n",
      "Episode length: 269.00 +/- 118.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 269      |\n",
      "|    mean_reward     | 269      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=262.00 +/- 86.49\n",
      "Episode length: 262.00 +/- 86.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 262      |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 398   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 170   |\n",
      "|    total_timesteps | 68145 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=313.20 +/- 99.71\n",
      "Episode length: 313.20 +/- 99.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 313        |\n",
      "|    mean_reward          | 313        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 68500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46880946 |\n",
      "|    clip_fraction        | 0.0487     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.00758   |\n",
      "|    explained_variance   | 0.651      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.00396    |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | 0.0222     |\n",
      "|    value_loss           | 0.566      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=312.00 +/- 138.60\n",
      "Episode length: 312.00 +/- 138.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 312      |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 173   |\n",
      "|    total_timesteps | 69384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=362.60 +/- 116.02\n",
      "Episode length: 362.60 +/- 116.02\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 363      |\n",
      "|    mean_reward          | 363      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 69500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.160874 |\n",
      "|    clip_fraction        | 0.029    |\n",
      "|    clip_range           | 0.247    |\n",
      "|    entropy_loss         | -0.00936 |\n",
      "|    explained_variance   | 0.999    |\n",
      "|    learning_rate        | 0.00985  |\n",
      "|    loss                 | 0.0103   |\n",
      "|    n_updates            | 336      |\n",
      "|    policy_gradient_loss | 0.0287   |\n",
      "|    value_loss           | 0.00739  |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=338.80 +/- 141.01\n",
      "Episode length: 338.80 +/- 141.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 339      |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=301.80 +/- 75.64\n",
      "Episode length: 301.80 +/- 75.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 302      |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 397   |\n",
      "|    iterations      | 57    |\n",
      "|    time_elapsed    | 177   |\n",
      "|    total_timesteps | 70623 |\n",
      "------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=186.40 +/- 51.15\n",
      "Episode length: 186.40 +/- 51.15\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 186        |\n",
      "|    mean_reward          | 186        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 71000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17768368 |\n",
      "|    clip_fraction        | 0.033      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.00811   |\n",
      "|    explained_variance   | 0.732      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.00345    |\n",
      "|    n_updates            | 342        |\n",
      "|    policy_gradient_loss | 0.0272     |\n",
      "|    value_loss           | 0.43       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=218.20 +/- 40.76\n",
      "Episode length: 218.20 +/- 40.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 218      |\n",
      "|    mean_reward     | 218      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 58    |\n",
      "|    time_elapsed    | 179   |\n",
      "|    total_timesteps | 71862 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=138.00 +/- 39.27\n",
      "Episode length: 138.00 +/- 39.27\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 138        |\n",
      "|    mean_reward          | 138        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 72000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34795016 |\n",
      "|    clip_fraction        | 0.0578     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0257    |\n",
      "|    explained_variance   | 0.849      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 348        |\n",
      "|    policy_gradient_loss | 0.01       |\n",
      "|    value_loss           | 0.299      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=108.00 +/- 9.82\n",
      "Episode length: 108.00 +/- 9.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 108      |\n",
      "|    mean_reward     | 108      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=124.00 +/- 30.21\n",
      "Episode length: 124.00 +/- 30.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 124      |\n",
      "|    mean_reward     | 124      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 401   |\n",
      "|    iterations      | 59    |\n",
      "|    time_elapsed    | 181   |\n",
      "|    total_timesteps | 73101 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=155.60 +/- 39.60\n",
      "Episode length: 155.60 +/- 39.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 156        |\n",
      "|    mean_reward          | 156        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 73500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13282247 |\n",
      "|    clip_fraction        | 0.0687     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0453    |\n",
      "|    explained_variance   | 0.772      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.95       |\n",
      "|    n_updates            | 354        |\n",
      "|    policy_gradient_loss | -0.00771   |\n",
      "|    value_loss           | 0.926      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=137.40 +/- 13.47\n",
      "Episode length: 137.40 +/- 13.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 137      |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 404   |\n",
      "|    iterations      | 60    |\n",
      "|    time_elapsed    | 183   |\n",
      "|    total_timesteps | 74340 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=39.20 +/- 3.87\n",
      "Episode length: 39.20 +/- 3.87\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 39.2      |\n",
      "|    mean_reward          | 39.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 74500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.4742184 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.056    |\n",
      "|    explained_variance   | 0.95      |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.00803   |\n",
      "|    n_updates            | 360       |\n",
      "|    policy_gradient_loss | 0.0235    |\n",
      "|    value_loss           | 0.144     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=39.60 +/- 8.33\n",
      "Episode length: 39.60 +/- 8.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 39.6     |\n",
      "|    mean_reward     | 39.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=37.00 +/- 9.38\n",
      "Episode length: 37.00 +/- 9.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 37       |\n",
      "|    mean_reward     | 37       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 61    |\n",
      "|    time_elapsed    | 185   |\n",
      "|    total_timesteps | 75579 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=57.60 +/- 8.26\n",
      "Episode length: 57.60 +/- 8.26\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 57.6      |\n",
      "|    mean_reward          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 76000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4206154 |\n",
      "|    clip_fraction        | 0.467     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.114    |\n",
      "|    explained_variance   | 0.221     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 3.14      |\n",
      "|    n_updates            | 366       |\n",
      "|    policy_gradient_loss | 0.0941    |\n",
      "|    value_loss           | 8.84      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=56.60 +/- 7.94\n",
      "Episode length: 56.60 +/- 7.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 56.6     |\n",
      "|    mean_reward     | 56.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 410   |\n",
      "|    iterations      | 62    |\n",
      "|    time_elapsed    | 187   |\n",
      "|    total_timesteps | 76818 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=100.80 +/- 13.50\n",
      "Episode length: 100.80 +/- 13.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 101        |\n",
      "|    mean_reward          | 101        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 77000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43259996 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0775    |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 1.17       |\n",
      "|    n_updates            | 372        |\n",
      "|    policy_gradient_loss | 0.00174    |\n",
      "|    value_loss           | 1.12       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=95.20 +/- 17.42\n",
      "Episode length: 95.20 +/- 17.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 95.2     |\n",
      "|    mean_reward     | 95.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=104.00 +/- 12.28\n",
      "Episode length: 104.00 +/- 12.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 104      |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 412   |\n",
      "|    iterations      | 63    |\n",
      "|    time_elapsed    | 189   |\n",
      "|    total_timesteps | 78057 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=152.60 +/- 20.80\n",
      "Episode length: 152.60 +/- 20.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 153        |\n",
      "|    mean_reward          | 153        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 78500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43205723 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0545    |\n",
      "|    explained_variance   | 0.796      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.201      |\n",
      "|    n_updates            | 378        |\n",
      "|    policy_gradient_loss | 0.0183     |\n",
      "|    value_loss           | 0.731      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=154.60 +/- 25.24\n",
      "Episode length: 154.60 +/- 25.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 155      |\n",
      "|    mean_reward     | 155      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 414   |\n",
      "|    iterations      | 64    |\n",
      "|    time_elapsed    | 191   |\n",
      "|    total_timesteps | 79296 |\n",
      "------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=200.40 +/- 30.72\n",
      "Episode length: 200.40 +/- 30.72\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | 200        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 79500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82103425 |\n",
      "|    clip_fraction        | 0.0507     |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0135    |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.00627    |\n",
      "|    n_updates            | 384        |\n",
      "|    policy_gradient_loss | 0.00315    |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=194.80 +/- 35.04\n",
      "Episode length: 194.80 +/- 35.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 195      |\n",
      "|    mean_reward     | 195      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=266.60 +/- 33.84\n",
      "Episode length: 266.60 +/- 33.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 267      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 414   |\n",
      "|    iterations      | 65    |\n",
      "|    time_elapsed    | 194   |\n",
      "|    total_timesteps | 80535 |\n",
      "------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=147.20 +/- 26.83\n",
      "Episode length: 147.20 +/- 26.83\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 147       |\n",
      "|    mean_reward          | 147       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 81000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4740532 |\n",
      "|    clip_fraction        | 0.112     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0531   |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.149     |\n",
      "|    n_updates            | 390       |\n",
      "|    policy_gradient_loss | 0.0216    |\n",
      "|    value_loss           | 0.105     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=126.80 +/- 6.31\n",
      "Episode length: 126.80 +/- 6.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 127      |\n",
      "|    mean_reward     | 127      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 416   |\n",
      "|    iterations      | 66    |\n",
      "|    time_elapsed    | 196   |\n",
      "|    total_timesteps | 81774 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=160.40 +/- 7.00\n",
      "Episode length: 160.40 +/- 7.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 160       |\n",
      "|    mean_reward          | 160       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 82000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2845959 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.247     |\n",
      "|    entropy_loss         | -0.0564   |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.00985   |\n",
      "|    loss                 | 0.0157    |\n",
      "|    n_updates            | 396       |\n",
      "|    policy_gradient_loss | 0.0233    |\n",
      "|    value_loss           | 0.0504    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=175.40 +/- 19.29\n",
      "Episode length: 175.40 +/- 19.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 175      |\n",
      "|    mean_reward     | 175      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=166.20 +/- 14.92\n",
      "Episode length: 166.20 +/- 14.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 166      |\n",
      "|    mean_reward     | 166      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 418   |\n",
      "|    iterations      | 67    |\n",
      "|    time_elapsed    | 198   |\n",
      "|    total_timesteps | 83013 |\n",
      "------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=178.60 +/- 10.71\n",
      "Episode length: 178.60 +/- 10.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 179        |\n",
      "|    mean_reward          | 179        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 83500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12026803 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.247      |\n",
      "|    entropy_loss         | -0.0558    |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.00985    |\n",
      "|    loss                 | 0.107      |\n",
      "|    n_updates            | 402        |\n",
      "|    policy_gradient_loss | 0.0156     |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=161.80 +/- 10.30\n",
      "Episode length: 161.80 +/- 10.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 162      |\n",
      "|    mean_reward     | 162      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 419   |\n",
      "|    iterations      | 68    |\n",
      "|    time_elapsed    | 200   |\n",
      "|    total_timesteps | 84252 |\n",
      "------------------------------\n",
      "Single environment training took 201.23 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>154.9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-sweep-8</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cjyosavt' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cjyosavt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_132911-cjyosavt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 31fzcdem with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.20957637228720696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0034818015362356627\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9378237002052218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.98153159353979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001194324075765231\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 8.788583518270771\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 23736\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_133248-31fzcdem</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/31fzcdem' target=\"_blank\">valiant-sweep-9</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/31fzcdem' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/31fzcdem</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 517`, after every 8 untruncated mini-batches, there will be a truncated mini-batch of size 5\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=517 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=333.80 +/- 203.89\n",
      "Episode length: 333.80 +/- 203.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 334      |\n",
      "|    mean_reward     | 334      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 444 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 1   |\n",
      "|    total_timesteps | 517 |\n",
      "----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=113.20 +/- 7.93\n",
      "Episode length: 113.20 +/- 7.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 113         |\n",
      "|    mean_reward          | 113         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016470388 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.0151     |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 4.93        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 501  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 1034 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=225.20 +/- 53.83\n",
      "Episode length: 225.20 +/- 53.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 225         |\n",
      "|    mean_reward          | 225         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008537425 |\n",
      "|    clip_fraction        | 0.0227      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | 0.0603      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 8.61        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    value_loss           | 29.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 456  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 1551 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=378.80 +/- 149.09\n",
      "Episode length: 378.80 +/- 149.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 379         |\n",
      "|    mean_reward          | 379         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005693347 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 0.0582      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 22          |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.0087     |\n",
      "|    value_loss           | 30.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 406  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2068 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=407.80 +/- 73.99\n",
      "Episode length: 407.80 +/- 73.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 408         |\n",
      "|    mean_reward          | 408         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009034561 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | -0.0263     |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 38.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 393  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 2585 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=222.20 +/- 56.78\n",
      "Episode length: 222.20 +/- 56.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 222          |\n",
      "|    mean_reward          | 222          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068341857 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 3.27         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0149      |\n",
      "|    value_loss           | 33.8         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 415  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 3102 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=332.20 +/- 53.26\n",
      "Episode length: 332.20 +/- 53.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 332        |\n",
      "|    mean_reward          | 332        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00711244 |\n",
      "|    clip_fraction        | 0.0737     |\n",
      "|    clip_range           | 0.21       |\n",
      "|    entropy_loss         | -0.573     |\n",
      "|    explained_variance   | 0.192      |\n",
      "|    learning_rate        | 0.00119    |\n",
      "|    loss                 | 7.5        |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    value_loss           | 37.7       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 426  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 3619 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=437.20 +/- 77.58\n",
      "Episode length: 437.20 +/- 77.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 437         |\n",
      "|    mean_reward          | 437         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010145828 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 27.5        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 424  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4136 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=355.60 +/- 87.25\n",
      "Episode length: 355.60 +/- 87.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 356          |\n",
      "|    mean_reward          | 356          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062112734 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.584       |\n",
      "|    explained_variance   | -0.00579     |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 92           |\n",
      "|    n_updates            | 48           |\n",
      "|    policy_gradient_loss | -0.00471     |\n",
      "|    value_loss           | 31.8         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 426  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 4653 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=467.60 +/- 64.80\n",
      "Episode length: 467.60 +/- 64.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 468         |\n",
      "|    mean_reward          | 468         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011140987 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.419       |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | 0.00246     |\n",
      "|    value_loss           | 8.38        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 421  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 5170 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=243.00 +/- 89.58\n",
      "Episode length: 243.00 +/- 89.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 243         |\n",
      "|    mean_reward          | 243         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005567926 |\n",
      "|    clip_fraction        | 0.0606      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 0.0317      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00667     |\n",
      "|    value_loss           | 34.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 434  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 5687 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=212.60 +/- 47.65\n",
      "Episode length: 212.60 +/- 47.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 213          |\n",
      "|    mean_reward          | 213          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031300278 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | 0.438        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 6.41         |\n",
      "|    n_updates            | 66           |\n",
      "|    policy_gradient_loss | -0.00536     |\n",
      "|    value_loss           | 38.3         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 447  |\n",
      "|    iterations      | 12   |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 6204 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=291.80 +/- 78.76\n",
      "Episode length: 291.80 +/- 78.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 292         |\n",
      "|    mean_reward          | 292         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013113646 |\n",
      "|    clip_fraction        | 0.0517      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.541       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.00315     |\n",
      "|    value_loss           | 24.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 451  |\n",
      "|    iterations      | 13   |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 6721 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=254.80 +/- 123.82\n",
      "Episode length: 254.80 +/- 123.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 255         |\n",
      "|    mean_reward          | 255         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002803468 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 8.87        |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.00313     |\n",
      "|    value_loss           | 12.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 454  |\n",
      "|    iterations      | 14   |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 7238 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=169.40 +/- 5.82\n",
      "Episode length: 169.40 +/- 5.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 169          |\n",
      "|    mean_reward          | 169          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012220412 |\n",
      "|    clip_fraction        | 0.00984      |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 0.484        |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -3.35e-05    |\n",
      "|    value_loss           | 4.85         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 461  |\n",
      "|    iterations      | 15   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 7755 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=187.60 +/- 24.06\n",
      "Episode length: 187.60 +/- 24.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 188         |\n",
      "|    mean_reward          | 188         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003098011 |\n",
      "|    clip_fraction        | 0.0586      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00423    |\n",
      "|    value_loss           | 5.32        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 470  |\n",
      "|    iterations      | 16   |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 8272 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=230.60 +/- 27.40\n",
      "Episode length: 230.60 +/- 27.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 231         |\n",
      "|    mean_reward          | 231         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003046035 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 3.8         |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.00655    |\n",
      "|    value_loss           | 3.42        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 473  |\n",
      "|    iterations      | 17   |\n",
      "|    time_elapsed    | 18   |\n",
      "|    total_timesteps | 8789 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=495.20 +/- 9.60\n",
      "Episode length: 495.20 +/- 9.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 495         |\n",
      "|    mean_reward          | 495         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016684182 |\n",
      "|    clip_fraction        | 0.0925      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.292       |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | -0.00601    |\n",
      "|    value_loss           | 1.49        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 466  |\n",
      "|    iterations      | 18   |\n",
      "|    time_elapsed    | 19   |\n",
      "|    total_timesteps | 9306 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=498.80 +/- 2.40\n",
      "Episode length: 498.80 +/- 2.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 499         |\n",
      "|    mean_reward          | 499         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008329439 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.27        |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    value_loss           | 0.395       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 460  |\n",
      "|    iterations      | 19   |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 9823 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=296.60 +/- 27.05\n",
      "Episode length: 296.60 +/- 27.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 297         |\n",
      "|    mean_reward          | 297         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014298217 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.704       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.342       |\n",
      "|    n_updates            | 114         |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    value_loss           | 0.674       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 462   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 10340 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=271.20 +/- 22.89\n",
      "Episode length: 271.20 +/- 22.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 271         |\n",
      "|    mean_reward          | 271         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007990612 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.00713    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 6.81        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 10857 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=254.20 +/- 82.03\n",
      "Episode length: 254.20 +/- 82.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 254         |\n",
      "|    mean_reward          | 254         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010277299 |\n",
      "|    clip_fraction        | 0.0705      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 1.13        |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | 0.00173     |\n",
      "|    value_loss           | 6.47        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 469   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 11374 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=179.20 +/- 30.76\n",
      "Episode length: 179.20 +/- 30.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 179          |\n",
      "|    mean_reward          | 179          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 11500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064577963 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 0.157        |\n",
      "|    n_updates            | 132          |\n",
      "|    policy_gradient_loss | 0.00123      |\n",
      "|    value_loss           | 3.05         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 11891 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=221.40 +/- 20.40\n",
      "Episode length: 221.40 +/- 20.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 221          |\n",
      "|    mean_reward          | 221          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016037186 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.586       |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 0.0917       |\n",
      "|    n_updates            | 138          |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 3.15         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 12408 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=411.60 +/- 79.02\n",
      "Episode length: 411.60 +/- 79.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 412         |\n",
      "|    mean_reward          | 412         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008721283 |\n",
      "|    clip_fraction        | 0.0627      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 2.51        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.00771    |\n",
      "|    value_loss           | 21.4        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 455   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 12925 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=456.00 +/- 41.75\n",
      "Episode length: 456.00 +/- 41.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 456          |\n",
      "|    mean_reward          | 456          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 13000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064806235 |\n",
      "|    clip_fraction        | 0.0898       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.573       |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 0.317        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.014       |\n",
      "|    value_loss           | 6.26         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 453   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 13442 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=411.00 +/- 46.90\n",
      "Episode length: 411.00 +/- 46.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 411         |\n",
      "|    mean_reward          | 411         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018560084 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 445   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 13959 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=260.80 +/- 16.15\n",
      "Episode length: 260.80 +/- 16.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 261         |\n",
      "|    mean_reward          | 261         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016712435 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | -0.224      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 162         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    value_loss           | 0.0729      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 431   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 14476 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=290.80 +/- 18.09\n",
      "Episode length: 290.80 +/- 18.09\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 291        |\n",
      "|    mean_reward          | 291        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00361206 |\n",
      "|    clip_fraction        | 0.0407     |\n",
      "|    clip_range           | 0.21       |\n",
      "|    entropy_loss         | -0.547     |\n",
      "|    explained_variance   | 0.683      |\n",
      "|    learning_rate        | 0.00119    |\n",
      "|    loss                 | 1.45       |\n",
      "|    n_updates            | 168        |\n",
      "|    policy_gradient_loss | -0.00535   |\n",
      "|    value_loss           | 39.5       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 14993 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=343.00 +/- 25.08\n",
      "Episode length: 343.00 +/- 25.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | 343          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034891544 |\n",
      "|    clip_fraction        | 0.0932       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 3.9          |\n",
      "|    n_updates            | 174          |\n",
      "|    policy_gradient_loss | -0.00357     |\n",
      "|    value_loss           | 29.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=343.40 +/- 25.59\n",
      "Episode length: 343.40 +/- 25.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 410   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 15510 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=489.60 +/- 20.80\n",
      "Episode length: 489.60 +/- 20.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 490        |\n",
      "|    mean_reward          | 490        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03114252 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.21       |\n",
      "|    entropy_loss         | -0.601     |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.00119    |\n",
      "|    loss                 | 0.04       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    value_loss           | 1.1        |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 16027 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018472781 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.853       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 186         |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 0.421       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 388   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 16544 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023525607 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.207       |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 385   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 17061 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097862035 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.546       |\n",
      "|    explained_variance   | -0.441       |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.00633     |\n",
      "|    n_updates            | 198          |\n",
      "|    policy_gradient_loss | -0.00789     |\n",
      "|    value_loss           | 0.0673       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 384   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 17578 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012788475 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | -0.506      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.000291    |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | -0.00978    |\n",
      "|    value_loss           | 0.0375      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 381   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 18095 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011681523 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.519      |\n",
      "|    explained_variance   | -1.78       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.026      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.00101     |\n",
      "|    value_loss           | 0.0165      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 374   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 18612 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013626251 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | -0.803      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.0415     |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    value_loss           | 0.00825     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 374   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 19129 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009668537 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | -0.285      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.0936     |\n",
      "|    n_updates            | 222         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 0.00535     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 373   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 19646 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023288084 |\n",
      "|    clip_fraction        | 0.0442       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.445       |\n",
      "|    explained_variance   | -2.09        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | -0.0294      |\n",
      "|    n_updates            | 228          |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    value_loss           | 0.00442      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 372   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 20163 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008704031 |\n",
      "|    clip_fraction        | 0.0874      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | -3.98       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.00797    |\n",
      "|    n_updates            | 234         |\n",
      "|    policy_gradient_loss | -0.00371    |\n",
      "|    value_loss           | 0.00231     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 372   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 20680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 21000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078745065 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | -1.38        |\n",
      "|    learning_rate        | 0.00119      |\n",
      "|    loss                 | 0.00178      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 0.00112      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 372   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 21197 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015466493 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.346      |\n",
      "|    explained_variance   | -0.101      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 246         |\n",
      "|    policy_gradient_loss | -0.00348    |\n",
      "|    value_loss           | 0.000687    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 370   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 21714 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007053137 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | -2.4        |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 252         |\n",
      "|    policy_gradient_loss | -0.00501    |\n",
      "|    value_loss           | 0.000805    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 22231 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009393411 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | -1.23       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.00233    |\n",
      "|    n_updates            | 258         |\n",
      "|    policy_gradient_loss | -0.000996   |\n",
      "|    value_loss           | 0.000402    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 22748 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009085686 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | -2.27       |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.0283     |\n",
      "|    n_updates            | 264         |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    value_loss           | 0.00045     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 23265 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004110321 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | -0.594      |\n",
      "|    learning_rate        | 0.00119     |\n",
      "|    loss                 | -0.0284     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00414    |\n",
      "|    value_loss           | 0.00038     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 23782 |\n",
      "------------------------------\n",
      "Single environment training took 65.54 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-sweep-9</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/31fzcdem' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/31fzcdem</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_133248-31fzcdem/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x7wb0wjf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.1374481607323551\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.009735425180169654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9756959002856878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9003759089141894\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005230985670726304\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.9788839789054828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 64297\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_133406-x7wb0wjf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/x7wb0wjf' target=\"_blank\">solar-sweep-10</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/x7wb0wjf' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/x7wb0wjf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1853`, after every 28 untruncated mini-batches, there will be a truncated mini-batch of size 61\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1853 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=9.60 +/- 0.49\n",
      "Episode length: 9.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=9.40 +/- 0.80\n",
      "Episode length: 9.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=9.20 +/- 0.40\n",
      "Episode length: 9.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 9.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1053 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1853 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=15.00 +/- 6.26\n",
      "Episode length: 15.00 +/- 6.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 15          |\n",
      "|    mean_reward          | 15          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003836138 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.00327     |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 2.61        |\n",
      "|    n_updates            | 1           |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=11.20 +/- 2.14\n",
      "Episode length: 11.20 +/- 2.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.2     |\n",
      "|    mean_reward     | 11.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=18.40 +/- 6.15\n",
      "Episode length: 18.40 +/- 6.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.4     |\n",
      "|    mean_reward     | 18.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=15.60 +/- 5.50\n",
      "Episode length: 15.60 +/- 5.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 15.6     |\n",
      "|    mean_reward     | 15.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 973  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 3706 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=326.20 +/- 84.72\n",
      "Episode length: 326.20 +/- 84.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 326         |\n",
      "|    mean_reward          | 326         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005274958 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 2           |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    value_loss           | 3.56        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=377.00 +/- 103.37\n",
      "Episode length: 377.00 +/- 103.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=405.40 +/- 117.46\n",
      "Episode length: 405.40 +/- 117.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 405      |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5500, episode_reward=346.00 +/- 126.35\n",
      "Episode length: 346.00 +/- 126.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 346      |\n",
      "|    mean_reward     | 346      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 647  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 5559 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=224.00 +/- 43.54\n",
      "Episode length: 224.00 +/- 43.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 224          |\n",
      "|    mean_reward          | 224          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069649774 |\n",
      "|    clip_fraction        | 0.242        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | 0.407        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 1.12         |\n",
      "|    n_updates            | 3            |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 2.95         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=288.40 +/- 115.64\n",
      "Episode length: 288.40 +/- 115.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 288      |\n",
      "|    mean_reward     | 288      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=329.80 +/- 142.16\n",
      "Episode length: 329.80 +/- 142.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 330      |\n",
      "|    mean_reward     | 330      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 620  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 7412 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=362.40 +/- 119.98\n",
      "Episode length: 362.40 +/- 119.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 362         |\n",
      "|    mean_reward          | 362         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010876156 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.623      |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 2.72        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=341.00 +/- 121.30\n",
      "Episode length: 341.00 +/- 121.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 341      |\n",
      "|    mean_reward     | 341      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=321.60 +/- 95.76\n",
      "Episode length: 321.60 +/- 95.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 322      |\n",
      "|    mean_reward     | 322      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=374.60 +/- 91.60\n",
      "Episode length: 374.60 +/- 91.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 375      |\n",
      "|    mean_reward     | 375      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 585  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 9265 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=418.00 +/- 101.98\n",
      "Episode length: 418.00 +/- 101.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 418         |\n",
      "|    mean_reward          | 418         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013920269 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.463       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.86        |\n",
      "|    n_updates            | 5           |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 1.99        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=498.40 +/- 3.20\n",
      "Episode length: 498.40 +/- 3.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | 498      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10500, episode_reward=343.00 +/- 82.64\n",
      "Episode length: 343.00 +/- 82.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 552   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 11118 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=400.20 +/- 79.36\n",
      "Episode length: 400.20 +/- 79.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 400         |\n",
      "|    mean_reward          | 400         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009813286 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.707       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.00254    |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=398.00 +/- 119.21\n",
      "Episode length: 398.00 +/- 119.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 398      |\n",
      "|    mean_reward     | 398      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=322.40 +/- 72.05\n",
      "Episode length: 322.40 +/- 72.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 322      |\n",
      "|    mean_reward     | 322      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 559   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 12971 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=494.00 +/- 12.00\n",
      "Episode length: 494.00 +/- 12.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 494         |\n",
      "|    mean_reward          | 494         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011790905 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.553       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.561       |\n",
      "|    n_updates            | 7           |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 0.693       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=445.00 +/- 97.98\n",
      "Episode length: 445.00 +/- 97.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 445      |\n",
      "|    mean_reward     | 445      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=445.80 +/- 78.25\n",
      "Episode length: 445.80 +/- 78.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 446      |\n",
      "|    mean_reward     | 446      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=465.00 +/- 70.00\n",
      "Episode length: 465.00 +/- 70.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 465      |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 534   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 14824 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=429.40 +/- 101.32\n",
      "Episode length: 429.40 +/- 101.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 429         |\n",
      "|    mean_reward          | 429         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014451041 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.62        |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.436       |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | 0.0121      |\n",
      "|    value_loss           | 0.452       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=396.80 +/- 120.83\n",
      "Episode length: 396.80 +/- 120.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 397      |\n",
      "|    mean_reward     | 397      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=391.60 +/- 104.35\n",
      "Episode length: 391.60 +/- 104.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 392      |\n",
      "|    mean_reward     | 392      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=446.80 +/- 70.13\n",
      "Episode length: 446.80 +/- 70.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 447      |\n",
      "|    mean_reward     | 447      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 519   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 16677 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=412.80 +/- 98.20\n",
      "Episode length: 412.80 +/- 98.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 413          |\n",
      "|    mean_reward          | 413          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035548033 |\n",
      "|    clip_fraction        | 0.091        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.52        |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 0.0894       |\n",
      "|    n_updates            | 9            |\n",
      "|    policy_gradient_loss | 0.00652      |\n",
      "|    value_loss           | 0.354        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=370.20 +/- 132.54\n",
      "Episode length: 370.20 +/- 132.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 370      |\n",
      "|    mean_reward     | 370      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=468.60 +/- 62.80\n",
      "Episode length: 468.60 +/- 62.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 469      |\n",
      "|    mean_reward     | 469      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=455.40 +/- 31.74\n",
      "Episode length: 455.40 +/- 31.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 455      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 504   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 18530 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=204.80 +/- 76.49\n",
      "Episode length: 204.80 +/- 76.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 205         |\n",
      "|    mean_reward          | 205         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009120105 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.0784      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.007       |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=166.80 +/- 18.48\n",
      "Episode length: 166.80 +/- 18.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 167      |\n",
      "|    mean_reward     | 167      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=174.40 +/- 19.01\n",
      "Episode length: 174.40 +/- 19.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 174      |\n",
      "|    mean_reward     | 174      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 524   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 20383 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=206.00 +/- 38.04\n",
      "Episode length: 206.00 +/- 38.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 206         |\n",
      "|    mean_reward          | 206         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004923834 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.0591      |\n",
      "|    n_updates            | 11          |\n",
      "|    policy_gradient_loss | 0.000869    |\n",
      "|    value_loss           | 0.262       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=220.60 +/- 63.32\n",
      "Episode length: 220.60 +/- 63.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 221      |\n",
      "|    mean_reward     | 221      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=280.20 +/- 97.17\n",
      "Episode length: 280.20 +/- 97.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=266.40 +/- 79.13\n",
      "Episode length: 266.40 +/- 79.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 266      |\n",
      "|    mean_reward     | 266      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 530   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 22236 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=400.40 +/- 126.02\n",
      "Episode length: 400.40 +/- 126.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 400         |\n",
      "|    mean_reward          | 400         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004042655 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | 0.00573     |\n",
      "|    value_loss           | 0.254       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=392.00 +/- 114.76\n",
      "Episode length: 392.00 +/- 114.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 392      |\n",
      "|    mean_reward     | 392      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=353.80 +/- 131.33\n",
      "Episode length: 353.80 +/- 131.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 354      |\n",
      "|    mean_reward     | 354      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=237.00 +/- 72.70\n",
      "Episode length: 237.00 +/- 72.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 237      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 525   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 24089 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=227.00 +/- 91.66\n",
      "Episode length: 227.00 +/- 91.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 227         |\n",
      "|    mean_reward          | 227         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011997173 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.0803      |\n",
      "|    n_updates            | 13          |\n",
      "|    policy_gradient_loss | 0.00724     |\n",
      "|    value_loss           | 0.43        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=198.80 +/- 33.63\n",
      "Episode length: 198.80 +/- 33.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 199      |\n",
      "|    mean_reward     | 199      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=268.20 +/- 66.92\n",
      "Episode length: 268.20 +/- 66.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 268      |\n",
      "|    mean_reward     | 268      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 537   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 25942 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=189.60 +/- 11.38\n",
      "Episode length: 189.60 +/- 11.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 190          |\n",
      "|    mean_reward          | 190          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054336567 |\n",
      "|    clip_fraction        | 0.135        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.454       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 0.0333       |\n",
      "|    n_updates            | 14           |\n",
      "|    policy_gradient_loss | 0.00493      |\n",
      "|    value_loss           | 0.0978       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=169.60 +/- 18.61\n",
      "Episode length: 169.60 +/- 18.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 170      |\n",
      "|    mean_reward     | 170      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=258.40 +/- 130.86\n",
      "Episode length: 258.40 +/- 130.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 258      |\n",
      "|    mean_reward     | 258      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=256.20 +/- 99.50\n",
      "Episode length: 256.20 +/- 99.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 546   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 27795 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=252.20 +/- 110.53\n",
      "Episode length: 252.20 +/- 110.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 252         |\n",
      "|    mean_reward          | 252         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004893505 |\n",
      "|    clip_fraction        | 0.0955      |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.414       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | 0.0038      |\n",
      "|    value_loss           | 0.407       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=325.80 +/- 143.42\n",
      "Episode length: 325.80 +/- 143.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 326      |\n",
      "|    mean_reward     | 326      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=228.80 +/- 66.62\n",
      "Episode length: 228.80 +/- 66.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=368.80 +/- 147.40\n",
      "Episode length: 368.80 +/- 147.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 369      |\n",
      "|    mean_reward     | 369      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 546   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 29648 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=188.20 +/- 91.15\n",
      "Episode length: 188.20 +/- 91.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 188          |\n",
      "|    mean_reward          | 188          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069617536 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 0.0581       |\n",
      "|    n_updates            | 16           |\n",
      "|    policy_gradient_loss | 0.00582      |\n",
      "|    value_loss           | 0.23         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=134.80 +/- 6.58\n",
      "Episode length: 134.80 +/- 6.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 135      |\n",
      "|    mean_reward     | 135      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=156.80 +/- 18.88\n",
      "Episode length: 156.80 +/- 18.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 157      |\n",
      "|    mean_reward     | 157      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=151.80 +/- 29.75\n",
      "Episode length: 151.80 +/- 29.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 152      |\n",
      "|    mean_reward     | 152      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 555   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 31501 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=220.00 +/- 140.70\n",
      "Episode length: 220.00 +/- 140.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 220         |\n",
      "|    mean_reward          | 220         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006777564 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.139       |\n",
      "|    n_updates            | 17          |\n",
      "|    policy_gradient_loss | 0.00222     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=158.60 +/- 4.88\n",
      "Episode length: 158.60 +/- 4.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 159      |\n",
      "|    mean_reward     | 159      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=151.60 +/- 14.47\n",
      "Episode length: 151.60 +/- 14.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 152      |\n",
      "|    mean_reward     | 152      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 561   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 33354 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=235.00 +/- 138.10\n",
      "Episode length: 235.00 +/- 138.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 235         |\n",
      "|    mean_reward          | 235         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007444083 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=157.40 +/- 13.72\n",
      "Episode length: 157.40 +/- 13.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 157      |\n",
      "|    mean_reward     | 157      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=209.20 +/- 117.98\n",
      "Episode length: 209.20 +/- 117.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 209      |\n",
      "|    mean_reward     | 209      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=166.20 +/- 35.26\n",
      "Episode length: 166.20 +/- 35.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 166      |\n",
      "|    mean_reward     | 166      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 567   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 35207 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=242.00 +/- 129.07\n",
      "Episode length: 242.00 +/- 129.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 242         |\n",
      "|    mean_reward          | 242         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004696602 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 19          |\n",
      "|    policy_gradient_loss | 0.00951     |\n",
      "|    value_loss           | 0.0586      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=159.00 +/- 12.70\n",
      "Episode length: 159.00 +/- 12.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 159      |\n",
      "|    mean_reward     | 159      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=274.60 +/- 130.29\n",
      "Episode length: 274.60 +/- 130.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 275      |\n",
      "|    mean_reward     | 275      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=223.00 +/- 93.27\n",
      "Episode length: 223.00 +/- 93.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 223      |\n",
      "|    mean_reward     | 223      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 568   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 37060 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=230.80 +/- 85.17\n",
      "Episode length: 230.80 +/- 85.17\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 231          |\n",
      "|    mean_reward          | 231          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074195275 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.415       |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 0.003        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.0105       |\n",
      "|    value_loss           | 0.037        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=253.40 +/- 116.21\n",
      "Episode length: 253.40 +/- 116.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 253      |\n",
      "|    mean_reward     | 253      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=327.40 +/- 128.85\n",
      "Episode length: 327.40 +/- 128.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 327      |\n",
      "|    mean_reward     | 327      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 568   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 38913 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=425.00 +/- 123.49\n",
      "Episode length: 425.00 +/- 123.49\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 425          |\n",
      "|    mean_reward          | 425          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018574146 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.94         |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | -0.00276     |\n",
      "|    n_updates            | 21           |\n",
      "|    policy_gradient_loss | 0.00423      |\n",
      "|    value_loss           | 0.147        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=490.60 +/- 18.80\n",
      "Episode length: 490.60 +/- 18.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 491      |\n",
      "|    mean_reward     | 491      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=387.80 +/- 137.77\n",
      "Episode length: 387.80 +/- 137.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 388      |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=496.00 +/- 8.00\n",
      "Episode length: 496.00 +/- 8.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 496      |\n",
      "|    mean_reward     | 496      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 556   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 40766 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=340.60 +/- 112.88\n",
      "Episode length: 340.60 +/- 112.88\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 341         |\n",
      "|    mean_reward          | 341         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011766921 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.407      |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.178       |\n",
      "|    n_updates            | 22          |\n",
      "|    policy_gradient_loss | 0.00278     |\n",
      "|    value_loss           | 0.288       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=441.00 +/- 118.00\n",
      "Episode length: 441.00 +/- 118.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 441      |\n",
      "|    mean_reward     | 441      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=459.80 +/- 69.92\n",
      "Episode length: 459.80 +/- 69.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 460      |\n",
      "|    mean_reward     | 460      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=364.20 +/- 110.52\n",
      "Episode length: 364.20 +/- 110.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 364      |\n",
      "|    mean_reward     | 364      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 541   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 42619 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=307.40 +/- 125.52\n",
      "Episode length: 307.40 +/- 125.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 307         |\n",
      "|    mean_reward          | 307         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 43000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004397099 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | -0.00313    |\n",
      "|    n_updates            | 23          |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    value_loss           | 0.268       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=347.60 +/- 128.45\n",
      "Episode length: 347.60 +/- 128.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 348      |\n",
      "|    mean_reward     | 348      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=378.20 +/- 148.83\n",
      "Episode length: 378.20 +/- 148.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 378      |\n",
      "|    mean_reward     | 378      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 542   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 44472 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=246.80 +/- 127.77\n",
      "Episode length: 246.80 +/- 127.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 247          |\n",
      "|    mean_reward          | 247          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063806577 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.423       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | -0.00144     |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | 0.00485      |\n",
      "|    value_loss           | 0.0735       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=246.20 +/- 129.75\n",
      "Episode length: 246.20 +/- 129.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 246      |\n",
      "|    mean_reward     | 246      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=169.20 +/- 11.14\n",
      "Episode length: 169.20 +/- 11.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 169      |\n",
      "|    mean_reward     | 169      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=247.60 +/- 119.62\n",
      "Episode length: 247.60 +/- 119.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 248      |\n",
      "|    mean_reward     | 248      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 546   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 84    |\n",
      "|    total_timesteps | 46325 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=356.60 +/- 133.08\n",
      "Episode length: 356.60 +/- 133.08\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 357          |\n",
      "|    mean_reward          | 357          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 46500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044046175 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 0.0101       |\n",
      "|    n_updates            | 25           |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    value_loss           | 0.0654       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=357.00 +/- 144.13\n",
      "Episode length: 357.00 +/- 144.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 357      |\n",
      "|    mean_reward     | 357      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=319.00 +/- 127.25\n",
      "Episode length: 319.00 +/- 127.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 319      |\n",
      "|    mean_reward     | 319      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=314.40 +/- 151.20\n",
      "Episode length: 314.40 +/- 151.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 314      |\n",
      "|    mean_reward     | 314      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 545   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 88    |\n",
      "|    total_timesteps | 48178 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=427.20 +/- 117.00\n",
      "Episode length: 427.20 +/- 117.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 427         |\n",
      "|    mean_reward          | 427         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006042308 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | -0.00948    |\n",
      "|    n_updates            | 26          |\n",
      "|    policy_gradient_loss | 0.00559     |\n",
      "|    value_loss           | 0.0244      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=440.40 +/- 119.20\n",
      "Episode length: 440.40 +/- 119.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 440      |\n",
      "|    mean_reward     | 440      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=377.20 +/- 151.84\n",
      "Episode length: 377.20 +/- 151.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=249.20 +/- 107.95\n",
      "Episode length: 249.20 +/- 107.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 249      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 539   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 92    |\n",
      "|    total_timesteps | 50031 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=274.40 +/- 138.76\n",
      "Episode length: 274.40 +/- 138.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 274         |\n",
      "|    mean_reward          | 274         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004907514 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | 0.00391     |\n",
      "|    value_loss           | 0.0929      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=252.80 +/- 112.54\n",
      "Episode length: 252.80 +/- 112.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 253      |\n",
      "|    mean_reward     | 253      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=240.40 +/- 118.79\n",
      "Episode length: 240.40 +/- 118.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 240      |\n",
      "|    mean_reward     | 240      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 544   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 95    |\n",
      "|    total_timesteps | 51884 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=282.80 +/- 127.10\n",
      "Episode length: 282.80 +/- 127.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 283         |\n",
      "|    mean_reward          | 283         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006410096 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | -0.00221    |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.000815   |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=276.00 +/- 141.96\n",
      "Episode length: 276.00 +/- 141.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 276      |\n",
      "|    mean_reward     | 276      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=226.40 +/- 110.61\n",
      "Episode length: 226.40 +/- 110.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 226      |\n",
      "|    mean_reward     | 226      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=219.60 +/- 103.49\n",
      "Episode length: 219.60 +/- 103.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 220      |\n",
      "|    mean_reward     | 220      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 543   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 53737 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=374.80 +/- 116.25\n",
      "Episode length: 374.80 +/- 116.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 375          |\n",
      "|    mean_reward          | 375          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058611427 |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | -0.000837    |\n",
      "|    n_updates            | 29           |\n",
      "|    policy_gradient_loss | 0.00652      |\n",
      "|    value_loss           | 0.21         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=401.20 +/- 111.73\n",
      "Episode length: 401.20 +/- 111.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 401      |\n",
      "|    mean_reward     | 401      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=366.40 +/- 133.48\n",
      "Episode length: 366.40 +/- 133.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 366      |\n",
      "|    mean_reward     | 366      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=322.20 +/- 137.05\n",
      "Episode length: 322.20 +/- 137.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 322      |\n",
      "|    mean_reward     | 322      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 540   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 102   |\n",
      "|    total_timesteps | 55590 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=165.20 +/- 17.65\n",
      "Episode length: 165.20 +/- 17.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 165         |\n",
      "|    mean_reward          | 165         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009682714 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.00347     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.000496    |\n",
      "|    value_loss           | 0.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=176.80 +/- 48.19\n",
      "Episode length: 176.80 +/- 48.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 177      |\n",
      "|    mean_reward     | 177      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=235.60 +/- 133.82\n",
      "Episode length: 235.60 +/- 133.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 546   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 105   |\n",
      "|    total_timesteps | 57443 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=218.80 +/- 107.75\n",
      "Episode length: 218.80 +/- 107.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 219         |\n",
      "|    mean_reward          | 219         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 57500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011624194 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.137       |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00523     |\n",
      "|    loss                 | 0.585       |\n",
      "|    n_updates            | 31          |\n",
      "|    policy_gradient_loss | 0.00485     |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=223.20 +/- 138.99\n",
      "Episode length: 223.20 +/- 138.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 223      |\n",
      "|    mean_reward     | 223      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=153.80 +/- 12.84\n",
      "Episode length: 153.80 +/- 12.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 154      |\n",
      "|    mean_reward     | 154      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=159.60 +/- 14.84\n",
      "Episode length: 159.60 +/- 14.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 160      |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 550   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 107   |\n",
      "|    total_timesteps | 59296 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=239.80 +/- 131.07\n",
      "Episode length: 239.80 +/- 131.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 240          |\n",
      "|    mean_reward          | 240          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 59500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0082535455 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.595        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 0.173        |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 0.959        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=256.00 +/- 98.34\n",
      "Episode length: 256.00 +/- 98.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=285.60 +/- 124.39\n",
      "Episode length: 285.60 +/- 124.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 286      |\n",
      "|    mean_reward     | 286      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=267.80 +/- 109.00\n",
      "Episode length: 267.80 +/- 109.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 268      |\n",
      "|    mean_reward     | 268      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 552   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 110   |\n",
      "|    total_timesteps | 61149 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=190.60 +/- 57.66\n",
      "Episode length: 190.60 +/- 57.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 191          |\n",
      "|    mean_reward          | 191          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 61500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077782017 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.137        |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.00523      |\n",
      "|    loss                 | 0.0133       |\n",
      "|    n_updates            | 33           |\n",
      "|    policy_gradient_loss | 0.00194      |\n",
      "|    value_loss           | 0.25         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=296.40 +/- 129.82\n",
      "Episode length: 296.40 +/- 129.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=227.40 +/- 82.60\n",
      "Episode length: 227.40 +/- 82.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 227      |\n",
      "|    mean_reward     | 227      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=178.80 +/- 16.63\n",
      "Episode length: 178.80 +/- 16.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 179      |\n",
      "|    mean_reward     | 179      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 556   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 113   |\n",
      "|    total_timesteps | 63002 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=151.20 +/- 14.92\n",
      "Episode length: 151.20 +/- 14.92\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 151        |\n",
      "|    mean_reward          | 151        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 63500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00509247 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.137      |\n",
      "|    entropy_loss         | -0.492     |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 0.00523    |\n",
      "|    loss                 | 0.116      |\n",
      "|    n_updates            | 34         |\n",
      "|    policy_gradient_loss | 0.00361    |\n",
      "|    value_loss           | 0.253      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=150.60 +/- 10.29\n",
      "Episode length: 150.60 +/- 10.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 151      |\n",
      "|    mean_reward     | 151      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=159.80 +/- 17.72\n",
      "Episode length: 159.80 +/- 17.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 160      |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 563   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 115   |\n",
      "|    total_timesteps | 64855 |\n",
      "------------------------------\n",
      "Single environment training took 115.20 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>204.9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-10</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/x7wb0wjf' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/x7wb0wjf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_133406-x7wb0wjf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z7wtrzz1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.196986950898157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0031477170199456597\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9184080285739772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.915802251699358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005392630155081313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 9.547772011181069\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1711\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 69478\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_133615-z7wtrzz1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/z7wtrzz1' target=\"_blank\">jumping-sweep-11</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/z7wtrzz1' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/z7wtrzz1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1711`, after every 26 untruncated mini-batches, there will be a truncated mini-batch of size 47\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1711 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=9.40 +/- 0.80\n",
      "Episode length: 9.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=9.20 +/- 0.75\n",
      "Episode length: 9.20 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 9.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=8.20 +/- 0.40\n",
      "Episode length: 8.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.2      |\n",
      "|    mean_reward     | 8.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1494 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1711 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=160.20 +/- 145.12\n",
      "Episode length: 160.20 +/- 145.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 160         |\n",
      "|    mean_reward          | 160         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009709813 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | -0.00311    |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.625       |\n",
      "|    n_updates            | 2           |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 6.32        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=176.20 +/- 152.35\n",
      "Episode length: 176.20 +/- 152.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 176      |\n",
      "|    mean_reward     | 176      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=167.40 +/- 145.39\n",
      "Episode length: 167.40 +/- 145.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 167      |\n",
      "|    mean_reward     | 167      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 954  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 3422 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=179.80 +/- 6.82\n",
      "Episode length: 179.80 +/- 6.82\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 180        |\n",
      "|    mean_reward          | 180        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02210808 |\n",
      "|    clip_fraction        | 0.317      |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.644     |\n",
      "|    explained_variance   | 0.341      |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.776      |\n",
      "|    n_updates            | 4          |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 2.91       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=224.00 +/- 67.47\n",
      "Episode length: 224.00 +/- 67.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 224      |\n",
      "|    mean_reward     | 224      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=172.80 +/- 34.42\n",
      "Episode length: 172.80 +/- 34.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 173      |\n",
      "|    mean_reward     | 173      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=201.40 +/- 46.74\n",
      "Episode length: 201.40 +/- 46.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 829  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 5133 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=259.20 +/- 61.19\n",
      "Episode length: 259.20 +/- 61.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 259         |\n",
      "|    mean_reward          | 259         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022820704 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.844       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=251.00 +/- 94.38\n",
      "Episode length: 251.00 +/- 94.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=262.40 +/- 80.67\n",
      "Episode length: 262.40 +/- 80.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 262      |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 780  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 6844 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=352.40 +/- 91.75\n",
      "Episode length: 352.40 +/- 91.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 352         |\n",
      "|    mean_reward          | 352         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032656416 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    value_loss           | 0.947       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7500, episode_reward=297.80 +/- 73.50\n",
      "Episode length: 297.80 +/- 73.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 298      |\n",
      "|    mean_reward     | 298      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=368.80 +/- 93.62\n",
      "Episode length: 368.80 +/- 93.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 369      |\n",
      "|    mean_reward     | 369      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8500, episode_reward=262.80 +/- 37.89\n",
      "Episode length: 262.80 +/- 37.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 263      |\n",
      "|    mean_reward     | 263      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 682  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 8555 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=358.20 +/- 97.60\n",
      "Episode length: 358.20 +/- 97.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 358         |\n",
      "|    mean_reward          | 358         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019748123 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0321      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 0.426       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=460.80 +/- 75.92\n",
      "Episode length: 460.80 +/- 75.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 461      |\n",
      "|    mean_reward     | 461      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=373.60 +/- 32.21\n",
      "Episode length: 373.60 +/- 32.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 623   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 10266 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=438.60 +/- 60.69\n",
      "Episode length: 438.60 +/- 60.69\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 439        |\n",
      "|    mean_reward          | 439        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00819717 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.554     |\n",
      "|    explained_variance   | 0.8        |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.114      |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | 0.00376    |\n",
      "|    value_loss           | 0.402      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=498.60 +/- 2.80\n",
      "Episode length: 498.60 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 499      |\n",
      "|    mean_reward     | 499      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11500, episode_reward=463.40 +/- 43.05\n",
      "Episode length: 463.40 +/- 43.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 463      |\n",
      "|    mean_reward     | 463      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 577   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 11977 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=419.40 +/- 78.10\n",
      "Episode length: 419.40 +/- 78.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 419          |\n",
      "|    mean_reward          | 419          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046472293 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.533       |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.0759       |\n",
      "|    n_updates            | 14           |\n",
      "|    policy_gradient_loss | 0.00329      |\n",
      "|    value_loss           | 0.468        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=450.80 +/- 62.81\n",
      "Episode length: 450.80 +/- 62.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 451      |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=459.60 +/- 80.80\n",
      "Episode length: 459.60 +/- 80.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 460      |\n",
      "|    mean_reward     | 460      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=455.00 +/- 90.00\n",
      "Episode length: 455.00 +/- 90.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 455      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 529   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 13688 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=441.20 +/- 80.48\n",
      "Episode length: 441.20 +/- 80.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 441         |\n",
      "|    mean_reward          | 441         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004824362 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 0.00217     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=482.20 +/- 22.61\n",
      "Episode length: 482.20 +/- 22.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 482      |\n",
      "|    mean_reward     | 482      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=472.20 +/- 36.90\n",
      "Episode length: 472.20 +/- 36.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 472      |\n",
      "|    mean_reward     | 472      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 15399 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=415.20 +/- 80.70\n",
      "Episode length: 415.20 +/- 80.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 415          |\n",
      "|    mean_reward          | 415          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070765014 |\n",
      "|    clip_fraction        | 0.166        |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.511       |\n",
      "|    explained_variance   | 0.854        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.0755       |\n",
      "|    n_updates            | 18           |\n",
      "|    policy_gradient_loss | 0.00664      |\n",
      "|    value_loss           | 0.255        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=362.00 +/- 44.50\n",
      "Episode length: 362.00 +/- 44.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 362      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=365.00 +/- 86.94\n",
      "Episode length: 365.00 +/- 86.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 365      |\n",
      "|    mean_reward     | 365      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=383.80 +/- 98.66\n",
      "Episode length: 383.80 +/- 98.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 384      |\n",
      "|    mean_reward     | 384      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 502   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 17110 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=381.20 +/- 85.24\n",
      "Episode length: 381.20 +/- 85.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 381         |\n",
      "|    mean_reward          | 381         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008392726 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00276     |\n",
      "|    value_loss           | 0.108       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=369.00 +/- 63.70\n",
      "Episode length: 369.00 +/- 63.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 369      |\n",
      "|    mean_reward     | 369      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=396.80 +/- 92.15\n",
      "Episode length: 396.80 +/- 92.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 397      |\n",
      "|    mean_reward     | 397      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 505   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 18821 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=288.60 +/- 52.38\n",
      "Episode length: 288.60 +/- 52.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 289         |\n",
      "|    mean_reward          | 289         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007479235 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.351       |\n",
      "|    n_updates            | 22          |\n",
      "|    policy_gradient_loss | 0.00118     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=332.80 +/- 104.19\n",
      "Episode length: 332.80 +/- 104.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 333      |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=269.60 +/- 41.64\n",
      "Episode length: 269.60 +/- 41.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 270      |\n",
      "|    mean_reward     | 270      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=323.00 +/- 99.19\n",
      "Episode length: 323.00 +/- 99.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 323      |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 504   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 20532 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=322.60 +/- 56.31\n",
      "Episode length: 322.60 +/- 56.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 323         |\n",
      "|    mean_reward          | 323         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004315173 |\n",
      "|    clip_fraction        | 0.0732      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.000429   |\n",
      "|    value_loss           | 0.084       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=306.80 +/- 102.33\n",
      "Episode length: 306.80 +/- 102.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 307      |\n",
      "|    mean_reward     | 307      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=256.40 +/- 14.04\n",
      "Episode length: 256.40 +/- 14.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 510   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 22243 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=310.80 +/- 57.80\n",
      "Episode length: 310.80 +/- 57.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 311         |\n",
      "|    mean_reward          | 311         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014929871 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.452      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | -0.00571    |\n",
      "|    n_updates            | 26          |\n",
      "|    policy_gradient_loss | 0.00886     |\n",
      "|    value_loss           | 0.0577      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=406.80 +/- 118.42\n",
      "Episode length: 406.80 +/- 118.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 407      |\n",
      "|    mean_reward     | 407      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=318.20 +/- 68.78\n",
      "Episode length: 318.20 +/- 68.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 318      |\n",
      "|    mean_reward     | 318      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 23954 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=401.40 +/- 98.84\n",
      "Episode length: 401.40 +/- 98.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 401         |\n",
      "|    mean_reward          | 401         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011128141 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | 0.00857     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=385.80 +/- 93.79\n",
      "Episode length: 385.80 +/- 93.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 386      |\n",
      "|    mean_reward     | 386      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=451.80 +/- 96.40\n",
      "Episode length: 451.80 +/- 96.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 452      |\n",
      "|    mean_reward     | 452      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=461.80 +/- 67.30\n",
      "Episode length: 461.80 +/- 67.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 25665 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=427.40 +/- 104.07\n",
      "Episode length: 427.40 +/- 104.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 427         |\n",
      "|    mean_reward          | 427         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010358085 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.441      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    value_loss           | 0.0786      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=27000, episode_reward=496.00 +/- 8.00\n",
      "Episode length: 496.00 +/- 8.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 496      |\n",
      "|    mean_reward     | 496      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 496   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 27376 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=474.20 +/- 51.60\n",
      "Episode length: 474.20 +/- 51.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 474         |\n",
      "|    mean_reward          | 474         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011948915 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | 0.00456     |\n",
      "|    value_loss           | 0.21        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=456.60 +/- 86.30\n",
      "Episode length: 456.60 +/- 86.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 457      |\n",
      "|    mean_reward     | 457      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=455.60 +/- 79.18\n",
      "Episode length: 455.60 +/- 79.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 456      |\n",
      "|    mean_reward     | 456      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=430.80 +/- 92.10\n",
      "Episode length: 430.80 +/- 92.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 431      |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 29087 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=374.40 +/- 143.98\n",
      "Episode length: 374.40 +/- 143.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 374         |\n",
      "|    mean_reward          | 374         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017904188 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.143       |\n",
      "|    n_updates            | 34          |\n",
      "|    policy_gradient_loss | 0.0147      |\n",
      "|    value_loss           | 0.343       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=426.40 +/- 92.97\n",
      "Episode length: 426.40 +/- 92.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 426      |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=374.00 +/- 104.34\n",
      "Episode length: 374.00 +/- 104.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 30798 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=441.40 +/- 78.05\n",
      "Episode length: 441.40 +/- 78.05\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 441        |\n",
      "|    mean_reward          | 441        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 31000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00860974 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.41      |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.0388     |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | 0.000339   |\n",
      "|    value_loss           | 0.273      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=365.20 +/- 114.37\n",
      "Episode length: 365.20 +/- 114.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 365      |\n",
      "|    mean_reward     | 365      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=420.40 +/- 97.78\n",
      "Episode length: 420.40 +/- 97.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 420      |\n",
      "|    mean_reward     | 420      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=382.20 +/- 105.49\n",
      "Episode length: 382.20 +/- 105.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 382      |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 471   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 32509 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=499.80 +/- 0.40\n",
      "Episode length: 499.80 +/- 0.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074198125 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.00883      |\n",
      "|    n_updates            | 38           |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 0.164        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=442.40 +/- 110.75\n",
      "Episode length: 442.40 +/- 110.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 442      |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=451.20 +/- 88.87\n",
      "Episode length: 451.20 +/- 88.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 451      |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 470   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 34220 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=286.80 +/- 79.50\n",
      "Episode length: 286.80 +/- 79.50\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 287        |\n",
      "|    mean_reward          | 287        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 34500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00965452 |\n",
      "|    clip_fraction        | 0.095      |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.437     |\n",
      "|    explained_variance   | 0.914      |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.0737     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.00764    |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=308.00 +/- 102.97\n",
      "Episode length: 308.00 +/- 102.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 308      |\n",
      "|    mean_reward     | 308      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=247.60 +/- 55.42\n",
      "Episode length: 247.60 +/- 55.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 248      |\n",
      "|    mean_reward     | 248      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 35931 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=481.80 +/- 27.51\n",
      "Episode length: 481.80 +/- 27.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 482        |\n",
      "|    mean_reward          | 482        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01009736 |\n",
      "|    clip_fraction        | 0.0858     |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.481     |\n",
      "|    explained_variance   | 0.889      |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.164      |\n",
      "|    n_updates            | 42         |\n",
      "|    policy_gradient_loss | -0.00947   |\n",
      "|    value_loss           | 0.377      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=377.00 +/- 105.24\n",
      "Episode length: 377.00 +/- 105.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=411.40 +/- 115.27\n",
      "Episode length: 411.40 +/- 115.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 411      |\n",
      "|    mean_reward     | 411      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=421.60 +/- 95.65\n",
      "Episode length: 421.60 +/- 95.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 422      |\n",
      "|    mean_reward     | 422      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 468   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 37642 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=300.00 +/- 104.76\n",
      "Episode length: 300.00 +/- 104.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 300         |\n",
      "|    mean_reward          | 300         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006190863 |\n",
      "|    clip_fraction        | 0.0897      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.123       |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | 0.000584    |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=429.40 +/- 50.82\n",
      "Episode length: 429.40 +/- 50.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 429      |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=428.20 +/- 87.95\n",
      "Episode length: 428.20 +/- 87.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 428      |\n",
      "|    mean_reward     | 428      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 463   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 84    |\n",
      "|    total_timesteps | 39353 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=380.20 +/- 91.28\n",
      "Episode length: 380.20 +/- 91.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 380         |\n",
      "|    mean_reward          | 380         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006728905 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 46          |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 0.0949      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=424.20 +/- 86.93\n",
      "Episode length: 424.20 +/- 86.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 424      |\n",
      "|    mean_reward     | 424      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=374.40 +/- 102.70\n",
      "Episode length: 374.40 +/- 102.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=298.20 +/- 106.37\n",
      "Episode length: 298.20 +/- 106.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 298      |\n",
      "|    mean_reward     | 298      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 454   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 90    |\n",
      "|    total_timesteps | 41064 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=374.00 +/- 142.34\n",
      "Episode length: 374.00 +/- 142.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 374        |\n",
      "|    mean_reward          | 374        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 41500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00491846 |\n",
      "|    clip_fraction        | 0.0502     |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.409     |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.059      |\n",
      "|    n_updates            | 48         |\n",
      "|    policy_gradient_loss | 0.00634    |\n",
      "|    value_loss           | 0.0937     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=447.80 +/- 104.40\n",
      "Episode length: 447.80 +/- 104.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 448      |\n",
      "|    mean_reward     | 448      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=370.40 +/- 132.39\n",
      "Episode length: 370.40 +/- 132.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 370      |\n",
      "|    mean_reward     | 370      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 448   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 95    |\n",
      "|    total_timesteps | 42775 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=406.40 +/- 120.56\n",
      "Episode length: 406.40 +/- 120.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 406          |\n",
      "|    mean_reward          | 406          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046637543 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 0.976        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | -0.0171      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | 0.00407      |\n",
      "|    value_loss           | 0.056        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=419.40 +/- 90.91\n",
      "Episode length: 419.40 +/- 90.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 419      |\n",
      "|    mean_reward     | 419      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 449   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 44486 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=388.00 +/- 116.51\n",
      "Episode length: 388.00 +/- 116.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 388         |\n",
      "|    mean_reward          | 388         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011554545 |\n",
      "|    clip_fraction        | 0.0969      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.443      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | -0.00346    |\n",
      "|    value_loss           | 0.0786      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=376.80 +/- 139.30\n",
      "Episode length: 376.80 +/- 139.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=267.20 +/- 111.44\n",
      "Episode length: 267.20 +/- 111.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 267      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=290.60 +/- 120.40\n",
      "Episode length: 290.60 +/- 120.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 291      |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 450   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 102   |\n",
      "|    total_timesteps | 46197 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=449.60 +/- 67.20\n",
      "Episode length: 449.60 +/- 67.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 450          |\n",
      "|    mean_reward          | 450          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 46500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038321072 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.00255      |\n",
      "|    n_updates            | 54           |\n",
      "|    policy_gradient_loss | 0.000393     |\n",
      "|    value_loss           | 0.0322       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=467.60 +/- 64.80\n",
      "Episode length: 467.60 +/- 64.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 468      |\n",
      "|    mean_reward     | 468      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=413.20 +/- 71.06\n",
      "Episode length: 413.20 +/- 71.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 413      |\n",
      "|    mean_reward     | 413      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 450   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 106   |\n",
      "|    total_timesteps | 47908 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013956536 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0356      |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    value_loss           | 0.0755      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=459.40 +/- 81.20\n",
      "Episode length: 459.40 +/- 81.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 459      |\n",
      "|    mean_reward     | 459      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=382.40 +/- 101.19\n",
      "Episode length: 382.40 +/- 101.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 382      |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 444   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 111   |\n",
      "|    total_timesteps | 49619 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=457.00 +/- 57.25\n",
      "Episode length: 457.00 +/- 57.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 457         |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004658603 |\n",
      "|    clip_fraction        | 0.0861      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0378      |\n",
      "|    n_updates            | 58          |\n",
      "|    policy_gradient_loss | 0.00255     |\n",
      "|    value_loss           | 0.0824      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=453.60 +/- 44.94\n",
      "Episode length: 453.60 +/- 44.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 454      |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=469.60 +/- 60.80\n",
      "Episode length: 469.60 +/- 60.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 470      |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 443   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 115   |\n",
      "|    total_timesteps | 51330 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 51500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022656547 |\n",
      "|    clip_fraction        | 0.0283       |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.184        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | 0.0012       |\n",
      "|    value_loss           | 0.0775       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=474.00 +/- 52.00\n",
      "Episode length: 474.00 +/- 52.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 474      |\n",
      "|    mean_reward     | 474      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=418.00 +/- 71.86\n",
      "Episode length: 418.00 +/- 71.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 418      |\n",
      "|    mean_reward     | 418      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 438   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 121   |\n",
      "|    total_timesteps | 53041 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=435.00 +/- 69.57\n",
      "Episode length: 435.00 +/- 69.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 435          |\n",
      "|    mean_reward          | 435          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 53500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056497534 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.348       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.0335       |\n",
      "|    n_updates            | 62           |\n",
      "|    policy_gradient_loss | 0.00561      |\n",
      "|    value_loss           | 0.0701       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=470.60 +/- 58.80\n",
      "Episode length: 470.60 +/- 58.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=425.00 +/- 85.27\n",
      "Episode length: 425.00 +/- 85.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 425      |\n",
      "|    mean_reward     | 425      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 437   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 125   |\n",
      "|    total_timesteps | 54752 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=496.20 +/- 7.60\n",
      "Episode length: 496.20 +/- 7.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 496         |\n",
      "|    mean_reward          | 496         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004945991 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.334      |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.164       |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | 0.00405     |\n",
      "|    value_loss           | 0.0521      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=441.20 +/- 76.33\n",
      "Episode length: 441.20 +/- 76.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 441      |\n",
      "|    mean_reward     | 441      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=497.40 +/- 5.20\n",
      "Episode length: 497.40 +/- 5.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 497      |\n",
      "|    mean_reward     | 497      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 434   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 129   |\n",
      "|    total_timesteps | 56463 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=459.80 +/- 51.62\n",
      "Episode length: 459.80 +/- 51.62\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 460          |\n",
      "|    mean_reward          | 460          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077129225 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.325       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.0381       |\n",
      "|    n_updates            | 66           |\n",
      "|    policy_gradient_loss | 0.00172      |\n",
      "|    value_loss           | 0.0581       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=440.60 +/- 73.00\n",
      "Episode length: 440.60 +/- 73.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 441      |\n",
      "|    mean_reward     | 441      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=431.20 +/- 111.73\n",
      "Episode length: 431.20 +/- 111.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 431      |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 429   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 135   |\n",
      "|    total_timesteps | 58174 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=374.60 +/- 122.06\n",
      "Episode length: 374.60 +/- 122.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 375         |\n",
      "|    mean_reward          | 375         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003978756 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.361      |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 68          |\n",
      "|    policy_gradient_loss | 0.00475     |\n",
      "|    value_loss           | 0.0318      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=375.80 +/- 122.10\n",
      "Episode length: 375.80 +/- 122.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 376      |\n",
      "|    mean_reward     | 376      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 139   |\n",
      "|    total_timesteps | 59885 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=373.00 +/- 112.35\n",
      "Episode length: 373.00 +/- 112.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 373         |\n",
      "|    mean_reward          | 373         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009687811 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.327      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | -0.0125     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.0065      |\n",
      "|    value_loss           | 0.027       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=451.00 +/- 98.00\n",
      "Episode length: 451.00 +/- 98.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 451      |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=466.80 +/- 66.40\n",
      "Episode length: 466.80 +/- 66.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 467      |\n",
      "|    mean_reward     | 467      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=389.00 +/- 103.76\n",
      "Episode length: 389.00 +/- 103.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 389      |\n",
      "|    mean_reward     | 389      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 429   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 143   |\n",
      "|    total_timesteps | 61596 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=473.00 +/- 54.00\n",
      "Episode length: 473.00 +/- 54.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 473        |\n",
      "|    mean_reward          | 473        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 62000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01860931 |\n",
      "|    clip_fraction        | 0.0858     |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.287     |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.0736     |\n",
      "|    n_updates            | 72         |\n",
      "|    policy_gradient_loss | 0.00816    |\n",
      "|    value_loss           | 0.0572     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=453.80 +/- 92.40\n",
      "Episode length: 453.80 +/- 92.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 454      |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 147   |\n",
      "|    total_timesteps | 63307 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 63500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04492298 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.197      |\n",
      "|    entropy_loss         | -0.231     |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.00539    |\n",
      "|    loss                 | 0.0579     |\n",
      "|    n_updates            | 74         |\n",
      "|    policy_gradient_loss | 0.0105     |\n",
      "|    value_loss           | 0.0382     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=471.60 +/- 49.19\n",
      "Episode length: 471.60 +/- 49.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 472      |\n",
      "|    mean_reward     | 472      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=499.60 +/- 0.80\n",
      "Episode length: 499.60 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=481.20 +/- 37.60\n",
      "Episode length: 481.20 +/- 37.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 481      |\n",
      "|    mean_reward     | 481      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 153   |\n",
      "|    total_timesteps | 65018 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=427.60 +/- 95.02\n",
      "Episode length: 427.60 +/- 95.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 428          |\n",
      "|    mean_reward          | 428          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 65500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072789085 |\n",
      "|    clip_fraction        | 0.0733       |\n",
      "|    clip_range           | 0.197        |\n",
      "|    entropy_loss         | -0.237       |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.00539      |\n",
      "|    loss                 | 0.00797      |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | 0.00385      |\n",
      "|    value_loss           | 0.0507       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=452.80 +/- 58.69\n",
      "Episode length: 452.80 +/- 58.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 453      |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 420   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 158   |\n",
      "|    total_timesteps | 66729 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=462.80 +/- 46.00\n",
      "Episode length: 462.80 +/- 46.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 463         |\n",
      "|    mean_reward          | 463         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 67000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009216354 |\n",
      "|    clip_fraction        | 0.0681      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.00911     |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.00131     |\n",
      "|    value_loss           | 0.0405      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 417   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 163   |\n",
      "|    total_timesteps | 68440 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=457.20 +/- 85.60\n",
      "Episode length: 457.20 +/- 85.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 457         |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013482052 |\n",
      "|    clip_fraction        | 0.0623      |\n",
      "|    clip_range           | 0.197       |\n",
      "|    entropy_loss         | -0.231      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00539     |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00378     |\n",
      "|    value_loss           | 0.0362      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=466.20 +/- 67.60\n",
      "Episode length: 466.20 +/- 67.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 466      |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=493.20 +/- 13.60\n",
      "Episode length: 493.20 +/- 13.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 493      |\n",
      "|    mean_reward     | 493      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 412   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 169   |\n",
      "|    total_timesteps | 70151 |\n",
      "------------------------------\n",
      "Single environment training took 170.20 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>498.6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-sweep-11</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/z7wtrzz1' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/z7wtrzz1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_133615-z7wtrzz1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jkwh301z with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.1849857630551267\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.007180567750631555\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9942681549969496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9506654249791108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.004436602202663294\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 8.707682615770127\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 81218\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_133922-jkwh301z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jkwh301z' target=\"_blank\">vocal-sweep-12</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jkwh301z' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jkwh301z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1748`, after every 27 untruncated mini-batches, there will be a truncated mini-batch of size 20\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1748 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=8.80 +/- 0.75\n",
      "Episode length: 8.80 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 8.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=9.40 +/- 0.80\n",
      "Episode length: 9.40 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=10.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 10       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1378 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1748 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=83.60 +/- 11.72\n",
      "Episode length: 83.60 +/- 11.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 83.6        |\n",
      "|    mean_reward          | 83.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011234532 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | -0.00772    |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 1.95        |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 25.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=130.40 +/- 76.90\n",
      "Episode length: 130.40 +/- 76.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 130      |\n",
      "|    mean_reward     | 130      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=220.00 +/- 175.91\n",
      "Episode length: 220.00 +/- 175.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 220      |\n",
      "|    mean_reward     | 220      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 968  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 3496 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=149.80 +/- 35.50\n",
      "Episode length: 149.80 +/- 35.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 150         |\n",
      "|    mean_reward          | 150         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017020987 |\n",
      "|    clip_fraction        | 0.333       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.366       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 6.6         |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    value_loss           | 11.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=212.40 +/- 84.47\n",
      "Episode length: 212.40 +/- 84.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 212      |\n",
      "|    mean_reward     | 212      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=204.40 +/- 78.16\n",
      "Episode length: 204.40 +/- 78.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 204      |\n",
      "|    mean_reward     | 204      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=177.60 +/- 43.46\n",
      "Episode length: 177.60 +/- 43.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 178      |\n",
      "|    mean_reward     | 178      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 801  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 5244 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=302.00 +/- 66.30\n",
      "Episode length: 302.00 +/- 66.30\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 302        |\n",
      "|    mean_reward          | 302        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01332018 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.185      |\n",
      "|    entropy_loss         | -0.588     |\n",
      "|    explained_variance   | 0.527      |\n",
      "|    learning_rate        | 0.00444    |\n",
      "|    loss                 | 6.01       |\n",
      "|    n_updates            | 9          |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    value_loss           | 11.8       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=185.00 +/- 52.21\n",
      "Episode length: 185.00 +/- 52.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 185      |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=236.00 +/- 137.22\n",
      "Episode length: 236.00 +/- 137.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 675  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 6992 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=260.40 +/- 98.33\n",
      "Episode length: 260.40 +/- 98.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 260         |\n",
      "|    mean_reward          | 260         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016132658 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 2.5         |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 9.32        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=271.00 +/- 69.88\n",
      "Episode length: 271.00 +/- 69.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 271      |\n",
      "|    mean_reward     | 271      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=285.00 +/- 71.08\n",
      "Episode length: 285.00 +/- 71.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 285      |\n",
      "|    mean_reward     | 285      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=293.60 +/- 113.92\n",
      "Episode length: 293.60 +/- 113.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 294      |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 608  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 8740 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=363.80 +/- 74.96\n",
      "Episode length: 363.80 +/- 74.96\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 364          |\n",
      "|    mean_reward          | 364          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084582325 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.697        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.647        |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 4.4          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9500, episode_reward=275.00 +/- 56.80\n",
      "Episode length: 275.00 +/- 56.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 275      |\n",
      "|    mean_reward     | 275      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=236.40 +/- 47.99\n",
      "Episode length: 236.40 +/- 47.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 580   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 10488 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=346.20 +/- 102.25\n",
      "Episode length: 346.20 +/- 102.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 346          |\n",
      "|    mean_reward          | 346          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069689765 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.968        |\n",
      "|    n_updates            | 18           |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    value_loss           | 3.45         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=295.60 +/- 104.11\n",
      "Episode length: 295.60 +/- 104.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=327.60 +/- 95.89\n",
      "Episode length: 327.60 +/- 95.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 328      |\n",
      "|    mean_reward     | 328      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=311.00 +/- 96.99\n",
      "Episode length: 311.00 +/- 96.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 311      |\n",
      "|    mean_reward     | 311      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 469   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 12236 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=273.00 +/- 78.66\n",
      "Episode length: 273.00 +/- 78.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 273         |\n",
      "|    mean_reward          | 273         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007360627 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.329       |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | -0.00769    |\n",
      "|    value_loss           | 1.24        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=327.00 +/- 79.97\n",
      "Episode length: 327.00 +/- 79.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 327      |\n",
      "|    mean_reward     | 327      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=355.20 +/- 90.66\n",
      "Episode length: 355.20 +/- 90.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 355      |\n",
      "|    mean_reward     | 355      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 414   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 13984 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=257.60 +/- 37.33\n",
      "Episode length: 257.60 +/- 37.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 258         |\n",
      "|    mean_reward          | 258         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009229043 |\n",
      "|    clip_fraction        | 0.0901      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 1.37        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00226    |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=302.40 +/- 101.68\n",
      "Episode length: 302.40 +/- 101.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 302      |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=344.20 +/- 129.56\n",
      "Episode length: 344.20 +/- 129.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 344      |\n",
      "|    mean_reward     | 344      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=265.00 +/- 36.01\n",
      "Episode length: 265.00 +/- 36.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 265      |\n",
      "|    mean_reward     | 265      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 396   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 15732 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=193.40 +/- 16.19\n",
      "Episode length: 193.40 +/- 16.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 193         |\n",
      "|    mean_reward          | 193         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008853552 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.843       |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    value_loss           | 3.71        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=188.40 +/- 18.28\n",
      "Episode length: 188.40 +/- 18.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 188      |\n",
      "|    mean_reward     | 188      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=201.20 +/- 16.36\n",
      "Episode length: 201.20 +/- 16.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 17480 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=187.00 +/- 7.80\n",
      "Episode length: 187.00 +/- 7.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 187          |\n",
      "|    mean_reward          | 187          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058309543 |\n",
      "|    clip_fraction        | 0.0913       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0.962        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 1.36         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000749     |\n",
      "|    value_loss           | 0.402        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=190.60 +/- 13.72\n",
      "Episode length: 190.60 +/- 13.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 191      |\n",
      "|    mean_reward     | 191      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=192.60 +/- 19.14\n",
      "Episode length: 192.60 +/- 19.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 193      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=193.40 +/- 11.62\n",
      "Episode length: 193.40 +/- 11.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 193      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 401   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 19228 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=211.00 +/- 15.65\n",
      "Episode length: 211.00 +/- 15.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 211         |\n",
      "|    mean_reward          | 211         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007684778 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.84        |\n",
      "|    n_updates            | 33          |\n",
      "|    policy_gradient_loss | 0.00568     |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=204.80 +/- 28.81\n",
      "Episode length: 204.80 +/- 28.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 205      |\n",
      "|    mean_reward     | 205      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=184.80 +/- 9.99\n",
      "Episode length: 184.80 +/- 9.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 185      |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 20976 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=208.20 +/- 3.12\n",
      "Episode length: 208.20 +/- 3.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 208         |\n",
      "|    mean_reward          | 208         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008026504 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    value_loss           | 2.79        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=201.40 +/- 21.90\n",
      "Episode length: 201.40 +/- 21.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=205.60 +/- 21.97\n",
      "Episode length: 205.60 +/- 21.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 206      |\n",
      "|    mean_reward     | 206      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=202.80 +/- 26.62\n",
      "Episode length: 202.80 +/- 26.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 203      |\n",
      "|    mean_reward     | 203      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 414   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 22724 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=258.20 +/- 14.12\n",
      "Episode length: 258.20 +/- 14.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 258          |\n",
      "|    mean_reward          | 258          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075243674 |\n",
      "|    clip_fraction        | 0.116        |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 3.1          |\n",
      "|    n_updates            | 39           |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    value_loss           | 2.52         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=247.20 +/- 7.81\n",
      "Episode length: 247.20 +/- 7.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 247      |\n",
      "|    mean_reward     | 247      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=250.80 +/- 11.30\n",
      "Episode length: 250.80 +/- 11.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 410   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 24472 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=290.80 +/- 12.89\n",
      "Episode length: 290.80 +/- 12.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 291         |\n",
      "|    mean_reward          | 291         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010830319 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.547      |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.336       |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 5.44        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=289.40 +/- 14.83\n",
      "Episode length: 289.40 +/- 14.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 289      |\n",
      "|    mean_reward     | 289      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=296.20 +/- 18.07\n",
      "Episode length: 296.20 +/- 18.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=291.80 +/- 13.42\n",
      "Episode length: 291.80 +/- 13.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 292      |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 396   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 26220 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=344.60 +/- 21.95\n",
      "Episode length: 344.60 +/- 21.95\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 345        |\n",
      "|    mean_reward          | 345        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01056584 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.185      |\n",
      "|    entropy_loss         | -0.536     |\n",
      "|    explained_variance   | 0.51       |\n",
      "|    learning_rate        | 0.00444    |\n",
      "|    loss                 | 1.54       |\n",
      "|    n_updates            | 45         |\n",
      "|    policy_gradient_loss | 0.00257    |\n",
      "|    value_loss           | 4.22       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=349.20 +/- 22.14\n",
      "Episode length: 349.20 +/- 22.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 349      |\n",
      "|    mean_reward     | 349      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=346.40 +/- 17.72\n",
      "Episode length: 346.40 +/- 17.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 346      |\n",
      "|    mean_reward     | 346      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 395   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 27968 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=390.40 +/- 32.66\n",
      "Episode length: 390.40 +/- 32.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 390         |\n",
      "|    mean_reward          | 390         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010180356 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.546      |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=28500, episode_reward=403.40 +/- 51.77\n",
      "Episode length: 403.40 +/- 51.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 403      |\n",
      "|    mean_reward     | 403      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29000, episode_reward=410.60 +/- 47.40\n",
      "Episode length: 410.60 +/- 47.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 411      |\n",
      "|    mean_reward     | 411      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=29500, episode_reward=411.60 +/- 48.37\n",
      "Episode length: 411.60 +/- 48.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 412      |\n",
      "|    mean_reward     | 412      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 381   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 29716 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=360.00 +/- 26.40\n",
      "Episode length: 360.00 +/- 26.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 360         |\n",
      "|    mean_reward          | 360         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004925319 |\n",
      "|    clip_fraction        | 0.0534      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.181       |\n",
      "|    n_updates            | 51          |\n",
      "|    policy_gradient_loss | 0.00238     |\n",
      "|    value_loss           | 0.728       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=361.40 +/- 24.43\n",
      "Episode length: 361.40 +/- 24.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 361      |\n",
      "|    mean_reward     | 361      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=356.20 +/- 21.84\n",
      "Episode length: 356.20 +/- 21.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 356      |\n",
      "|    mean_reward     | 356      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 31464 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=432.60 +/- 49.69\n",
      "Episode length: 432.60 +/- 49.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 433         |\n",
      "|    mean_reward          | 433         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005961555 |\n",
      "|    clip_fraction        | 0.0988      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.0197      |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | 0.000254    |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32000, episode_reward=436.80 +/- 41.09\n",
      "Episode length: 436.80 +/- 41.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 437      |\n",
      "|    mean_reward     | 437      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=32500, episode_reward=441.20 +/- 50.09\n",
      "Episode length: 441.20 +/- 50.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 441      |\n",
      "|    mean_reward     | 441      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=33000, episode_reward=452.40 +/- 52.81\n",
      "Episode length: 452.40 +/- 52.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 452      |\n",
      "|    mean_reward     | 452      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 357   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 92    |\n",
      "|    total_timesteps | 33212 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=322.00 +/- 18.38\n",
      "Episode length: 322.00 +/- 18.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 322         |\n",
      "|    mean_reward          | 322         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011428011 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.547      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | -0.0264     |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | 0.00524     |\n",
      "|    value_loss           | 0.0812      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=313.40 +/- 15.41\n",
      "Episode length: 313.40 +/- 15.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 313      |\n",
      "|    mean_reward     | 313      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=334.20 +/- 23.09\n",
      "Episode length: 334.20 +/- 23.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 334      |\n",
      "|    mean_reward     | 334      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 334   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 104   |\n",
      "|    total_timesteps | 34960 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=472.40 +/- 34.09\n",
      "Episode length: 472.40 +/- 34.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 472         |\n",
      "|    mean_reward          | 472         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008655368 |\n",
      "|    clip_fraction        | 0.0996      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | -0.0065     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00357     |\n",
      "|    value_loss           | 0.0644      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35500, episode_reward=490.60 +/- 12.99\n",
      "Episode length: 490.60 +/- 12.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 491      |\n",
      "|    mean_reward     | 491      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=444.00 +/- 69.86\n",
      "Episode length: 444.00 +/- 69.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 444      |\n",
      "|    mean_reward     | 444      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=467.80 +/- 52.19\n",
      "Episode length: 467.80 +/- 52.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 468      |\n",
      "|    mean_reward     | 468      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 331   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 110   |\n",
      "|    total_timesteps | 36708 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007172947 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.0149      |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | 0.00588     |\n",
      "|    value_loss           | 0.104       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 327   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 117   |\n",
      "|    total_timesteps | 38456 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008395582 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | 0.398       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 66          |\n",
      "|    policy_gradient_loss | -0.00424    |\n",
      "|    value_loss           | 5.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=493.40 +/- 13.20\n",
      "Episode length: 493.40 +/- 13.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 493      |\n",
      "|    mean_reward     | 493      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 305   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 131   |\n",
      "|    total_timesteps | 40204 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008088383 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 1.52        |\n",
      "|    n_updates            | 69          |\n",
      "|    policy_gradient_loss | -0.00617    |\n",
      "|    value_loss           | 1.45        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 306   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 136   |\n",
      "|    total_timesteps | 41952 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059240684 |\n",
      "|    clip_fraction        | 0.0952       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.528       |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.81         |\n",
      "|    n_updates            | 72           |\n",
      "|    policy_gradient_loss | 0.0034       |\n",
      "|    value_loss           | 0.358        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=477.20 +/- 27.59\n",
      "Episode length: 477.20 +/- 27.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 477      |\n",
      "|    mean_reward     | 477      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=485.80 +/- 18.85\n",
      "Episode length: 485.80 +/- 18.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 486      |\n",
      "|    mean_reward     | 486      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=484.20 +/- 31.60\n",
      "Episode length: 484.20 +/- 31.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 484      |\n",
      "|    mean_reward     | 484      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 304   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 143   |\n",
      "|    total_timesteps | 43700 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=492.20 +/- 15.60\n",
      "Episode length: 492.20 +/- 15.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 492          |\n",
      "|    mean_reward          | 492          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055283867 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.00199      |\n",
      "|    n_updates            | 75           |\n",
      "|    policy_gradient_loss | -0.00324     |\n",
      "|    value_loss           | 0.0917       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=485.20 +/- 16.46\n",
      "Episode length: 485.20 +/- 16.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 485      |\n",
      "|    mean_reward     | 485      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 303   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 149   |\n",
      "|    total_timesteps | 45448 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=419.60 +/- 9.52\n",
      "Episode length: 419.60 +/- 9.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 420         |\n",
      "|    mean_reward          | 420         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007870081 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.386       |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.00444     |\n",
      "|    value_loss           | 0.18        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=449.80 +/- 34.13\n",
      "Episode length: 449.80 +/- 34.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 450      |\n",
      "|    mean_reward     | 450      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=381.60 +/- 40.23\n",
      "Episode length: 381.60 +/- 40.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 382      |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=428.60 +/- 41.38\n",
      "Episode length: 428.60 +/- 41.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 429      |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 300   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 156   |\n",
      "|    total_timesteps | 47196 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 47500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056566596 |\n",
      "|    clip_fraction        | 0.0874       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.503       |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.0303       |\n",
      "|    n_updates            | 81           |\n",
      "|    policy_gradient_loss | 0.00219      |\n",
      "|    value_loss           | 0.0812       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 298   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 163   |\n",
      "|    total_timesteps | 48944 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 49000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040117432 |\n",
      "|    clip_fraction        | 0.08         |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.00618      |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -0.00104     |\n",
      "|    value_loss           | 0.06         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 293   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 172   |\n",
      "|    total_timesteps | 50692 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017368032 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.479      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 87          |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 0.00936     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 296   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 177   |\n",
      "|    total_timesteps | 52440 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038561183 |\n",
      "|    clip_fraction        | 0.0537       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | -26          |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.0281       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.000155     |\n",
      "|    value_loss           | 0.000393     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 296   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 182   |\n",
      "|    total_timesteps | 54188 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005337016 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 93          |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 0.000216    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 299   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 187   |\n",
      "|    total_timesteps | 55936 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004968983 |\n",
      "|    clip_fraction        | 0.0556      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | 0.00139     |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 299   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 192   |\n",
      "|    total_timesteps | 57684 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016300722 |\n",
      "|    clip_fraction        | 0.0588       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.417       |\n",
      "|    explained_variance   | 0.709        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | -0.00869     |\n",
      "|    n_updates            | 99           |\n",
      "|    policy_gradient_loss | 0.00189      |\n",
      "|    value_loss           | 0.00822      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 302   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 196   |\n",
      "|    total_timesteps | 59432 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 59500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009629731 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | -22.3       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.0917      |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | -9.09e-05   |\n",
      "|    value_loss           | 0.000137    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 303   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 201   |\n",
      "|    total_timesteps | 61180 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=483.20 +/- 33.60\n",
      "Episode length: 483.20 +/- 33.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | 483          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 61500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053394386 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | 0.646        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | -0.00132     |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | 0.0065       |\n",
      "|    value_loss           | 0.00118      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=480.00 +/- 24.70\n",
      "Episode length: 480.00 +/- 24.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 480      |\n",
      "|    mean_reward     | 480      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 306   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 205   |\n",
      "|    total_timesteps | 62928 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=460.20 +/- 39.30\n",
      "Episode length: 460.20 +/- 39.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 460          |\n",
      "|    mean_reward          | 460          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 63000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039955797 |\n",
      "|    clip_fraction        | 0.0929       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.504        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.00596      |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | 0.0031       |\n",
      "|    value_loss           | 0.688        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=425.40 +/- 45.37\n",
      "Episode length: 425.40 +/- 45.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 425      |\n",
      "|    mean_reward     | 425      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=468.80 +/- 39.87\n",
      "Episode length: 468.80 +/- 39.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 469      |\n",
      "|    mean_reward     | 469      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=454.20 +/- 38.17\n",
      "Episode length: 454.20 +/- 38.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 454      |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 307   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 210   |\n",
      "|    total_timesteps | 64676 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010785304 |\n",
      "|    clip_fraction        | 0.0715      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.677       |\n",
      "|    n_updates            | 111         |\n",
      "|    policy_gradient_loss | -0.000996   |\n",
      "|    value_loss           | 1.11        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 308   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 215   |\n",
      "|    total_timesteps | 66424 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007590907 |\n",
      "|    clip_fraction        | 0.072       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.0503      |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.00566     |\n",
      "|    n_updates            | 114         |\n",
      "|    policy_gradient_loss | 0.00359     |\n",
      "|    value_loss           | 0.00631     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 305   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 223   |\n",
      "|    total_timesteps | 68172 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021044633 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.441       |\n",
      "|    explained_variance   | -107         |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.0106       |\n",
      "|    n_updates            | 117          |\n",
      "|    policy_gradient_loss | 0.000868     |\n",
      "|    value_loss           | 9.99e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 307   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 227   |\n",
      "|    total_timesteps | 69920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036893582 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.425       |\n",
      "|    explained_variance   | -53.3        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | -0.000562    |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.0006       |\n",
      "|    value_loss           | 2.26e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 307   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 233   |\n",
      "|    total_timesteps | 71668 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003460136 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | -155        |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.000995    |\n",
      "|    n_updates            | 123         |\n",
      "|    policy_gradient_loss | 0.000756    |\n",
      "|    value_loss           | 3.24e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 308   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 237   |\n",
      "|    total_timesteps | 73416 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 73500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047863955 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | -75.9        |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.00795      |\n",
      "|    n_updates            | 126          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 1.03e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 309   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 243   |\n",
      "|    total_timesteps | 75164 |\n",
      "------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 75500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042847125 |\n",
      "|    clip_fraction        | 0.0508       |\n",
      "|    clip_range           | 0.185        |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | -127         |\n",
      "|    learning_rate        | 0.00444      |\n",
      "|    loss                 | 0.00518      |\n",
      "|    n_updates            | 129          |\n",
      "|    policy_gradient_loss | 0.00176      |\n",
      "|    value_loss           | 1.69e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 310   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 247   |\n",
      "|    total_timesteps | 76912 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 77000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003162251 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | -7.04       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | 0.000978    |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | 0.000495    |\n",
      "|    value_loss           | 4.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 311   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 252   |\n",
      "|    total_timesteps | 78660 |\n",
      "------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 79000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006076331 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | -57.6       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | -0.000807   |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | -0.000882   |\n",
      "|    value_loss           | 2.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 313   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 256   |\n",
      "|    total_timesteps | 80408 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012506337 |\n",
      "|    clip_fraction        | 0.0962      |\n",
      "|    clip_range           | 0.185       |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | -29.9       |\n",
      "|    learning_rate        | 0.00444     |\n",
      "|    loss                 | -0.000173   |\n",
      "|    n_updates            | 138         |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    value_loss           | 1.6e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 313   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 262   |\n",
      "|    total_timesteps | 82156 |\n",
      "------------------------------\n",
      "Single environment training took 262.42 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vocal-sweep-12</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jkwh301z' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jkwh301z</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_133922-jkwh301z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ll06qkgr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.29314870202467325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.009570412220468533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.958532008728457\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9248538784674732\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007857759340494057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 8.553789880818755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 16904\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_134357-ll06qkgr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ll06qkgr' target=\"_blank\">atomic-sweep-13</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ll06qkgr' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ll06qkgr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1282`, after every 20 untruncated mini-batches, there will be a truncated mini-batch of size 2\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1282 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=8.80 +/- 0.75\n",
      "Episode length: 8.80 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 8.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=9.40 +/- 0.49\n",
      "Episode length: 9.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 638  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 1282 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=154.80 +/- 117.34\n",
      "Episode length: 154.80 +/- 117.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 155         |\n",
      "|    mean_reward          | 155         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033191353 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | -0.00794    |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | 0.0581      |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    value_loss           | 6.69        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=99.20 +/- 18.91\n",
      "Episode length: 99.20 +/- 18.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 99.2     |\n",
      "|    mean_reward     | 99.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=148.00 +/- 113.45\n",
      "Episode length: 148.00 +/- 113.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 148      |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 277  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 2564 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05715119 |\n",
      "|    clip_fraction        | 0.321      |\n",
      "|    clip_range           | 0.293      |\n",
      "|    entropy_loss         | -0.551     |\n",
      "|    explained_variance   | 0.41       |\n",
      "|    learning_rate        | 0.00786    |\n",
      "|    loss                 | 0.375      |\n",
      "|    n_updates            | 8          |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    value_loss           | 3.32       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 182  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 3846 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=465.40 +/- 51.22\n",
      "Episode length: 465.40 +/- 51.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 465         |\n",
      "|    mean_reward          | 465         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032203026 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.504      |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 1.77        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=427.80 +/- 75.51\n",
      "Episode length: 427.80 +/- 75.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 428      |\n",
      "|    mean_reward     | 428      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=447.00 +/- 67.60\n",
      "Episode length: 447.00 +/- 67.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 447      |\n",
      "|    mean_reward     | 447      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 147  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 34   |\n",
      "|    total_timesteps | 5128 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049897972 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 0.00421     |\n",
      "|    value_loss           | 0.813       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 161  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 39   |\n",
      "|    total_timesteps | 6410 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=200.20 +/- 10.91\n",
      "Episode length: 200.20 +/- 10.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023771804 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | -0.129      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00453     |\n",
      "|    value_loss           | 0.766       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=327.00 +/- 142.52\n",
      "Episode length: 327.00 +/- 142.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 327      |\n",
      "|    mean_reward     | 327      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=258.80 +/- 121.87\n",
      "Episode length: 258.80 +/- 121.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 259      |\n",
      "|    mean_reward     | 259      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 180  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 42   |\n",
      "|    total_timesteps | 7692 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=251.20 +/- 125.50\n",
      "Episode length: 251.20 +/- 125.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | 251         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067503095 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | 0.0239      |\n",
      "|    value_loss           | 0.497       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=266.80 +/- 117.05\n",
      "Episode length: 266.80 +/- 117.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 267      |\n",
      "|    mean_reward     | 267      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 200  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 44   |\n",
      "|    total_timesteps | 8974 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=373.80 +/- 154.56\n",
      "Episode length: 373.80 +/- 154.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 374         |\n",
      "|    mean_reward          | 374         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027416853 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | 0.00132     |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | 0.0403      |\n",
      "|    value_loss           | 0.0326      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=391.60 +/- 132.76\n",
      "Episode length: 391.60 +/- 132.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 392      |\n",
      "|    mean_reward     | 392      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=320.20 +/- 147.49\n",
      "Episode length: 320.20 +/- 147.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 320      |\n",
      "|    mean_reward     | 320      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 205   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 10256 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14930424 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.293      |\n",
      "|    entropy_loss         | -0.361     |\n",
      "|    explained_variance   | 0.868      |\n",
      "|    learning_rate        | 0.00786    |\n",
      "|    loss                 | -0.134     |\n",
      "|    n_updates            | 32         |\n",
      "|    policy_gradient_loss | 0.0201     |\n",
      "|    value_loss           | 0.371      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 203   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 11538 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009111653 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.245      |\n",
      "|    explained_variance   | -0.652      |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | -0.0703     |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.0011      |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 212   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 12820 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=369.40 +/- 133.59\n",
      "Episode length: 369.40 +/- 133.59\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 369        |\n",
      "|    mean_reward          | 369        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06369306 |\n",
      "|    clip_fraction        | 0.0954     |\n",
      "|    clip_range           | 0.293      |\n",
      "|    entropy_loss         | -0.221     |\n",
      "|    explained_variance   | 0.781      |\n",
      "|    learning_rate        | 0.00786    |\n",
      "|    loss                 | -0.0179    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.00165    |\n",
      "|    value_loss           | 0.339      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=408.20 +/- 112.49\n",
      "Episode length: 408.20 +/- 112.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 408      |\n",
      "|    mean_reward     | 408      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=386.60 +/- 94.72\n",
      "Episode length: 386.60 +/- 94.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 387      |\n",
      "|    mean_reward     | 387      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 219   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 14102 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=499.40 +/- 1.20\n",
      "Episode length: 499.40 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 499         |\n",
      "|    mean_reward          | 499         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024650028 |\n",
      "|    clip_fraction        | 0.0673      |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | -0.00923    |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | 0.00789     |\n",
      "|    value_loss           | 0.793       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=450.60 +/- 84.16\n",
      "Episode length: 450.60 +/- 84.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 451      |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 228   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 15384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052276157 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | 0.268       |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    value_loss           | 0.0725      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=422.40 +/- 101.65\n",
      "Episode length: 422.40 +/- 101.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 422      |\n",
      "|    mean_reward     | 422      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=476.40 +/- 30.62\n",
      "Episode length: 476.40 +/- 30.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 476      |\n",
      "|    mean_reward     | 476      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 233   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 16666 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=325.00 +/- 126.04\n",
      "Episode length: 325.00 +/- 126.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 325         |\n",
      "|    mean_reward          | 325         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034511823 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.293       |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | -0.139      |\n",
      "|    learning_rate        | 0.00786     |\n",
      "|    loss                 | 0.0567      |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | 0.00601     |\n",
      "|    value_loss           | 0.464       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=331.80 +/- 112.41\n",
      "Episode length: 331.80 +/- 112.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 332      |\n",
      "|    mean_reward     | 332      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 242   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 17948 |\n",
      "------------------------------\n",
      "Single environment training took 74.20 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-13</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ll06qkgr' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ll06qkgr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_134357-ll06qkgr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: boi8yjct with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.1508130227534313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.009426977350216113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9757205918062372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9058556536144868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008185328776173572\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 3.0563725417889787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 37824\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_134530-boi8yjct</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/boi8yjct' target=\"_blank\">cool-sweep-14</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/boi8yjct' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/boi8yjct</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 653`, after every 10 untruncated mini-batches, there will be a truncated mini-batch of size 13\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=653 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=18.80 +/- 7.52\n",
      "Episode length: 18.80 +/- 7.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 18.8     |\n",
      "|    mean_reward     | 18.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 507 |\n",
      "|    iterations      | 1   |\n",
      "|    time_elapsed    | 1   |\n",
      "|    total_timesteps | 653 |\n",
      "----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=317.20 +/- 100.74\n",
      "Episode length: 317.20 +/- 100.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 317          |\n",
      "|    mean_reward          | 317          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039205356 |\n",
      "|    clip_fraction        | 0.0923       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.689       |\n",
      "|    explained_variance   | -0.00968     |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 6.86         |\n",
      "|    n_updates            | 1            |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 29.7         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 310  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 1306 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=296.00 +/- 28.35\n",
      "Episode length: 296.00 +/- 28.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 296         |\n",
      "|    mean_reward          | 296         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008819249 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | -0.0835     |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 7.12        |\n",
      "|    n_updates            | 2           |\n",
      "|    policy_gradient_loss | 0.00109     |\n",
      "|    value_loss           | 8.67        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 296  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 1959 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=232.00 +/- 24.40\n",
      "Episode length: 232.00 +/- 24.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 232          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048764176 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.681       |\n",
      "|    explained_variance   | -0.109       |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 1.29         |\n",
      "|    n_updates            | 3            |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    value_loss           | 5.68         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=211.40 +/- 15.32\n",
      "Episode length: 211.40 +/- 15.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 211      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 296  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 2612 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004539777 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 2.59        |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.000356   |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 304  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 3265 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=239.80 +/- 36.96\n",
      "Episode length: 239.80 +/- 36.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 240         |\n",
      "|    mean_reward          | 240         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004749851 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 2.68        |\n",
      "|    n_updates            | 5           |\n",
      "|    policy_gradient_loss | -0.00896    |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 336  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 3918 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012945549 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.478       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    value_loss           | 2.65        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=494.00 +/- 12.00\n",
      "Episode length: 494.00 +/- 12.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 320  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 4571 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=295.40 +/- 167.64\n",
      "Episode length: 295.40 +/- 167.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 295         |\n",
      "|    mean_reward          | 295         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027714115 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 7           |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 2.13        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 342  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 5224 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=144.60 +/- 44.77\n",
      "Episode length: 144.60 +/- 44.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 145          |\n",
      "|    mean_reward          | 145          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038967552 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.567       |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.555        |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | 0.00572      |\n",
      "|    value_loss           | 2            |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 367  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 5877 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=130.60 +/- 6.80\n",
      "Episode length: 130.60 +/- 6.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 131          |\n",
      "|    mean_reward          | 131          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080059795 |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.547       |\n",
      "|    explained_variance   | 0.394        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.278        |\n",
      "|    n_updates            | 9            |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 1.5          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=140.80 +/- 16.30\n",
      "Episode length: 140.80 +/- 16.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 141      |\n",
      "|    mean_reward     | 141      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 384  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 6530 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=129.00 +/- 15.76\n",
      "Episode length: 129.00 +/- 15.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 129          |\n",
      "|    mean_reward          | 129          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029772723 |\n",
      "|    clip_fraction        | 0.0682       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | 0.237        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 1.09         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.00493      |\n",
      "|    value_loss           | 1.76         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 407  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 7183 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=147.80 +/- 12.58\n",
      "Episode length: 147.80 +/- 12.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 148         |\n",
      "|    mean_reward          | 148         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024374263 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.366       |\n",
      "|    n_updates            | 11          |\n",
      "|    policy_gradient_loss | 0.0232      |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 427  |\n",
      "|    iterations      | 12   |\n",
      "|    time_elapsed    | 18   |\n",
      "|    total_timesteps | 7836 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=179.40 +/- 18.36\n",
      "Episode length: 179.40 +/- 18.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 179         |\n",
      "|    mean_reward          | 179         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010201918 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | 0.00639     |\n",
      "|    value_loss           | 0.459       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 442  |\n",
      "|    iterations      | 13   |\n",
      "|    time_elapsed    | 19   |\n",
      "|    total_timesteps | 8489 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=144.20 +/- 12.38\n",
      "Episode length: 144.20 +/- 12.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 144          |\n",
      "|    mean_reward          | 144          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048069437 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.288        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.172        |\n",
      "|    n_updates            | 13           |\n",
      "|    policy_gradient_loss | 0.00606      |\n",
      "|    value_loss           | 1.63         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=152.00 +/- 18.74\n",
      "Episode length: 152.00 +/- 18.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 152      |\n",
      "|    mean_reward     | 152      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 448  |\n",
      "|    iterations      | 14   |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 9142 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=163.00 +/- 18.02\n",
      "Episode length: 163.00 +/- 18.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 163         |\n",
      "|    mean_reward          | 163         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009887381 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.848       |\n",
      "|    n_updates            | 14          |\n",
      "|    policy_gradient_loss | 0.0144      |\n",
      "|    value_loss           | 0.669       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 459  |\n",
      "|    iterations      | 15   |\n",
      "|    time_elapsed    | 21   |\n",
      "|    total_timesteps | 9795 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=220.60 +/- 26.42\n",
      "Episode length: 220.60 +/- 26.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 221         |\n",
      "|    mean_reward          | 221         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012214098 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.191       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | 0.00387     |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 471   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 10448 |\n",
      "------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=258.00 +/- 53.71\n",
      "Episode length: 258.00 +/- 53.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 258         |\n",
      "|    mean_reward          | 258         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010476932 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.53        |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 0.0204      |\n",
      "|    value_loss           | 0.576       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=241.80 +/- 37.98\n",
      "Episode length: 241.80 +/- 37.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 242      |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 472   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 11101 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=349.60 +/- 77.47\n",
      "Episode length: 349.60 +/- 77.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 350         |\n",
      "|    mean_reward          | 350         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005880423 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 1.05        |\n",
      "|    n_updates            | 17          |\n",
      "|    policy_gradient_loss | 0.002       |\n",
      "|    value_loss           | 0.72        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 468   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 11754 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005653318 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | -0.0342     |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 0.29        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 470   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 12407 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=446.00 +/- 66.14\n",
      "Episode length: 446.00 +/- 66.14\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 446          |\n",
      "|    mean_reward          | 446          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037361518 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.527       |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.106        |\n",
      "|    n_updates            | 19           |\n",
      "|    policy_gradient_loss | 0.00383      |\n",
      "|    value_loss           | 0.358        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=299.00 +/- 61.51\n",
      "Episode length: 299.00 +/- 61.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 299      |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 428   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 13060 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=453.80 +/- 63.80\n",
      "Episode length: 453.80 +/- 63.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 454         |\n",
      "|    mean_reward          | 454         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005943945 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | -0.00609    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00421     |\n",
      "|    value_loss           | 0.134       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 427   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 13713 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=443.80 +/- 90.33\n",
      "Episode length: 443.80 +/- 90.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 444          |\n",
      "|    mean_reward          | 444          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060682553 |\n",
      "|    clip_fraction        | 0.18         |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.53        |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.134        |\n",
      "|    n_updates            | 21           |\n",
      "|    policy_gradient_loss | 0.00819      |\n",
      "|    value_loss           | 0.503        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 14366 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=183.00 +/- 25.54\n",
      "Episode length: 183.00 +/- 25.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 183          |\n",
      "|    mean_reward          | 183          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0126898615 |\n",
      "|    clip_fraction        | 0.266        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.541       |\n",
      "|    explained_variance   | 0.708        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.325        |\n",
      "|    n_updates            | 22           |\n",
      "|    policy_gradient_loss | 0.00797      |\n",
      "|    value_loss           | 1.04         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=157.00 +/- 14.14\n",
      "Episode length: 157.00 +/- 14.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 157      |\n",
      "|    mean_reward     | 157      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 435   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 15019 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=182.80 +/- 19.46\n",
      "Episode length: 182.80 +/- 19.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 183         |\n",
      "|    mean_reward          | 183         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009591585 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.914       |\n",
      "|    n_updates            | 23          |\n",
      "|    policy_gradient_loss | 0.000839    |\n",
      "|    value_loss           | 0.493       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 444   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 15672 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=187.80 +/- 22.29\n",
      "Episode length: 187.80 +/- 22.29\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 188          |\n",
      "|    mean_reward          | 188          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030893544 |\n",
      "|    clip_fraction        | 0.0708       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.538       |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.000274    |\n",
      "|    value_loss           | 0.249        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 449   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 16325 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=251.20 +/- 50.88\n",
      "Episode length: 251.20 +/- 50.88\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | 251          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053838706 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.519       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.0674       |\n",
      "|    n_updates            | 25           |\n",
      "|    policy_gradient_loss | 0.00767      |\n",
      "|    value_loss           | 0.238        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 436   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 16978 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=441.00 +/- 76.05\n",
      "Episode length: 441.00 +/- 76.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 441          |\n",
      "|    mean_reward          | 441          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065187756 |\n",
      "|    clip_fraction        | 0.178        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.00019      |\n",
      "|    n_updates            | 26           |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 0.211        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=432.00 +/- 38.35\n",
      "Episode length: 432.00 +/- 38.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 432      |\n",
      "|    mean_reward     | 432      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 17631 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=461.20 +/- 47.67\n",
      "Episode length: 461.20 +/- 47.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 461          |\n",
      "|    mean_reward          | 461          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036178045 |\n",
      "|    clip_fraction        | 0.0893       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.489       |\n",
      "|    explained_variance   | 0.941        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.0344       |\n",
      "|    n_updates            | 27           |\n",
      "|    policy_gradient_loss | 0.00901      |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 18284 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=446.40 +/- 65.89\n",
      "Episode length: 446.40 +/- 65.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 446          |\n",
      "|    mean_reward          | 446          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032043236 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.533       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.0213       |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | 0.00298      |\n",
      "|    value_loss           | 0.086        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 411   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 18937 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=329.00 +/- 131.90\n",
      "Episode length: 329.00 +/- 131.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 329         |\n",
      "|    mean_reward          | 329         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004490503 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.137       |\n",
      "|    n_updates            | 29          |\n",
      "|    policy_gradient_loss | 0.00405     |\n",
      "|    value_loss           | 0.304       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=342.60 +/- 130.59\n",
      "Episode length: 342.60 +/- 130.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 407   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 19590 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=175.40 +/- 26.33\n",
      "Episode length: 175.40 +/- 26.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 175         |\n",
      "|    mean_reward          | 175         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005329042 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.00984     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.000823    |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 409   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 20243 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=198.00 +/- 13.99\n",
      "Episode length: 198.00 +/- 13.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | 198         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010928424 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 31          |\n",
      "|    policy_gradient_loss | 0.0165      |\n",
      "|    value_loss           | 0.0549      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 408   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 20896 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=185.60 +/- 17.65\n",
      "Episode length: 185.60 +/- 17.65\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 186        |\n",
      "|    mean_reward          | 186        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 21000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00643371 |\n",
      "|    clip_fraction        | 0.0952     |\n",
      "|    clip_range           | 0.151      |\n",
      "|    entropy_loss         | -0.461     |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.00819    |\n",
      "|    loss                 | 0.0424     |\n",
      "|    n_updates            | 32         |\n",
      "|    policy_gradient_loss | 0.00595    |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=181.80 +/- 8.16\n",
      "Episode length: 181.80 +/- 8.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 182      |\n",
      "|    mean_reward     | 182      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 411   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 21549 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=171.20 +/- 28.42\n",
      "Episode length: 171.20 +/- 28.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 171        |\n",
      "|    mean_reward          | 171        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00806352 |\n",
      "|    clip_fraction        | 0.068      |\n",
      "|    clip_range           | 0.151      |\n",
      "|    entropy_loss         | -0.528     |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.00819    |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 33         |\n",
      "|    policy_gradient_loss | -0.00305   |\n",
      "|    value_loss           | 0.0846     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 417   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 53    |\n",
      "|    total_timesteps | 22202 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=156.60 +/- 16.12\n",
      "Episode length: 156.60 +/- 16.12\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 157          |\n",
      "|    mean_reward          | 157          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035941906 |\n",
      "|    clip_fraction        | 0.0908       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.551       |\n",
      "|    explained_variance   | 0.375        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.0235       |\n",
      "|    n_updates            | 34           |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    value_loss           | 1.36         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 423   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 53    |\n",
      "|    total_timesteps | 22855 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=168.80 +/- 13.56\n",
      "Episode length: 168.80 +/- 13.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 169          |\n",
      "|    mean_reward          | 169          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026254002 |\n",
      "|    clip_fraction        | 0.0708       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.534       |\n",
      "|    explained_variance   | 0.913        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.245        |\n",
      "|    n_updates            | 35           |\n",
      "|    policy_gradient_loss | -0.00641     |\n",
      "|    value_loss           | 0.348        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=174.20 +/- 27.29\n",
      "Episode length: 174.20 +/- 27.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 174      |\n",
      "|    mean_reward     | 174      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 424   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 23508 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=163.40 +/- 4.76\n",
      "Episode length: 163.40 +/- 4.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 163          |\n",
      "|    mean_reward          | 163          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052657514 |\n",
      "|    clip_fraction        | 0.125        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.503       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | 0.00547      |\n",
      "|    value_loss           | 0.271        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 24161 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=123.20 +/- 5.64\n",
      "Episode length: 123.20 +/- 5.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 123         |\n",
      "|    mean_reward          | 123         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029830325 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 37          |\n",
      "|    policy_gradient_loss | 0.0213      |\n",
      "|    value_loss           | 0.0767      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 437   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 24814 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=136.80 +/- 16.68\n",
      "Episode length: 136.80 +/- 16.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 137         |\n",
      "|    mean_reward          | 137         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017655758 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.0698      |\n",
      "|    n_updates            | 38          |\n",
      "|    policy_gradient_loss | 0.00341     |\n",
      "|    value_loss           | 0.45        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 444   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 25467 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=135.20 +/- 5.00\n",
      "Episode length: 135.20 +/- 5.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 135          |\n",
      "|    mean_reward          | 135          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012283359 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.481       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.0168       |\n",
      "|    n_updates            | 39           |\n",
      "|    policy_gradient_loss | 0.000791     |\n",
      "|    value_loss           | 0.125        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=133.20 +/- 12.17\n",
      "Episode length: 133.20 +/- 12.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 133      |\n",
      "|    mean_reward     | 133      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 448   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 26120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=139.40 +/- 15.36\n",
      "Episode length: 139.40 +/- 15.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 139         |\n",
      "|    mean_reward          | 139         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010384532 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.0702      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.00469     |\n",
      "|    value_loss           | 0.425       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 453   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 26773 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=132.20 +/- 13.66\n",
      "Episode length: 132.20 +/- 13.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 132          |\n",
      "|    mean_reward          | 132          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 27000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075151506 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.41        |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.134        |\n",
      "|    n_updates            | 41           |\n",
      "|    policy_gradient_loss | 0.00128      |\n",
      "|    value_loss           | 0.182        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 459   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 27426 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=144.40 +/- 20.47\n",
      "Episode length: 144.40 +/- 20.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 144          |\n",
      "|    mean_reward          | 144          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 27500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0080616325 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.072        |\n",
      "|    n_updates            | 42           |\n",
      "|    policy_gradient_loss | 0.0138       |\n",
      "|    value_loss           | 0.0388       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=145.60 +/- 13.44\n",
      "Episode length: 145.60 +/- 13.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 146      |\n",
      "|    mean_reward     | 146      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 461   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 28079 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=140.60 +/- 10.19\n",
      "Episode length: 140.60 +/- 10.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 141         |\n",
      "|    mean_reward          | 141         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949062 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.372      |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.00375     |\n",
      "|    n_updates            | 43          |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    value_loss           | 0.078       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 28732 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=161.40 +/- 30.10\n",
      "Episode length: 161.40 +/- 30.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 161         |\n",
      "|    mean_reward          | 161         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006598422 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.373      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | -0.004      |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | -0.00556    |\n",
      "|    value_loss           | 0.096       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 467   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 29385 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=148.80 +/- 9.77\n",
      "Episode length: 148.80 +/- 9.77\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 149          |\n",
      "|    mean_reward          | 149          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065034865 |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.012        |\n",
      "|    n_updates            | 45           |\n",
      "|    policy_gradient_loss | 0.0145       |\n",
      "|    value_loss           | 0.0226       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=157.00 +/- 12.51\n",
      "Episode length: 157.00 +/- 12.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 157      |\n",
      "|    mean_reward     | 157      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 469   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 30038 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=218.20 +/- 88.02\n",
      "Episode length: 218.20 +/- 88.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 218         |\n",
      "|    mean_reward          | 218         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004177254 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.397      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 46          |\n",
      "|    policy_gradient_loss | 0.000166    |\n",
      "|    value_loss           | 0.0517      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 30691 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=203.20 +/- 112.66\n",
      "Episode length: 203.20 +/- 112.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 203          |\n",
      "|    mean_reward          | 203          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056200875 |\n",
      "|    clip_fraction        | 0.0863       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.458       |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | -0.00228     |\n",
      "|    n_updates            | 47           |\n",
      "|    policy_gradient_loss | -0.000191    |\n",
      "|    value_loss           | 0.0277       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 31344 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=166.40 +/- 14.76\n",
      "Episode length: 166.40 +/- 14.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 166          |\n",
      "|    mean_reward          | 166          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011300487 |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.47        |\n",
      "|    explained_variance   | 0.916        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.18         |\n",
      "|    n_updates            | 48           |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 0.203        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 49    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 31997 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=179.20 +/- 22.06\n",
      "Episode length: 179.20 +/- 22.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 179         |\n",
      "|    mean_reward          | 179         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015186885 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 49          |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    value_loss           | 0.952       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=163.60 +/- 9.35\n",
      "Episode length: 163.60 +/- 9.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 164      |\n",
      "|    mean_reward     | 164      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 32650 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=275.40 +/- 99.20\n",
      "Episode length: 275.40 +/- 99.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 275         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016890364 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.0379      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00366     |\n",
      "|    value_loss           | 0.117       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 472   |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 33303 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=190.60 +/- 14.42\n",
      "Episode length: 190.60 +/- 14.42\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 191           |\n",
      "|    mean_reward          | 191           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 33500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086639076 |\n",
      "|    clip_fraction        | 0.0225        |\n",
      "|    clip_range           | 0.151         |\n",
      "|    entropy_loss         | -0.505        |\n",
      "|    explained_variance   | 0.803         |\n",
      "|    learning_rate        | 0.00819       |\n",
      "|    loss                 | 0.0179        |\n",
      "|    n_updates            | 51            |\n",
      "|    policy_gradient_loss | 0.0031        |\n",
      "|    value_loss           | 0.126         |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 33956 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=445.40 +/- 109.20\n",
      "Episode length: 445.40 +/- 109.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 445         |\n",
      "|    mean_reward          | 445         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017232971 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | -0.00779    |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | -0.000938   |\n",
      "|    value_loss           | 0.595       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=399.40 +/- 96.13\n",
      "Episode length: 399.40 +/- 96.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 399      |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 465   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 74    |\n",
      "|    total_timesteps | 34609 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=412.20 +/- 117.22\n",
      "Episode length: 412.20 +/- 117.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 412          |\n",
      "|    mean_reward          | 412          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027732716 |\n",
      "|    clip_fraction        | 0.0696       |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.435       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | -0.0098      |\n",
      "|    n_updates            | 53           |\n",
      "|    policy_gradient_loss | -0.000661    |\n",
      "|    value_loss           | 0.0432       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 35262 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=421.80 +/- 64.06\n",
      "Episode length: 421.80 +/- 64.06\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 422        |\n",
      "|    mean_reward          | 422        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03388159 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.151      |\n",
      "|    entropy_loss         | -0.365     |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.00819    |\n",
      "|    loss                 | -0.0554    |\n",
      "|    n_updates            | 54         |\n",
      "|    policy_gradient_loss | 0.00289    |\n",
      "|    value_loss           | 0.0662     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 35915 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00309499 |\n",
      "|    clip_fraction        | 0.0748     |\n",
      "|    clip_range           | 0.151      |\n",
      "|    entropy_loss         | -0.374     |\n",
      "|    explained_variance   | 0.806      |\n",
      "|    learning_rate        | 0.00819    |\n",
      "|    loss                 | 0.0338     |\n",
      "|    n_updates            | 55         |\n",
      "|    policy_gradient_loss | 0.00435    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=463.00 +/- 45.50\n",
      "Episode length: 463.00 +/- 45.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 463      |\n",
      "|    mean_reward     | 463      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 460   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 36568 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=460.40 +/- 79.20\n",
      "Episode length: 460.40 +/- 79.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 460          |\n",
      "|    mean_reward          | 460          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068526193 |\n",
      "|    clip_fraction        | 0.133        |\n",
      "|    clip_range           | 0.151        |\n",
      "|    entropy_loss         | -0.349       |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.00819      |\n",
      "|    loss                 | 0.0485       |\n",
      "|    n_updates            | 56           |\n",
      "|    policy_gradient_loss | 0.00825      |\n",
      "|    value_loss           | 0.161        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 460   |\n",
      "|    iterations      | 57    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 37221 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=406.80 +/- 101.29\n",
      "Episode length: 406.80 +/- 101.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 407         |\n",
      "|    mean_reward          | 407         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014984407 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.151       |\n",
      "|    entropy_loss         | -0.359      |\n",
      "|    explained_variance   | 0.873       |\n",
      "|    learning_rate        | 0.00819     |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | 0.00111     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 460   |\n",
      "|    iterations      | 58    |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 37874 |\n",
      "------------------------------\n",
      "Single environment training took 82.29 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>450.6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-sweep-14</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/boi8yjct' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/boi8yjct</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_134530-boi8yjct/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eqxegvos with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2996068659099387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0012493095907159064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9262006170642736\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9623579638081332\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00860166487035158\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.63623071112113\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 756\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 54292\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_134707-eqxegvos</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/eqxegvos' target=\"_blank\">worldly-sweep-15</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/eqxegvos' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/eqxegvos</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 756`, after every 11 untruncated mini-batches, there will be a truncated mini-batch of size 52\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=756 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=40.60 +/- 7.61\n",
      "Episode length: 40.60 +/- 7.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 40.6     |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1153 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 756  |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=9.60 +/- 0.49\n",
      "Episode length: 9.60 +/- 0.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.6         |\n",
      "|    mean_reward          | 9.6         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021646874 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.00395     |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | 0.000283    |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=9.40 +/- 0.49\n",
      "Episode length: 9.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1206 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1512 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=118.60 +/- 6.02\n",
      "Episode length: 118.60 +/- 6.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 119         |\n",
      "|    mean_reward          | 119         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050860494 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 2.36        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0422     |\n",
      "|    value_loss           | 7.38        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1137 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2268 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=174.00 +/- 66.85\n",
      "Episode length: 174.00 +/- 66.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 174         |\n",
      "|    mean_reward          | 174         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042226393 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 3.88        |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.0627     |\n",
      "|    value_loss           | 8.79        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=241.60 +/- 135.53\n",
      "Episode length: 241.60 +/- 135.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 242      |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 894  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 3024 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3500, episode_reward=193.80 +/- 23.97\n",
      "Episode length: 193.80 +/- 23.97\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 194        |\n",
      "|    mean_reward          | 194        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07346863 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.565     |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 6.08       |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    value_loss           | 10         |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 894  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 3780 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=363.40 +/- 20.11\n",
      "Episode length: 363.40 +/- 20.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 363         |\n",
      "|    mean_reward          | 363         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035988484 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.904       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 4.24        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=373.00 +/- 27.91\n",
      "Episode length: 373.00 +/- 27.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 373      |\n",
      "|    mean_reward     | 373      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 711  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 4536 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114947855 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.721       |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    value_loss           | 2.78        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 666  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 5292 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=479.00 +/- 42.00\n",
      "Episode length: 479.00 +/- 42.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 479         |\n",
      "|    mean_reward          | 479         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045243066 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.713       |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | -0.00444    |\n",
      "|    value_loss           | 4.68        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=482.40 +/- 35.20\n",
      "Episode length: 482.40 +/- 35.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 482      |\n",
      "|    mean_reward     | 482      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 589  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 6048 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=440.80 +/- 70.27\n",
      "Episode length: 440.80 +/- 70.27\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 441         |\n",
      "|    mean_reward          | 441         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027145064 |\n",
      "|    clip_fraction        | 0.0937      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.504      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.988       |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 587  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 6804 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=395.60 +/- 56.98\n",
      "Episode length: 395.60 +/- 56.98\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 396        |\n",
      "|    mean_reward          | 396        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04249664 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.441     |\n",
      "|    explained_variance   | 0.658      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 1.41       |\n",
      "|    n_updates            | 27         |\n",
      "|    policy_gradient_loss | -0.00519   |\n",
      "|    value_loss           | 2.76       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=380.80 +/- 73.28\n",
      "Episode length: 380.80 +/- 73.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 381      |\n",
      "|    mean_reward     | 381      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 564  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 7560 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032147232 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.318       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00425    |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 561  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 8316 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=408.60 +/- 20.54\n",
      "Episode length: 408.60 +/- 20.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 409        |\n",
      "|    mean_reward          | 409        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08923317 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.391     |\n",
      "|    explained_variance   | 0.725      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.0174     |\n",
      "|    n_updates            | 33         |\n",
      "|    policy_gradient_loss | -0.00053   |\n",
      "|    value_loss           | 0.0221     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=392.20 +/- 70.16\n",
      "Episode length: 392.20 +/- 70.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 392      |\n",
      "|    mean_reward     | 392      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 542  |\n",
      "|    iterations      | 12   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 9072 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=334.40 +/- 60.79\n",
      "Episode length: 334.40 +/- 60.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 334         |\n",
      "|    mean_reward          | 334         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032319736 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.236       |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    value_loss           | 0.775       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 553  |\n",
      "|    iterations      | 13   |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 9828 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=183.60 +/- 27.70\n",
      "Episode length: 183.60 +/- 27.70\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 184        |\n",
      "|    mean_reward          | 184        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17686851 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.399     |\n",
      "|    explained_variance   | 0.914      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.528      |\n",
      "|    n_updates            | 39         |\n",
      "|    policy_gradient_loss | 0.0186     |\n",
      "|    value_loss           | 2.09       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=161.60 +/- 8.09\n",
      "Episode length: 161.60 +/- 8.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 162      |\n",
      "|    mean_reward     | 162      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 560   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 10584 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=306.80 +/- 20.71\n",
      "Episode length: 306.80 +/- 20.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 307        |\n",
      "|    mean_reward          | 307        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08736169 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.395     |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.224      |\n",
      "|    n_updates            | 42         |\n",
      "|    policy_gradient_loss | 0.0418     |\n",
      "|    value_loss           | 0.886      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 562   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 11340 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=435.20 +/- 81.46\n",
      "Episode length: 435.20 +/- 81.46\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 435         |\n",
      "|    mean_reward          | 435         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035230625 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.217      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.241       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    value_loss           | 0.465       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=466.80 +/- 39.96\n",
      "Episode length: 466.80 +/- 39.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 467      |\n",
      "|    mean_reward     | 467      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 525   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 12096 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=497.80 +/- 4.40\n",
      "Episode length: 497.80 +/- 4.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 498         |\n",
      "|    mean_reward          | 498         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077632785 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.201      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | 0.036       |\n",
      "|    value_loss           | 0.277       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 518   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 12852 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=487.80 +/- 24.40\n",
      "Episode length: 487.80 +/- 24.40\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 488      |\n",
      "|    mean_reward          | 488      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 13000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.510098 |\n",
      "|    clip_fraction        | 0.211    |\n",
      "|    clip_range           | 0.3      |\n",
      "|    entropy_loss         | -0.179   |\n",
      "|    explained_variance   | 0.995    |\n",
      "|    learning_rate        | 0.0086   |\n",
      "|    loss                 | 0.0278   |\n",
      "|    n_updates            | 51       |\n",
      "|    policy_gradient_loss | 0.028    |\n",
      "|    value_loss           | 0.0656   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=480.00 +/- 24.57\n",
      "Episode length: 480.00 +/- 24.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 480      |\n",
      "|    mean_reward     | 480      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 13608 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=429.80 +/- 61.99\n",
      "Episode length: 429.80 +/- 61.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 430         |\n",
      "|    mean_reward          | 430         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010068355 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.215      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.00097    |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | 0.00602     |\n",
      "|    value_loss           | 0.0264      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 506   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 14364 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064226985 |\n",
      "|    clip_fraction        | 0.0947      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.194      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.111       |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | 0.0124      |\n",
      "|    value_loss           | 0.00101     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 15120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020108322 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.151      |\n",
      "|    explained_variance   | 0.443       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.00402    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00307     |\n",
      "|    value_loss           | 0.000339    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 480   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 15876 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=473.40 +/- 53.20\n",
      "Episode length: 473.40 +/- 53.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 473         |\n",
      "|    mean_reward          | 473         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010940631 |\n",
      "|    clip_fraction        | 0.091       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.199      |\n",
      "|    explained_variance   | -1.9        |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.00342    |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | 0.00294     |\n",
      "|    value_loss           | 5.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 16632 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=457.00 +/- 39.96\n",
      "Episode length: 457.00 +/- 39.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 457         |\n",
      "|    mean_reward          | 457         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007785234 |\n",
      "|    clip_fraction        | 0.0407      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.183      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.000108   |\n",
      "|    n_updates            | 66          |\n",
      "|    policy_gradient_loss | 0.00344     |\n",
      "|    value_loss           | 0.000204    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 468   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 17388 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=464.80 +/- 44.75\n",
      "Episode length: 464.80 +/- 44.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 465         |\n",
      "|    mean_reward          | 465         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009076435 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.168      |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 69          |\n",
      "|    policy_gradient_loss | 0.00656     |\n",
      "|    value_loss           | 0.137       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=499.40 +/- 1.20\n",
      "Episode length: 499.40 +/- 1.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 499      |\n",
      "|    mean_reward     | 499      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 455   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 18144 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=433.60 +/- 55.52\n",
      "Episode length: 433.60 +/- 55.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 434         |\n",
      "|    mean_reward          | 434         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004270272 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.154      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0161      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.00821     |\n",
      "|    value_loss           | 0.024       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 456   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 18900 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=433.80 +/- 61.50\n",
      "Episode length: 433.80 +/- 61.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 434         |\n",
      "|    mean_reward          | 434         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005505095 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.223      |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.00959     |\n",
      "|    value_loss           | 0.0432      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=493.80 +/- 12.40\n",
      "Episode length: 493.80 +/- 12.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 445   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 19656 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=439.20 +/- 83.87\n",
      "Episode length: 439.20 +/- 83.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 439          |\n",
      "|    mean_reward          | 439          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021600893 |\n",
      "|    clip_fraction        | 0.00521      |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -0.283       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0086       |\n",
      "|    loss                 | 0.272        |\n",
      "|    n_updates            | 78           |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    value_loss           | 1.44         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 439   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 20412 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=490.80 +/- 18.40\n",
      "Episode length: 490.80 +/- 18.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 491         |\n",
      "|    mean_reward          | 491         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034173775 |\n",
      "|    clip_fraction        | 0.0695      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 81          |\n",
      "|    policy_gradient_loss | 0.00572     |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=493.00 +/- 14.00\n",
      "Episode length: 493.00 +/- 14.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 493      |\n",
      "|    mean_reward     | 493      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 419   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 21168 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=469.40 +/- 38.58\n",
      "Episode length: 469.40 +/- 38.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 469         |\n",
      "|    mean_reward          | 469         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022708498 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.233      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    value_loss           | 0.0546      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 410   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 53    |\n",
      "|    total_timesteps | 21924 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06669272 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.265     |\n",
      "|    explained_variance   | 0.929      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.204      |\n",
      "|    n_updates            | 87         |\n",
      "|    policy_gradient_loss | -0.00934   |\n",
      "|    value_loss           | 0.914      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 22680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=497.00 +/- 6.00\n",
      "Episode length: 497.00 +/- 6.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | 497         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010208911 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.00389     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 399   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 23436 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0130135445 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -0.339       |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 0.0086       |\n",
      "|    loss                 | 0.02         |\n",
      "|    n_updates            | 93           |\n",
      "|    policy_gradient_loss | 0.00371      |\n",
      "|    value_loss           | 0.00045      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 391   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 24192 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=496.60 +/- 6.80\n",
      "Episode length: 496.60 +/- 6.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 497        |\n",
      "|    mean_reward          | 497        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09141803 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.39      |\n",
      "|    explained_variance   | 0.309      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | -0.0183    |\n",
      "|    n_updates            | 96         |\n",
      "|    policy_gradient_loss | 0.00101    |\n",
      "|    value_loss           | 0.000228   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 395   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 24948 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028358167 |\n",
      "|    clip_fraction        | 0.0871      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.00288     |\n",
      "|    n_updates            | 99          |\n",
      "|    policy_gradient_loss | 0.00561     |\n",
      "|    value_loss           | 0.000237    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 389   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 25704 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08250459 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.382     |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | -0.00831   |\n",
      "|    n_updates            | 102        |\n",
      "|    policy_gradient_loss | 0.00489    |\n",
      "|    value_loss           | 9.83e-05   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 392   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 26460 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086754054 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0665      |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | 0.0088      |\n",
      "|    value_loss           | 5.94e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 385   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 27216 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19798057 |\n",
      "|    clip_fraction        | 0.36       |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.348     |\n",
      "|    explained_variance   | -0.014     |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | -0.0635    |\n",
      "|    n_updates            | 108        |\n",
      "|    policy_gradient_loss | 0.00882    |\n",
      "|    value_loss           | 1.54e-05   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 383   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 27972 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=499.00 +/- 2.00\n",
      "Episode length: 499.00 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 499        |\n",
      "|    mean_reward          | 499        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08721152 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.421     |\n",
      "|    explained_variance   | 0.704      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 6.09       |\n",
      "|    n_updates            | 111        |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    value_loss           | 5.13       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 380   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 28728 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=458.60 +/- 36.64\n",
      "Episode length: 458.60 +/- 36.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 459        |\n",
      "|    mean_reward          | 459        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 29000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04162268 |\n",
      "|    clip_fraction        | 0.104      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.417     |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.00823    |\n",
      "|    n_updates            | 114        |\n",
      "|    policy_gradient_loss | 0.00908    |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 383   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 29484 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=493.80 +/- 12.40\n",
      "Episode length: 493.80 +/- 12.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 494        |\n",
      "|    mean_reward          | 494        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 29500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03325619 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.37      |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.395      |\n",
      "|    n_updates            | 117        |\n",
      "|    policy_gradient_loss | 0.0159     |\n",
      "|    value_loss           | 1.39       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=493.60 +/- 12.80\n",
      "Episode length: 493.60 +/- 12.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 375   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 30240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=489.80 +/- 17.12\n",
      "Episode length: 489.80 +/- 17.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | 490         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011891385 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.362      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 371   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 83    |\n",
      "|    total_timesteps | 30996 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=487.60 +/- 15.20\n",
      "Episode length: 487.60 +/- 15.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | 488         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025377793 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.376      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 123         |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    value_loss           | 0.0144      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=444.60 +/- 45.70\n",
      "Episode length: 444.60 +/- 45.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 445      |\n",
      "|    mean_reward     | 445      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 368   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 86    |\n",
      "|    total_timesteps | 31752 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=492.00 +/- 9.98\n",
      "Episode length: 492.00 +/- 9.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 492         |\n",
      "|    mean_reward          | 492         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012175698 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    value_loss           | 0.445       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=485.00 +/- 18.59\n",
      "Episode length: 485.00 +/- 18.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 485      |\n",
      "|    mean_reward     | 485      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 364   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 89    |\n",
      "|    total_timesteps | 32508 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 33000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03700395 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.3       |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | -0.0201    |\n",
      "|    n_updates            | 129        |\n",
      "|    policy_gradient_loss | 0.0163     |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 90    |\n",
      "|    total_timesteps | 33264 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016013216 |\n",
      "|    clip_fraction        | 0.0746      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.00524     |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    value_loss           | 0.000448    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 364   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 93    |\n",
      "|    total_timesteps | 34020 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072230506 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -0.314       |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.0086       |\n",
      "|    loss                 | -0.0225      |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | 0.000523     |\n",
      "|    value_loss           | 0.000194     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 94    |\n",
      "|    total_timesteps | 34776 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032219756 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.005       |\n",
      "|    n_updates            | 138         |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 0.000109    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 364   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 97    |\n",
      "|    total_timesteps | 35532 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01696426 |\n",
      "|    clip_fraction        | 0.077      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.318     |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | -0.0093    |\n",
      "|    n_updates            | 141        |\n",
      "|    policy_gradient_loss | 0.00513    |\n",
      "|    value_loss           | 9.39e-05   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 99    |\n",
      "|    total_timesteps | 36288 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=499.00 +/- 2.00\n",
      "Episode length: 499.00 +/- 2.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 499        |\n",
      "|    mean_reward          | 499        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00786971 |\n",
      "|    clip_fraction        | 0.0554     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.337     |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | -0.00613   |\n",
      "|    n_updates            | 144        |\n",
      "|    policy_gradient_loss | 0.00272    |\n",
      "|    value_loss           | 5.52e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 364   |\n",
      "|    iterations      | 49    |\n",
      "|    time_elapsed    | 101   |\n",
      "|    total_timesteps | 37044 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016280865 |\n",
      "|    clip_fraction        | 0.0769      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 147         |\n",
      "|    policy_gradient_loss | 0.0021      |\n",
      "|    value_loss           | 2.74e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 37800 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=488.40 +/- 23.20\n",
      "Episode length: 488.40 +/- 23.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | 488         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018197866 |\n",
      "|    clip_fraction        | 0.0798      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.234      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.00849    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00185     |\n",
      "|    value_loss           | 7.39e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=493.40 +/- 13.20\n",
      "Episode length: 493.40 +/- 13.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 493      |\n",
      "|    mean_reward     | 493      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 359   |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 107   |\n",
      "|    total_timesteps | 38556 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=322.20 +/- 43.67\n",
      "Episode length: 322.20 +/- 43.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 322         |\n",
      "|    mean_reward          | 322         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018930601 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0471      |\n",
      "|    n_updates            | 153         |\n",
      "|    policy_gradient_loss | 0.00923     |\n",
      "|    value_loss           | 2.67e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 360   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 109   |\n",
      "|    total_timesteps | 39312 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=440.80 +/- 54.72\n",
      "Episode length: 440.80 +/- 54.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 441         |\n",
      "|    mean_reward          | 441         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028085737 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    value_loss           | 7.36e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=413.60 +/- 71.78\n",
      "Episode length: 413.60 +/- 71.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 414      |\n",
      "|    mean_reward     | 414      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 358   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 111   |\n",
      "|    total_timesteps | 40068 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=458.80 +/- 82.40\n",
      "Episode length: 458.80 +/- 82.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 459        |\n",
      "|    mean_reward          | 459        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01743167 |\n",
      "|    clip_fraction        | 0.0643     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.183     |\n",
      "|    explained_variance   | 0.774      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.00358    |\n",
      "|    n_updates            | 159        |\n",
      "|    policy_gradient_loss | 0.000825   |\n",
      "|    value_loss           | 0.000263   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 360   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 113   |\n",
      "|    total_timesteps | 40824 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=355.60 +/- 118.36\n",
      "Episode length: 355.60 +/- 118.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 356         |\n",
      "|    mean_reward          | 356         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011437796 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.196      |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.00443     |\n",
      "|    n_updates            | 162         |\n",
      "|    policy_gradient_loss | 0.00353     |\n",
      "|    value_loss           | 2.38e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=306.20 +/- 98.33\n",
      "Episode length: 306.20 +/- 98.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 306      |\n",
      "|    mean_reward     | 306      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 361   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 115   |\n",
      "|    total_timesteps | 41580 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=393.40 +/- 67.47\n",
      "Episode length: 393.40 +/- 67.47\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 393          |\n",
      "|    mean_reward          | 393          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069052107 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -0.217       |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.0086       |\n",
      "|    loss                 | -0.00804     |\n",
      "|    n_updates            | 165          |\n",
      "|    policy_gradient_loss | 0.00395      |\n",
      "|    value_loss           | 0.00261      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 116   |\n",
      "|    total_timesteps | 42336 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=330.00 +/- 101.24\n",
      "Episode length: 330.00 +/- 101.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 330         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010555171 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.205      |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | 0.00659     |\n",
      "|    value_loss           | 0.000198    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=306.80 +/- 97.25\n",
      "Episode length: 306.80 +/- 97.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 307      |\n",
      "|    mean_reward     | 307      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 57    |\n",
      "|    time_elapsed    | 118   |\n",
      "|    total_timesteps | 43092 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=342.00 +/- 130.18\n",
      "Episode length: 342.00 +/- 130.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 342          |\n",
      "|    mean_reward          | 342          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030291767 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | 0.611        |\n",
      "|    learning_rate        | 0.0086       |\n",
      "|    loss                 | 0.00301      |\n",
      "|    n_updates            | 171          |\n",
      "|    policy_gradient_loss | -0.0005      |\n",
      "|    value_loss           | 5.5e-05      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 58    |\n",
      "|    time_elapsed    | 119   |\n",
      "|    total_timesteps | 43848 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=402.40 +/- 89.57\n",
      "Episode length: 402.40 +/- 89.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 402         |\n",
      "|    mean_reward          | 402         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018860796 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.207      |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.00472    |\n",
      "|    n_updates            | 174         |\n",
      "|    policy_gradient_loss | 0.00847     |\n",
      "|    value_loss           | 1.1e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=375.80 +/- 77.62\n",
      "Episode length: 375.80 +/- 77.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 376      |\n",
      "|    mean_reward     | 376      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 59    |\n",
      "|    time_elapsed    | 121   |\n",
      "|    total_timesteps | 44604 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026368773 |\n",
      "|    clip_fraction        | 0.0763      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.197      |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | -0.0444     |\n",
      "|    n_updates            | 177         |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    value_loss           | 3.73e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 368   |\n",
      "|    iterations      | 60    |\n",
      "|    time_elapsed    | 123   |\n",
      "|    total_timesteps | 45360 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 45500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09323611 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.41      |\n",
      "|    explained_variance   | 0.699      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.0131     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.00826   |\n",
      "|    value_loss           | 3.15e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 61    |\n",
      "|    time_elapsed    | 125   |\n",
      "|    total_timesteps | 46116 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018465443 |\n",
      "|    clip_fraction        | 0.085       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.369      |\n",
      "|    explained_variance   | 0.00496     |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.46        |\n",
      "|    n_updates            | 183         |\n",
      "|    policy_gradient_loss | -0.000862   |\n",
      "|    value_loss           | 7.93        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 367   |\n",
      "|    iterations      | 62    |\n",
      "|    time_elapsed    | 127   |\n",
      "|    total_timesteps | 46872 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=315.00 +/- 97.41\n",
      "Episode length: 315.00 +/- 97.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 315        |\n",
      "|    mean_reward          | 315        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 47000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01711143 |\n",
      "|    clip_fraction        | 0.0621     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.416     |\n",
      "|    explained_variance   | 0.143      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 1.28       |\n",
      "|    n_updates            | 186        |\n",
      "|    policy_gradient_loss | -1.99e-05  |\n",
      "|    value_loss           | 8.19       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=339.60 +/- 81.31\n",
      "Episode length: 339.60 +/- 81.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 340      |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 368   |\n",
      "|    iterations      | 63    |\n",
      "|    time_elapsed    | 129   |\n",
      "|    total_timesteps | 47628 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 48000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06843197 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.332     |\n",
      "|    explained_variance   | 0.373      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 5.08       |\n",
      "|    n_updates            | 189        |\n",
      "|    policy_gradient_loss | 0.00269    |\n",
      "|    value_loss           | 10.2       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 369   |\n",
      "|    iterations      | 64    |\n",
      "|    time_elapsed    | 131   |\n",
      "|    total_timesteps | 48384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013127106 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.25        |\n",
      "|    n_updates            | 192         |\n",
      "|    policy_gradient_loss | -0.0029     |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 65    |\n",
      "|    time_elapsed    | 134   |\n",
      "|    total_timesteps | 49140 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=478.40 +/- 43.20\n",
      "Episode length: 478.40 +/- 43.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 478         |\n",
      "|    mean_reward          | 478         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 49500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057144124 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 195         |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    value_loss           | 1.14        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 366   |\n",
      "|    iterations      | 66    |\n",
      "|    time_elapsed    | 136   |\n",
      "|    total_timesteps | 49896 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=439.20 +/- 80.66\n",
      "Episode length: 439.20 +/- 80.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 439        |\n",
      "|    mean_reward          | 439        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 50000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14011844 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.326     |\n",
      "|    explained_variance   | 0.411      |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 6.34       |\n",
      "|    n_updates            | 198        |\n",
      "|    policy_gradient_loss | 0.0107     |\n",
      "|    value_loss           | 6.38       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=466.60 +/- 36.38\n",
      "Episode length: 466.60 +/- 36.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 467      |\n",
      "|    mean_reward     | 467      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 364   |\n",
      "|    iterations      | 67    |\n",
      "|    time_elapsed    | 138   |\n",
      "|    total_timesteps | 50652 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057516698 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.235      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0494      |\n",
      "|    n_updates            | 201         |\n",
      "|    policy_gradient_loss | 0.00692     |\n",
      "|    value_loss           | 0.0438      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 68    |\n",
      "|    time_elapsed    | 141   |\n",
      "|    total_timesteps | 51408 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=436.00 +/- 87.93\n",
      "Episode length: 436.00 +/- 87.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 436        |\n",
      "|    mean_reward          | 436        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 51500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03815623 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.148     |\n",
      "|    explained_variance   | -0.407     |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | -0.00425   |\n",
      "|    n_updates            | 204        |\n",
      "|    policy_gradient_loss | 0.00632    |\n",
      "|    value_loss           | 0.0341     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=454.80 +/- 58.32\n",
      "Episode length: 454.80 +/- 58.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 455      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 361   |\n",
      "|    iterations      | 69    |\n",
      "|    time_elapsed    | 144   |\n",
      "|    total_timesteps | 52164 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 52500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.37369284 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -0.113     |\n",
      "|    explained_variance   | 0.0859     |\n",
      "|    learning_rate        | 0.0086     |\n",
      "|    loss                 | 0.725      |\n",
      "|    n_updates            | 207        |\n",
      "|    policy_gradient_loss | 0.0181     |\n",
      "|    value_loss           | 2.5        |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 70    |\n",
      "|    time_elapsed    | 145   |\n",
      "|    total_timesteps | 52920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=453.60 +/- 58.36\n",
      "Episode length: 453.60 +/- 58.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 454         |\n",
      "|    mean_reward          | 454         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025155267 |\n",
      "|    clip_fraction        | 0.0558      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.146      |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.0434      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.00203     |\n",
      "|    value_loss           | 0.00735     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=377.40 +/- 72.12\n",
      "Episode length: 377.40 +/- 72.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 377      |\n",
      "|    mean_reward     | 377      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 361   |\n",
      "|    iterations      | 71    |\n",
      "|    time_elapsed    | 148   |\n",
      "|    total_timesteps | 53676 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=309.40 +/- 120.25\n",
      "Episode length: 309.40 +/- 120.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 309         |\n",
      "|    mean_reward          | 309         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032575075 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0086      |\n",
      "|    loss                 | 0.00195     |\n",
      "|    n_updates            | 213         |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 0.000191    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 72    |\n",
      "|    time_elapsed    | 149   |\n",
      "|    total_timesteps | 54432 |\n",
      "------------------------------\n",
      "Single environment training took 149.68 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>315.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-sweep-15</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/eqxegvos' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/eqxegvos</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_134707-eqxegvos/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vnf3c7eq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.23728679947549733\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0016631842259084272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9134182622221212\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.942158218417942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00979225000275359\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 4.928934873366703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 879\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 69361\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_134953-vnf3c7eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vnf3c7eq' target=\"_blank\">tough-sweep-16</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vnf3c7eq' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vnf3c7eq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 879`, after every 13 untruncated mini-batches, there will be a truncated mini-batch of size 47\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=879 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=9.20 +/- 0.40\n",
      "Episode length: 9.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 9.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1632 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 879  |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1000, episode_reward=189.40 +/- 37.59\n",
      "Episode length: 189.40 +/- 37.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 189         |\n",
      "|    mean_reward          | 189         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024092585 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | -0.0206     |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.647       |\n",
      "|    n_updates            | 5           |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    value_loss           | 4.24        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=200.20 +/- 156.43\n",
      "Episode length: 200.20 +/- 156.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 913  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1758 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=171.80 +/- 83.07\n",
      "Episode length: 171.80 +/- 83.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 172         |\n",
      "|    mean_reward          | 172         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044928405 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.801       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0336     |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=239.00 +/- 102.69\n",
      "Episode length: 239.00 +/- 102.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 239      |\n",
      "|    mean_reward     | 239      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 753  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2637 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=288.60 +/- 76.78\n",
      "Episode length: 288.60 +/- 76.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 289         |\n",
      "|    mean_reward          | 289         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053867366 |\n",
      "|    clip_fraction        | 0.313       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.832       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    value_loss           | 2.39        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=295.60 +/- 59.73\n",
      "Episode length: 295.60 +/- 59.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 296      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 667  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 3516 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=287.00 +/- 21.08\n",
      "Episode length: 287.00 +/- 21.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 287         |\n",
      "|    mean_reward          | 287         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043630134 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.202       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 1.23        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 654  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 4395 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=346.00 +/- 66.38\n",
      "Episode length: 346.00 +/- 66.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 346         |\n",
      "|    mean_reward          | 346         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045089263 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.509      |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.142       |\n",
      "|    n_updates            | 25          |\n",
      "|    policy_gradient_loss | 0.00192     |\n",
      "|    value_loss           | 0.464       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=423.80 +/- 43.06\n",
      "Episode length: 423.80 +/- 43.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 424      |\n",
      "|    mean_reward     | 424      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 528  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 5274 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=319.20 +/- 46.94\n",
      "Episode length: 319.20 +/- 46.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 319         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015542234 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.00558     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    value_loss           | 0.476       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=322.40 +/- 92.64\n",
      "Episode length: 322.40 +/- 92.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 322      |\n",
      "|    mean_reward     | 322      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 514  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 6153 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6500, episode_reward=285.20 +/- 18.02\n",
      "Episode length: 285.20 +/- 18.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 285         |\n",
      "|    mean_reward          | 285         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016841156 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.302       |\n",
      "|    n_updates            | 35          |\n",
      "|    policy_gradient_loss | 0.00393     |\n",
      "|    value_loss           | 1.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=283.80 +/- 26.48\n",
      "Episode length: 283.80 +/- 26.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | 284      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 509  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 7032 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=266.20 +/- 15.12\n",
      "Episode length: 266.20 +/- 15.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 266         |\n",
      "|    mean_reward          | 266         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026187861 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0468      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 530  |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 7911 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=292.20 +/- 16.51\n",
      "Episode length: 292.20 +/- 16.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 292         |\n",
      "|    mean_reward          | 292         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032227106 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.194       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.0138      |\n",
      "|    value_loss           | 0.0754      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=278.60 +/- 14.29\n",
      "Episode length: 278.60 +/- 14.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 279      |\n",
      "|    mean_reward     | 279      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 524  |\n",
      "|    iterations      | 10   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 8790 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=300.20 +/- 29.11\n",
      "Episode length: 300.20 +/- 29.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 300         |\n",
      "|    mean_reward          | 300         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032469563 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0265      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00909    |\n",
      "|    value_loss           | 0.245       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=333.40 +/- 36.50\n",
      "Episode length: 333.40 +/- 36.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 333      |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 489  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 19   |\n",
      "|    total_timesteps | 9669 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=303.00 +/- 31.62\n",
      "Episode length: 303.00 +/- 31.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 303        |\n",
      "|    mean_reward          | 303        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03404339 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.381     |\n",
      "|    explained_variance   | 0.902      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.145      |\n",
      "|    n_updates            | 55         |\n",
      "|    policy_gradient_loss | 0.0172     |\n",
      "|    value_loss           | 1.14       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=331.60 +/- 24.14\n",
      "Episode length: 331.60 +/- 24.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 332      |\n",
      "|    mean_reward     | 332      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 10548 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=244.00 +/- 29.83\n",
      "Episode length: 244.00 +/- 29.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 244         |\n",
      "|    mean_reward          | 244         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040015217 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.0134      |\n",
      "|    value_loss           | 0.523       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 498   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 11427 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=254.20 +/- 22.65\n",
      "Episode length: 254.20 +/- 22.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 254         |\n",
      "|    mean_reward          | 254         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016438512 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 65          |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=243.80 +/- 17.63\n",
      "Episode length: 243.80 +/- 17.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 244      |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 502   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 12306 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=290.40 +/- 105.70\n",
      "Episode length: 290.40 +/- 105.70\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | 290          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0114891445 |\n",
      "|    clip_fraction        | 0.086        |\n",
      "|    clip_range           | 0.237        |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.00979      |\n",
      "|    loss                 | -0.0429      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 0.101        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=221.00 +/- 24.56\n",
      "Episode length: 221.00 +/- 24.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 221      |\n",
      "|    mean_reward     | 221      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 13185 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=468.20 +/- 59.68\n",
      "Episode length: 468.20 +/- 59.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 468         |\n",
      "|    mean_reward          | 468         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022588596 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0225     |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.0147      |\n",
      "|    value_loss           | 0.027       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=462.00 +/- 76.00\n",
      "Episode length: 462.00 +/- 76.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 486   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 14064 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006385308 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00271    |\n",
      "|    value_loss           | 0.05        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 14943 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=490.20 +/- 19.60\n",
      "Episode length: 490.20 +/- 19.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | 490         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010503313 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 85          |\n",
      "|    policy_gradient_loss | -0.000603   |\n",
      "|    value_loss           | 0.0125      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 471   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 15822 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021688964 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.394      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.00554     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00403     |\n",
      "|    value_loss           | 0.0772      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=496.80 +/- 6.40\n",
      "Episode length: 496.80 +/- 6.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 497      |\n",
      "|    mean_reward     | 497      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 463   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 16701 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=499.60 +/- 0.80\n",
      "Episode length: 499.60 +/- 0.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01653069 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.422     |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0173     |\n",
      "|    n_updates            | 95         |\n",
      "|    policy_gradient_loss | -0.0083    |\n",
      "|    value_loss           | 0.0808     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=435.20 +/- 79.40\n",
      "Episode length: 435.20 +/- 79.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 435      |\n",
      "|    mean_reward     | 435      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 452   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 17580 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=499.40 +/- 1.20\n",
      "Episode length: 499.40 +/- 1.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 499         |\n",
      "|    mean_reward          | 499         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031221945 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0226      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    value_loss           | 0.0536      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 441   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 18459 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01624215 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.263     |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0877     |\n",
      "|    n_updates            | 105        |\n",
      "|    policy_gradient_loss | 0.0291     |\n",
      "|    value_loss           | 0.271      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=469.00 +/- 62.00\n",
      "Episode length: 469.00 +/- 62.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 469      |\n",
      "|    mean_reward     | 469      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 426   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 19338 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=188.20 +/- 13.17\n",
      "Episode length: 188.20 +/- 13.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 188         |\n",
      "|    mean_reward          | 188         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014940052 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.393      |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0641      |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00569     |\n",
      "|    value_loss           | 0.911       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=218.00 +/- 41.56\n",
      "Episode length: 218.00 +/- 41.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 218      |\n",
      "|    mean_reward     | 218      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 430   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 20217 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=269.80 +/- 45.34\n",
      "Episode length: 269.80 +/- 45.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 270         |\n",
      "|    mean_reward          | 270         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020030944 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.509       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 115         |\n",
      "|    policy_gradient_loss | -0.00282    |\n",
      "|    value_loss           | 0.728       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=350.20 +/- 110.47\n",
      "Episode length: 350.20 +/- 110.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 350      |\n",
      "|    mean_reward     | 350      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 431   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 21096 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 21500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19882832 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.261     |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.127      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.00918    |\n",
      "|    value_loss           | 0.678      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 426   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 21975 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=464.40 +/- 43.62\n",
      "Episode length: 464.40 +/- 43.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 464        |\n",
      "|    mean_reward          | 464        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29536155 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.241     |\n",
      "|    explained_variance   | 0.882      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.336      |\n",
      "|    n_updates            | 125        |\n",
      "|    policy_gradient_loss | -0.00213   |\n",
      "|    value_loss           | 0.28       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=471.60 +/- 56.80\n",
      "Episode length: 471.60 +/- 56.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 472      |\n",
      "|    mean_reward     | 472      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 417   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 22854 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=495.40 +/- 9.20\n",
      "Episode length: 495.40 +/- 9.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 495        |\n",
      "|    mean_reward          | 495        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18145548 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.232     |\n",
      "|    explained_variance   | 0.651      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.262      |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | 0.0196     |\n",
      "|    value_loss           | 0.685      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 409   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 23733 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=497.40 +/- 5.20\n",
      "Episode length: 497.40 +/- 5.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | 497         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079610534 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.163      |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0801      |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    value_loss           | 1.19        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 405   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 24612 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018142233 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.203      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 0.00814     |\n",
      "|    value_loss           | 0.00826     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 408   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 25491 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029658645 |\n",
      "|    clip_fraction        | 0.0898      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.291      |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 145         |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    value_loss           | 0.45        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 405   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 26370 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027772063 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.00446    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00795     |\n",
      "|    value_loss           | 0.0062      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 400   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 27249 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 27500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09657929 |\n",
      "|    clip_fraction        | 0.348      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.285     |\n",
      "|    explained_variance   | 0.619      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 1.89       |\n",
      "|    n_updates            | 155        |\n",
      "|    policy_gradient_loss | 0.0169     |\n",
      "|    value_loss           | 4.01       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=494.20 +/- 11.60\n",
      "Episode length: 494.20 +/- 11.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 395   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 28128 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02360768 |\n",
      "|    clip_fraction        | 0.0867     |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.22      |\n",
      "|    explained_variance   | 0.89       |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0754     |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | 0.00829    |\n",
      "|    value_loss           | 0.263      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 384   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 29007 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039494745 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.293      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 165         |\n",
      "|    policy_gradient_loss | 0.0227      |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 386   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 29886 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011114215 |\n",
      "|    clip_fraction        | 0.0784      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.252      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.304       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 382   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 30765 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025502032 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.33       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0905      |\n",
      "|    n_updates            | 175         |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 378   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 83    |\n",
      "|    total_timesteps | 31644 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01996255 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.326     |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0226     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | 0.00439    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 376   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 86    |\n",
      "|    total_timesteps | 32523 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061432183 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.408      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0438      |\n",
      "|    n_updates            | 185         |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    value_loss           | 0.0898      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 378   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 88    |\n",
      "|    total_timesteps | 33402 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 500       |\n",
      "|    mean_reward          | 500       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 33500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7907237 |\n",
      "|    clip_fraction        | 0.377     |\n",
      "|    clip_range           | 0.237     |\n",
      "|    entropy_loss         | -0.243    |\n",
      "|    explained_variance   | 0.0633    |\n",
      "|    learning_rate        | 0.00979   |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 190       |\n",
      "|    policy_gradient_loss | 0.0117    |\n",
      "|    value_loss           | 0.851     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 374   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 34281 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=266.20 +/- 124.31\n",
      "Episode length: 266.20 +/- 124.31\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 266       |\n",
      "|    mean_reward          | 266       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 34500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0829861 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.237     |\n",
      "|    entropy_loss         | -0.174    |\n",
      "|    explained_variance   | 0.796     |\n",
      "|    learning_rate        | 0.00979   |\n",
      "|    loss                 | 0.349     |\n",
      "|    n_updates            | 195       |\n",
      "|    policy_gradient_loss | 0.0198    |\n",
      "|    value_loss           | 0.369     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=201.20 +/- 28.09\n",
      "Episode length: 201.20 +/- 28.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 375   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 93    |\n",
      "|    total_timesteps | 35160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=248.00 +/- 126.06\n",
      "Episode length: 248.00 +/- 126.06\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 248        |\n",
      "|    mean_reward          | 248        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07966608 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.179     |\n",
      "|    explained_variance   | 0.951      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.14       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.0125     |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=254.80 +/- 122.76\n",
      "Episode length: 254.80 +/- 122.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 255      |\n",
      "|    mean_reward     | 255      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 374   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 96    |\n",
      "|    total_timesteps | 36039 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17206629 |\n",
      "|    clip_fraction        | 0.0932     |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 205        |\n",
      "|    policy_gradient_loss | 0.0209     |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 376   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 36918 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009513174 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0711      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.0198      |\n",
      "|    value_loss           | 0.0232      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 373   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 101   |\n",
      "|    total_timesteps | 37797 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=497.20 +/- 5.60\n",
      "Episode length: 497.20 +/- 5.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | 497         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014473282 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 215         |\n",
      "|    policy_gradient_loss | 0.0169      |\n",
      "|    value_loss           | 0.00085     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 367   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 105   |\n",
      "|    total_timesteps | 38676 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=473.60 +/- 52.80\n",
      "Episode length: 473.60 +/- 52.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 474         |\n",
      "|    mean_reward          | 474         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013078606 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.182      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0325      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.0305      |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=301.80 +/- 54.47\n",
      "Episode length: 301.80 +/- 54.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 302      |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 365   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 108   |\n",
      "|    total_timesteps | 39555 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=488.20 +/- 23.60\n",
      "Episode length: 488.20 +/- 23.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | 488         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009377851 |\n",
      "|    clip_fraction        | 0.0723      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.141      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.00941     |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | 0.00797     |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 367   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 110   |\n",
      "|    total_timesteps | 40434 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05182786 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.178     |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0319     |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | 0.00228    |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 363   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 113   |\n",
      "|    total_timesteps | 41313 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018657636 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0332      |\n",
      "|    n_updates            | 235         |\n",
      "|    policy_gradient_loss | 0.00557     |\n",
      "|    value_loss           | 0.125       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 357   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 117   |\n",
      "|    total_timesteps | 42192 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 42500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01591002 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.23      |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0343     |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | 0.00699    |\n",
      "|    value_loss           | 0.0476     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 348   |\n",
      "|    iterations      | 49    |\n",
      "|    time_elapsed    | 123   |\n",
      "|    total_timesteps | 43071 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=476.80 +/- 46.40\n",
      "Episode length: 476.80 +/- 46.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 477        |\n",
      "|    mean_reward          | 477        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 43500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10728385 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.188     |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.00518    |\n",
      "|    n_updates            | 245        |\n",
      "|    policy_gradient_loss | 0.0309     |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 344   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 127   |\n",
      "|    total_timesteps | 43950 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007222812 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0241      |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | 0.00276     |\n",
      "|    value_loss           | 3.6e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=493.60 +/- 7.94\n",
      "Episode length: 493.60 +/- 7.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 494      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 331   |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 135   |\n",
      "|    total_timesteps | 44829 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=466.00 +/- 68.00\n",
      "Episode length: 466.00 +/- 68.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 466         |\n",
      "|    mean_reward          | 466         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017319424 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.185      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 255         |\n",
      "|    policy_gradient_loss | 0.0156      |\n",
      "|    value_loss           | 2.65e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 319   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 143   |\n",
      "|    total_timesteps | 45708 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 46000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03058101 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.169     |\n",
      "|    explained_variance   | 0.634      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.197      |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | 0.00251    |\n",
      "|    value_loss           | 0.464      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 319   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 145   |\n",
      "|    total_timesteps | 46587 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=424.20 +/- 94.23\n",
      "Episode length: 424.20 +/- 94.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 424        |\n",
      "|    mean_reward          | 424        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 47000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11422328 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.191     |\n",
      "|    explained_variance   | -0.0331    |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0448     |\n",
      "|    n_updates            | 265        |\n",
      "|    policy_gradient_loss | 0.00549    |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 322   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 147   |\n",
      "|    total_timesteps | 47466 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=464.40 +/- 71.20\n",
      "Episode length: 464.40 +/- 71.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 464         |\n",
      "|    mean_reward          | 464         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051015295 |\n",
      "|    clip_fraction        | 0.0683      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.000142    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.00817     |\n",
      "|    value_loss           | 0.0291      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=443.40 +/- 113.20\n",
      "Episode length: 443.40 +/- 113.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 443      |\n",
      "|    mean_reward     | 443      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 322   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 149   |\n",
      "|    total_timesteps | 48345 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 48500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09031492 |\n",
      "|    clip_fraction        | 0.0967     |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.181     |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0651     |\n",
      "|    n_updates            | 275        |\n",
      "|    policy_gradient_loss | 0.00758    |\n",
      "|    value_loss           | 0.0706     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=457.00 +/- 86.00\n",
      "Episode length: 457.00 +/- 86.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 457      |\n",
      "|    mean_reward     | 457      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 322   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 152   |\n",
      "|    total_timesteps | 49224 |\n",
      "------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 49500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07329879 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.141     |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0715     |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | 0.00911    |\n",
      "|    value_loss           | 0.00387    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 322   |\n",
      "|    iterations      | 57    |\n",
      "|    time_elapsed    | 155   |\n",
      "|    total_timesteps | 50103 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=465.00 +/- 70.00\n",
      "Episode length: 465.00 +/- 70.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 465         |\n",
      "|    mean_reward          | 465         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028306345 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0372      |\n",
      "|    n_updates            | 285         |\n",
      "|    policy_gradient_loss | 0.0076      |\n",
      "|    value_loss           | 0.0371      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 324   |\n",
      "|    iterations      | 58    |\n",
      "|    time_elapsed    | 156   |\n",
      "|    total_timesteps | 50982 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037282784 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.162      |\n",
      "|    explained_variance   | 0.011       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.0179      |\n",
      "|    value_loss           | 0.000304    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 325   |\n",
      "|    iterations      | 59    |\n",
      "|    time_elapsed    | 159   |\n",
      "|    total_timesteps | 51861 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009888151 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.00524     |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.000913   |\n",
      "|    n_updates            | 295         |\n",
      "|    policy_gradient_loss | 0.000824    |\n",
      "|    value_loss           | 4.68e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 327   |\n",
      "|    iterations      | 60    |\n",
      "|    time_elapsed    | 161   |\n",
      "|    total_timesteps | 52740 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010823035 |\n",
      "|    clip_fraction        | 0.0768      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.137      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    value_loss           | 0.157       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 328   |\n",
      "|    iterations      | 61    |\n",
      "|    time_elapsed    | 163   |\n",
      "|    total_timesteps | 53619 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032805067 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.344       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 305         |\n",
      "|    policy_gradient_loss | 0.00125     |\n",
      "|    value_loss           | 0.000411    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 330   |\n",
      "|    iterations      | 62    |\n",
      "|    time_elapsed    | 164   |\n",
      "|    total_timesteps | 54498 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 54500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02432868 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.253     |\n",
      "|    explained_variance   | 0.836      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0444     |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | 0.0087     |\n",
      "|    value_loss           | 0.283      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 331   |\n",
      "|    iterations      | 63    |\n",
      "|    time_elapsed    | 166   |\n",
      "|    total_timesteps | 55377 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 55500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08450152 |\n",
      "|    clip_fraction        | 0.307      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.247     |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.472      |\n",
      "|    n_updates            | 315        |\n",
      "|    policy_gradient_loss | 0.0339     |\n",
      "|    value_loss           | 1.25       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 332   |\n",
      "|    iterations      | 64    |\n",
      "|    time_elapsed    | 168   |\n",
      "|    total_timesteps | 56256 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 56500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10756584 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.224     |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | 0.00554    |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 333   |\n",
      "|    iterations      | 65    |\n",
      "|    time_elapsed    | 171   |\n",
      "|    total_timesteps | 57135 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 57500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43462688 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.13      |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | -0.0152    |\n",
      "|    n_updates            | 325        |\n",
      "|    policy_gradient_loss | 0.00779    |\n",
      "|    value_loss           | 0.0146     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 334   |\n",
      "|    iterations      | 66    |\n",
      "|    time_elapsed    | 173   |\n",
      "|    total_timesteps | 58014 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051111218 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | 0.000688    |\n",
      "|    value_loss           | 0.048       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 337   |\n",
      "|    iterations      | 67    |\n",
      "|    time_elapsed    | 174   |\n",
      "|    total_timesteps | 58893 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=490.40 +/- 19.20\n",
      "Episode length: 490.40 +/- 19.20\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 490       |\n",
      "|    mean_reward          | 490       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 59000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0318488 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.237     |\n",
      "|    entropy_loss         | -0.206    |\n",
      "|    explained_variance   | 0.14      |\n",
      "|    learning_rate        | 0.00979   |\n",
      "|    loss                 | 0.00672   |\n",
      "|    n_updates            | 335       |\n",
      "|    policy_gradient_loss | 0.0038    |\n",
      "|    value_loss           | 0.000135  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 338   |\n",
      "|    iterations      | 68    |\n",
      "|    time_elapsed    | 176   |\n",
      "|    total_timesteps | 59772 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058580887 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.17       |\n",
      "|    explained_variance   | 0.916       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.0116      |\n",
      "|    value_loss           | 0.156       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 339   |\n",
      "|    iterations      | 69    |\n",
      "|    time_elapsed    | 178   |\n",
      "|    total_timesteps | 60651 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=488.40 +/- 23.20\n",
      "Episode length: 488.40 +/- 23.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | 488         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 61000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058803823 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.169      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0685      |\n",
      "|    n_updates            | 345         |\n",
      "|    policy_gradient_loss | 0.00395     |\n",
      "|    value_loss           | 0.293       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 340   |\n",
      "|    iterations      | 70    |\n",
      "|    time_elapsed    | 180   |\n",
      "|    total_timesteps | 61530 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027744327 |\n",
      "|    clip_fraction        | 0.0831      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.174      |\n",
      "|    explained_variance   | -0.00022    |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | 0.0178      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.00767     |\n",
      "|    value_loss           | 0.000245    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 342   |\n",
      "|    iterations      | 71    |\n",
      "|    time_elapsed    | 182   |\n",
      "|    total_timesteps | 62409 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=460.80 +/- 51.86\n",
      "Episode length: 460.80 +/- 51.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 461         |\n",
      "|    mean_reward          | 461         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009662794 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.201      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.00158    |\n",
      "|    n_updates            | 355         |\n",
      "|    policy_gradient_loss | 0.00901     |\n",
      "|    value_loss           | 1.78e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 343   |\n",
      "|    iterations      | 72    |\n",
      "|    time_elapsed    | 184   |\n",
      "|    total_timesteps | 63288 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 63500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038676278 |\n",
      "|    clip_fraction        | 0.0812       |\n",
      "|    clip_range           | 0.237        |\n",
      "|    entropy_loss         | -0.171       |\n",
      "|    explained_variance   | 0.528        |\n",
      "|    learning_rate        | 0.00979      |\n",
      "|    loss                 | 0.00519      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | 0.00579      |\n",
      "|    value_loss           | 2.76e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 344   |\n",
      "|    iterations      | 73    |\n",
      "|    time_elapsed    | 186   |\n",
      "|    total_timesteps | 64167 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014934175 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.139      |\n",
      "|    explained_variance   | 0.365       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.00242    |\n",
      "|    n_updates            | 365         |\n",
      "|    policy_gradient_loss | -0.000602   |\n",
      "|    value_loss           | 1.27e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 345   |\n",
      "|    iterations      | 74    |\n",
      "|    time_elapsed    | 188   |\n",
      "|    total_timesteps | 65046 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031834517 |\n",
      "|    clip_fraction        | 0.0793      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.0025      |\n",
      "|    value_loss           | 8.59e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 347   |\n",
      "|    iterations      | 75    |\n",
      "|    time_elapsed    | 189   |\n",
      "|    total_timesteps | 65925 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 66000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08215816 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.178     |\n",
      "|    explained_variance   | -0.00089   |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.0217     |\n",
      "|    n_updates            | 375        |\n",
      "|    policy_gradient_loss | 0.0118     |\n",
      "|    value_loss           | 3.61e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 348   |\n",
      "|    iterations      | 76    |\n",
      "|    time_elapsed    | 191   |\n",
      "|    total_timesteps | 66804 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 67000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028087525 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | -0.0696     |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.00403    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 0.00323     |\n",
      "|    value_loss           | 1.37e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 348   |\n",
      "|    iterations      | 77    |\n",
      "|    time_elapsed    | 193   |\n",
      "|    total_timesteps | 67683 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 68000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03971589 |\n",
      "|    clip_fraction        | 0.0976     |\n",
      "|    clip_range           | 0.237      |\n",
      "|    entropy_loss         | -0.201     |\n",
      "|    explained_variance   | 0.196      |\n",
      "|    learning_rate        | 0.00979    |\n",
      "|    loss                 | 0.00819    |\n",
      "|    n_updates            | 385        |\n",
      "|    policy_gradient_loss | 0.00745    |\n",
      "|    value_loss           | 5.71e-08   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 349   |\n",
      "|    iterations      | 78    |\n",
      "|    time_elapsed    | 196   |\n",
      "|    total_timesteps | 68562 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 69000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009354572 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.237       |\n",
      "|    entropy_loss         | -0.156      |\n",
      "|    explained_variance   | -0.000325   |\n",
      "|    learning_rate        | 0.00979     |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.000127   |\n",
      "|    value_loss           | 1.56e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 351   |\n",
      "|    iterations      | 79    |\n",
      "|    time_elapsed    | 197   |\n",
      "|    total_timesteps | 69441 |\n",
      "------------------------------\n",
      "Single environment training took 197.65 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-16</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vnf3c7eq' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vnf3c7eq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_134953-vnf3c7eq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3nsyighl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.1678469335539313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0014188947493194357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9469893730279674\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9479189901092728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007317704867293711\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.6012951338231856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 85290\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_135325-3nsyighl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/3nsyighl' target=\"_blank\">clean-sweep-17</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/3nsyighl' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/3nsyighl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1782`, after every 27 untruncated mini-batches, there will be a truncated mini-batch of size 54\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1782 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=51.20 +/- 13.96\n",
      "Episode length: 51.20 +/- 13.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 51.2     |\n",
      "|    mean_reward     | 51.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=97.80 +/- 29.69\n",
      "Episode length: 97.80 +/- 29.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 97.8     |\n",
      "|    mean_reward     | 97.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=58.80 +/- 21.19\n",
      "Episode length: 58.80 +/- 21.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 58.8     |\n",
      "|    mean_reward     | 58.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1484 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1782 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2000, episode_reward=275.80 +/- 62.69\n",
      "Episode length: 275.80 +/- 62.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 276         |\n",
      "|    mean_reward          | 276         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011726804 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | -0.0215     |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.801       |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 3.72        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=300.20 +/- 85.20\n",
      "Episode length: 300.20 +/- 85.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 300      |\n",
      "|    mean_reward     | 300      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=228.80 +/- 25.44\n",
      "Episode length: 228.80 +/- 25.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=236.80 +/- 100.33\n",
      "Episode length: 236.80 +/- 100.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 237      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 846  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 3564 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=451.80 +/- 96.40\n",
      "Episode length: 451.80 +/- 96.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 452         |\n",
      "|    mean_reward          | 452         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014611243 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.649      |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 2.1         |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 4.91        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4500, episode_reward=381.40 +/- 111.94\n",
      "Episode length: 381.40 +/- 111.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 381      |\n",
      "|    mean_reward     | 381      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=403.00 +/- 119.06\n",
      "Episode length: 403.00 +/- 119.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 403      |\n",
      "|    mean_reward     | 403      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 713  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 5346 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01400468 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.168      |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 0.569      |\n",
      "|    learning_rate        | 0.00732    |\n",
      "|    loss                 | 3.52       |\n",
      "|    n_updates            | 27         |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    value_loss           | 5.08       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=455.40 +/- 85.74\n",
      "Episode length: 455.40 +/- 85.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 455      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=374.80 +/- 103.66\n",
      "Episode length: 374.80 +/- 103.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 375      |\n",
      "|    mean_reward     | 375      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=403.60 +/- 109.21\n",
      "Episode length: 403.60 +/- 109.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 404      |\n",
      "|    mean_reward     | 404      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 619  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 7128 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=391.40 +/- 113.96\n",
      "Episode length: 391.40 +/- 113.96\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 391        |\n",
      "|    mean_reward          | 391        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02144147 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.168      |\n",
      "|    entropy_loss         | -0.542     |\n",
      "|    explained_variance   | 0.591      |\n",
      "|    learning_rate        | 0.00732    |\n",
      "|    loss                 | 2.56       |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    value_loss           | 4.19       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=471.00 +/- 43.42\n",
      "Episode length: 471.00 +/- 43.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=459.40 +/- 24.13\n",
      "Episode length: 459.40 +/- 24.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 459      |\n",
      "|    mean_reward     | 459      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 599  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 8910 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9000, episode_reward=341.00 +/- 95.90\n",
      "Episode length: 341.00 +/- 95.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 341         |\n",
      "|    mean_reward          | 341         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018918425 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.122       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.000825    |\n",
      "|    value_loss           | 0.576       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=305.00 +/- 72.20\n",
      "Episode length: 305.00 +/- 72.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 305      |\n",
      "|    mean_reward     | 305      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=453.20 +/- 72.04\n",
      "Episode length: 453.20 +/- 72.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 453      |\n",
      "|    mean_reward     | 453      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=320.00 +/- 68.80\n",
      "Episode length: 320.00 +/- 68.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 320      |\n",
      "|    mean_reward     | 320      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 580   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 10692 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=320.20 +/- 94.58\n",
      "Episode length: 320.20 +/- 94.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 320         |\n",
      "|    mean_reward          | 320         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006694756 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0738      |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | -6.76e-06   |\n",
      "|    value_loss           | 0.752       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=375.00 +/- 95.19\n",
      "Episode length: 375.00 +/- 95.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 375      |\n",
      "|    mean_reward     | 375      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=347.20 +/- 67.09\n",
      "Episode length: 347.20 +/- 67.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 347      |\n",
      "|    mean_reward     | 347      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 581   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 12474 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=382.40 +/- 111.73\n",
      "Episode length: 382.40 +/- 111.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 382         |\n",
      "|    mean_reward          | 382         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028222892 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.11        |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | 0.00403     |\n",
      "|    value_loss           | 0.282       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=399.20 +/- 99.38\n",
      "Episode length: 399.20 +/- 99.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 399      |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=360.20 +/- 111.67\n",
      "Episode length: 360.20 +/- 111.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 360      |\n",
      "|    mean_reward     | 360      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=445.40 +/- 109.20\n",
      "Episode length: 445.40 +/- 109.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 445      |\n",
      "|    mean_reward     | 445      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 564   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 14256 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=319.40 +/- 98.59\n",
      "Episode length: 319.40 +/- 98.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 319         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011041853 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.00768     |\n",
      "|    value_loss           | 0.0821      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=393.80 +/- 130.54\n",
      "Episode length: 393.80 +/- 130.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 394      |\n",
      "|    mean_reward     | 394      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=345.00 +/- 94.09\n",
      "Episode length: 345.00 +/- 94.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 345      |\n",
      "|    mean_reward     | 345      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=387.60 +/- 119.11\n",
      "Episode length: 387.60 +/- 119.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 388      |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 554   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 16038 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=449.40 +/- 83.93\n",
      "Episode length: 449.40 +/- 83.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 449         |\n",
      "|    mean_reward          | 449         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014419432 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.308       |\n",
      "|    n_updates            | 81          |\n",
      "|    policy_gradient_loss | 0.00169     |\n",
      "|    value_loss           | 0.621       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=433.60 +/- 101.71\n",
      "Episode length: 433.60 +/- 101.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 434      |\n",
      "|    mean_reward     | 434      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=387.40 +/- 98.64\n",
      "Episode length: 387.40 +/- 98.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 387      |\n",
      "|    mean_reward     | 387      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 553   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 17820 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=429.20 +/- 99.69\n",
      "Episode length: 429.20 +/- 99.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 429         |\n",
      "|    mean_reward          | 429         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014012623 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.422      |\n",
      "|    explained_variance   | 0.885       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.244       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00304     |\n",
      "|    value_loss           | 0.631       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=430.60 +/- 40.83\n",
      "Episode length: 430.60 +/- 40.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 431      |\n",
      "|    mean_reward     | 431      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=458.40 +/- 51.34\n",
      "Episode length: 458.40 +/- 51.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 458      |\n",
      "|    mean_reward     | 458      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=416.20 +/- 92.87\n",
      "Episode length: 416.20 +/- 92.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 416      |\n",
      "|    mean_reward     | 416      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 540   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 19602 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=290.60 +/- 48.99\n",
      "Episode length: 290.60 +/- 48.99\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 291        |\n",
      "|    mean_reward          | 291        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01018366 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.168      |\n",
      "|    entropy_loss         | -0.477     |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.00732    |\n",
      "|    loss                 | -0.0238    |\n",
      "|    n_updates            | 99         |\n",
      "|    policy_gradient_loss | 0.000944   |\n",
      "|    value_loss           | 0.0674     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=340.40 +/- 77.39\n",
      "Episode length: 340.40 +/- 77.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 340      |\n",
      "|    mean_reward     | 340      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=324.60 +/- 99.79\n",
      "Episode length: 324.60 +/- 99.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 325      |\n",
      "|    mean_reward     | 325      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 546   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 21384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=185.40 +/- 4.45\n",
      "Episode length: 185.40 +/- 4.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 185         |\n",
      "|    mean_reward          | 185         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012430762 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | 0.00697     |\n",
      "|    value_loss           | 0.0343      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=181.20 +/- 9.54\n",
      "Episode length: 181.20 +/- 9.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 181      |\n",
      "|    mean_reward     | 181      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=178.20 +/- 7.73\n",
      "Episode length: 178.20 +/- 7.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 178      |\n",
      "|    mean_reward     | 178      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=188.60 +/- 9.11\n",
      "Episode length: 188.60 +/- 9.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 189      |\n",
      "|    mean_reward     | 189      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 557   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 23166 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=194.00 +/- 14.14\n",
      "Episode length: 194.00 +/- 14.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 194         |\n",
      "|    mean_reward          | 194         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 23500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016291311 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 117         |\n",
      "|    policy_gradient_loss | 0.00604     |\n",
      "|    value_loss           | 0.0613      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=197.80 +/- 8.93\n",
      "Episode length: 197.80 +/- 8.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | 198      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=204.60 +/- 20.61\n",
      "Episode length: 204.60 +/- 20.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 205      |\n",
      "|    mean_reward     | 205      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 568   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 24948 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=206.60 +/- 9.35\n",
      "Episode length: 206.60 +/- 9.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 207        |\n",
      "|    mean_reward          | 207        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00860972 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.168      |\n",
      "|    entropy_loss         | -0.432     |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.00732    |\n",
      "|    loss                 | 0.0254     |\n",
      "|    n_updates            | 126        |\n",
      "|    policy_gradient_loss | 2.38e-06   |\n",
      "|    value_loss           | 0.0349     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=211.40 +/- 8.21\n",
      "Episode length: 211.40 +/- 8.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 211      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=214.40 +/- 14.14\n",
      "Episode length: 214.40 +/- 14.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | 214      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=208.80 +/- 12.89\n",
      "Episode length: 208.80 +/- 12.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 209      |\n",
      "|    mean_reward     | 209      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 574   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 26730 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=209.80 +/- 15.97\n",
      "Episode length: 209.80 +/- 15.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 210         |\n",
      "|    mean_reward          | 210         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018388731 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.201       |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    value_loss           | 0.049       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=193.60 +/- 10.71\n",
      "Episode length: 193.60 +/- 10.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 194      |\n",
      "|    mean_reward     | 194      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=209.80 +/- 15.12\n",
      "Episode length: 209.80 +/- 15.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 210      |\n",
      "|    mean_reward     | 210      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=198.40 +/- 11.15\n",
      "Episode length: 198.40 +/- 11.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 198      |\n",
      "|    mean_reward     | 198      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 579   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 28512 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=219.00 +/- 11.30\n",
      "Episode length: 219.00 +/- 11.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 219         |\n",
      "|    mean_reward          | 219         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012950086 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0673      |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=222.00 +/- 10.06\n",
      "Episode length: 222.00 +/- 10.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 222      |\n",
      "|    mean_reward     | 222      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=240.80 +/- 11.62\n",
      "Episode length: 240.80 +/- 11.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 241      |\n",
      "|    mean_reward     | 241      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 587   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 30294 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=252.20 +/- 19.74\n",
      "Episode length: 252.20 +/- 19.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 252         |\n",
      "|    mean_reward          | 252         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019844925 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0255      |\n",
      "|    n_updates            | 153         |\n",
      "|    policy_gradient_loss | 0.000756    |\n",
      "|    value_loss           | 0.0529      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=250.80 +/- 12.50\n",
      "Episode length: 250.80 +/- 12.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 251      |\n",
      "|    mean_reward     | 251      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=243.00 +/- 15.27\n",
      "Episode length: 243.00 +/- 15.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 243      |\n",
      "|    mean_reward     | 243      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=245.00 +/- 7.69\n",
      "Episode length: 245.00 +/- 7.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 245      |\n",
      "|    mean_reward     | 245      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 588   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 32076 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=256.60 +/- 22.50\n",
      "Episode length: 256.60 +/- 22.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 257         |\n",
      "|    mean_reward          | 257         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010330266 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0223      |\n",
      "|    n_updates            | 162         |\n",
      "|    policy_gradient_loss | -0.00458    |\n",
      "|    value_loss           | 0.107       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=263.40 +/- 7.45\n",
      "Episode length: 263.40 +/- 7.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 263      |\n",
      "|    mean_reward     | 263      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=261.60 +/- 12.04\n",
      "Episode length: 261.60 +/- 12.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 262      |\n",
      "|    mean_reward     | 262      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 592   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 33858 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=325.80 +/- 23.40\n",
      "Episode length: 325.80 +/- 23.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 326         |\n",
      "|    mean_reward          | 326         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011873999 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0651      |\n",
      "|    n_updates            | 171         |\n",
      "|    policy_gradient_loss | 0.00544     |\n",
      "|    value_loss           | 0.0525      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=320.80 +/- 58.79\n",
      "Episode length: 320.80 +/- 58.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 321      |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=312.60 +/- 27.80\n",
      "Episode length: 312.60 +/- 27.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 313      |\n",
      "|    mean_reward     | 313      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=306.20 +/- 24.38\n",
      "Episode length: 306.20 +/- 24.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 306      |\n",
      "|    mean_reward     | 306      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 589   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 35640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=290.20 +/- 20.97\n",
      "Episode length: 290.20 +/- 20.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 290         |\n",
      "|    mean_reward          | 290         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019522617 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00936     |\n",
      "|    value_loss           | 0.0213      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=309.00 +/- 27.57\n",
      "Episode length: 309.00 +/- 27.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 309      |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=301.20 +/- 9.22\n",
      "Episode length: 301.20 +/- 9.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 301      |\n",
      "|    mean_reward     | 301      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 591   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 37422 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=432.20 +/- 52.23\n",
      "Episode length: 432.20 +/- 52.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 432         |\n",
      "|    mean_reward          | 432         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016373755 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.00941     |\n",
      "|    n_updates            | 189         |\n",
      "|    policy_gradient_loss | 0.00512     |\n",
      "|    value_loss           | 0.0236      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=457.80 +/- 26.11\n",
      "Episode length: 457.80 +/- 26.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 458      |\n",
      "|    mean_reward     | 458      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=407.60 +/- 54.66\n",
      "Episode length: 407.60 +/- 54.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 408      |\n",
      "|    mean_reward     | 408      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=430.40 +/- 47.67\n",
      "Episode length: 430.40 +/- 47.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 430      |\n",
      "|    mean_reward     | 430      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 579   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 39204 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=229.60 +/- 9.91\n",
      "Episode length: 229.60 +/- 9.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 230         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 39500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013149636 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0815      |\n",
      "|    n_updates            | 198         |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 0.982       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=236.40 +/- 9.62\n",
      "Episode length: 236.40 +/- 9.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=229.00 +/- 9.78\n",
      "Episode length: 229.00 +/- 9.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 569   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 40986 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=232.80 +/- 11.30\n",
      "Episode length: 232.80 +/- 11.30\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 233        |\n",
      "|    mean_reward          | 233        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 41000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02206702 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.168      |\n",
      "|    entropy_loss         | -0.472     |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.00732    |\n",
      "|    loss                 | 0.0554     |\n",
      "|    n_updates            | 207        |\n",
      "|    policy_gradient_loss | -0.00419   |\n",
      "|    value_loss           | 0.078      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=222.40 +/- 5.54\n",
      "Episode length: 222.40 +/- 5.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 222      |\n",
      "|    mean_reward     | 222      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=225.40 +/- 12.34\n",
      "Episode length: 225.40 +/- 12.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 225      |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=228.80 +/- 9.58\n",
      "Episode length: 228.80 +/- 9.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 568   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 42768 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=216.60 +/- 14.64\n",
      "Episode length: 216.60 +/- 14.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 217          |\n",
      "|    mean_reward          | 217          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 43000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125959385 |\n",
      "|    clip_fraction        | 0.175        |\n",
      "|    clip_range           | 0.168        |\n",
      "|    entropy_loss         | -0.46        |\n",
      "|    explained_variance   | 0.939        |\n",
      "|    learning_rate        | 0.00732      |\n",
      "|    loss                 | 0.0909       |\n",
      "|    n_updates            | 216          |\n",
      "|    policy_gradient_loss | -0.00614     |\n",
      "|    value_loss           | 0.364        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=225.40 +/- 7.50\n",
      "Episode length: 225.40 +/- 7.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 225      |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=217.00 +/- 6.93\n",
      "Episode length: 217.00 +/- 6.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 217      |\n",
      "|    mean_reward     | 217      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=215.40 +/- 8.89\n",
      "Episode length: 215.40 +/- 8.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 215      |\n",
      "|    mean_reward     | 215      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 567   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 44550 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=237.40 +/- 10.48\n",
      "Episode length: 237.40 +/- 10.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 237         |\n",
      "|    mean_reward          | 237         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009857315 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.071       |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    value_loss           | 0.255       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=228.80 +/- 8.23\n",
      "Episode length: 228.80 +/- 8.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=235.80 +/- 16.86\n",
      "Episode length: 235.80 +/- 16.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 236      |\n",
      "|    mean_reward     | 236      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 544   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 46332 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=237.60 +/- 11.20\n",
      "Episode length: 237.60 +/- 11.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 238         |\n",
      "|    mean_reward          | 238         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014041974 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0154      |\n",
      "|    n_updates            | 234         |\n",
      "|    policy_gradient_loss | -0.000996   |\n",
      "|    value_loss           | 0.603       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=248.60 +/- 7.20\n",
      "Episode length: 248.60 +/- 7.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 249      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=240.20 +/- 6.14\n",
      "Episode length: 240.20 +/- 6.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 240      |\n",
      "|    mean_reward     | 240      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=256.60 +/- 20.19\n",
      "Episode length: 256.60 +/- 20.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 257      |\n",
      "|    mean_reward     | 257      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 527   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 48114 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=260.60 +/- 20.17\n",
      "Episode length: 260.60 +/- 20.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 261         |\n",
      "|    mean_reward          | 261         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020612115 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.12        |\n",
      "|    n_updates            | 243         |\n",
      "|    policy_gradient_loss | 0.00191     |\n",
      "|    value_loss           | 0.294       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=260.80 +/- 26.72\n",
      "Episode length: 260.80 +/- 26.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 261      |\n",
      "|    mean_reward     | 261      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=259.80 +/- 10.17\n",
      "Episode length: 259.80 +/- 10.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 260      |\n",
      "|    mean_reward     | 260      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 96    |\n",
      "|    total_timesteps | 49896 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=231.20 +/- 8.28\n",
      "Episode length: 231.20 +/- 8.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 231         |\n",
      "|    mean_reward          | 231         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020537337 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0249      |\n",
      "|    n_updates            | 252         |\n",
      "|    policy_gradient_loss | 0.00155     |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=228.60 +/- 16.51\n",
      "Episode length: 228.60 +/- 16.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=233.00 +/- 5.44\n",
      "Episode length: 233.00 +/- 5.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 233      |\n",
      "|    mean_reward     | 233      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=232.40 +/- 6.83\n",
      "Episode length: 232.40 +/- 6.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 232      |\n",
      "|    mean_reward     | 232      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 507   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 101   |\n",
      "|    total_timesteps | 51678 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=252.40 +/- 7.39\n",
      "Episode length: 252.40 +/- 7.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 252         |\n",
      "|    mean_reward          | 252         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013001425 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0412      |\n",
      "|    n_updates            | 261         |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    value_loss           | 0.0811      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=254.80 +/- 11.02\n",
      "Episode length: 254.80 +/- 11.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 255      |\n",
      "|    mean_reward     | 255      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=258.20 +/- 8.16\n",
      "Episode length: 258.20 +/- 8.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 258      |\n",
      "|    mean_reward     | 258      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 106   |\n",
      "|    total_timesteps | 53460 |\n",
      "------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=275.00 +/- 9.53\n",
      "Episode length: 275.00 +/- 9.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 275         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 53500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009154857 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.00249     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.000626    |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=281.80 +/- 19.54\n",
      "Episode length: 281.80 +/- 19.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 282      |\n",
      "|    mean_reward     | 282      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=275.00 +/- 7.04\n",
      "Episode length: 275.00 +/- 7.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 275      |\n",
      "|    mean_reward     | 275      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=269.40 +/- 9.71\n",
      "Episode length: 269.40 +/- 9.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 269      |\n",
      "|    mean_reward     | 269      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 467   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 118   |\n",
      "|    total_timesteps | 55242 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=273.80 +/- 31.56\n",
      "Episode length: 273.80 +/- 31.56\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 274         |\n",
      "|    mean_reward          | 274         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 55500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030306397 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 279         |\n",
      "|    policy_gradient_loss | 0.00449     |\n",
      "|    value_loss           | 0.438       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=281.20 +/- 22.82\n",
      "Episode length: 281.20 +/- 22.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 281      |\n",
      "|    mean_reward     | 281      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=291.60 +/- 32.94\n",
      "Episode length: 291.60 +/- 32.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 292      |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=268.00 +/- 13.86\n",
      "Episode length: 268.00 +/- 13.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 268      |\n",
      "|    mean_reward     | 268      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 437   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 130   |\n",
      "|    total_timesteps | 57024 |\n",
      "------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=377.80 +/- 18.05\n",
      "Episode length: 377.80 +/- 18.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 378         |\n",
      "|    mean_reward          | 378         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 57500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020662887 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0247      |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | 0.00752     |\n",
      "|    value_loss           | 0.327       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=365.60 +/- 17.76\n",
      "Episode length: 365.60 +/- 17.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 366      |\n",
      "|    mean_reward     | 366      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=370.60 +/- 9.60\n",
      "Episode length: 370.60 +/- 9.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 371      |\n",
      "|    mean_reward     | 371      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 432   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 135   |\n",
      "|    total_timesteps | 58806 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=370.40 +/- 27.10\n",
      "Episode length: 370.40 +/- 27.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 370         |\n",
      "|    mean_reward          | 370         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 59000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015065649 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 297         |\n",
      "|    policy_gradient_loss | 0.00637     |\n",
      "|    value_loss           | 0.184       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=373.80 +/- 20.90\n",
      "Episode length: 373.80 +/- 20.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=374.20 +/- 10.93\n",
      "Episode length: 374.20 +/- 10.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 374      |\n",
      "|    mean_reward     | 374      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=357.60 +/- 40.80\n",
      "Episode length: 357.60 +/- 40.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 358      |\n",
      "|    mean_reward     | 358      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 419   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 144   |\n",
      "|    total_timesteps | 60588 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=462.80 +/- 31.08\n",
      "Episode length: 462.80 +/- 31.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 463         |\n",
      "|    mean_reward          | 463         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 61000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014054145 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.00507     |\n",
      "|    n_updates            | 306         |\n",
      "|    policy_gradient_loss | 0.00394     |\n",
      "|    value_loss           | 0.0077      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=394.60 +/- 171.07\n",
      "Episode length: 394.60 +/- 171.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 395      |\n",
      "|    mean_reward     | 395      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=486.20 +/- 16.90\n",
      "Episode length: 486.20 +/- 16.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 486      |\n",
      "|    mean_reward     | 486      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 406   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 153   |\n",
      "|    total_timesteps | 62370 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=22.80 +/- 10.65\n",
      "Episode length: 22.80 +/- 10.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 22.8        |\n",
      "|    mean_reward          | 22.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019967642 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.077       |\n",
      "|    n_updates            | 315         |\n",
      "|    policy_gradient_loss | 0.00967     |\n",
      "|    value_loss           | 0.801       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=19.20 +/- 4.07\n",
      "Episode length: 19.20 +/- 4.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19.2     |\n",
      "|    mean_reward     | 19.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=19.00 +/- 7.59\n",
      "Episode length: 19.00 +/- 7.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 19       |\n",
      "|    mean_reward     | 19       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=21.60 +/- 7.58\n",
      "Episode length: 21.60 +/- 7.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 21.6     |\n",
      "|    mean_reward     | 21.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 411   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 155   |\n",
      "|    total_timesteps | 64152 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=26.20 +/- 10.57\n",
      "Episode length: 26.20 +/- 10.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 26.2        |\n",
      "|    mean_reward          | 26.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011650401 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.487       |\n",
      "|    n_updates            | 324         |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=33.00 +/- 12.20\n",
      "Episode length: 33.00 +/- 12.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33       |\n",
      "|    mean_reward     | 33       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=54.20 +/- 64.65\n",
      "Episode length: 54.20 +/- 64.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 54.2     |\n",
      "|    mean_reward     | 54.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 415   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 158   |\n",
      "|    total_timesteps | 65934 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=135.40 +/- 94.89\n",
      "Episode length: 135.40 +/- 94.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 135         |\n",
      "|    mean_reward          | 135         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015661705 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.225       |\n",
      "|    n_updates            | 333         |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 1.56        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=144.20 +/- 91.64\n",
      "Episode length: 144.20 +/- 91.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 144      |\n",
      "|    mean_reward     | 144      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=199.60 +/- 72.24\n",
      "Episode length: 199.60 +/- 72.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=223.00 +/- 9.80\n",
      "Episode length: 223.00 +/- 9.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 223      |\n",
      "|    mean_reward     | 223      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 409   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 165   |\n",
      "|    total_timesteps | 67716 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=186.00 +/- 3.16\n",
      "Episode length: 186.00 +/- 3.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 186         |\n",
      "|    mean_reward          | 186         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020108473 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.817       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0892      |\n",
      "|    n_updates            | 342         |\n",
      "|    policy_gradient_loss | 0.0068      |\n",
      "|    value_loss           | 0.535       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=177.60 +/- 4.50\n",
      "Episode length: 177.60 +/- 4.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 178      |\n",
      "|    mean_reward     | 178      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=200.60 +/- 22.37\n",
      "Episode length: 200.60 +/- 22.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 406   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 171   |\n",
      "|    total_timesteps | 69498 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=162.20 +/- 8.38\n",
      "Episode length: 162.20 +/- 8.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 162         |\n",
      "|    mean_reward          | 162         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 69500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029825088 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.093       |\n",
      "|    n_updates            | 351         |\n",
      "|    policy_gradient_loss | 0.00681     |\n",
      "|    value_loss           | 0.399       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=175.80 +/- 9.74\n",
      "Episode length: 175.80 +/- 9.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 176      |\n",
      "|    mean_reward     | 176      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=174.40 +/- 7.00\n",
      "Episode length: 174.40 +/- 7.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 174      |\n",
      "|    mean_reward     | 174      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=167.40 +/- 7.31\n",
      "Episode length: 167.40 +/- 7.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 167      |\n",
      "|    mean_reward     | 167      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 405   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 175   |\n",
      "|    total_timesteps | 71280 |\n",
      "------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=205.60 +/- 4.63\n",
      "Episode length: 205.60 +/- 4.63\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 206        |\n",
      "|    mean_reward          | 206        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 71500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02048626 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.168      |\n",
      "|    entropy_loss         | -0.404     |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.00732    |\n",
      "|    loss                 | 0.188      |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.00467   |\n",
      "|    value_loss           | 0.457      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=204.00 +/- 5.76\n",
      "Episode length: 204.00 +/- 5.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 204      |\n",
      "|    mean_reward     | 204      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=225.60 +/- 12.56\n",
      "Episode length: 225.60 +/- 12.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 226      |\n",
      "|    mean_reward     | 226      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=207.20 +/- 12.38\n",
      "Episode length: 207.20 +/- 12.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | 207      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 406   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 179   |\n",
      "|    total_timesteps | 73062 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=239.80 +/- 12.04\n",
      "Episode length: 239.80 +/- 12.04\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 240         |\n",
      "|    mean_reward          | 240         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 73500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015663186 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.086       |\n",
      "|    n_updates            | 369         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 0.608       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=242.80 +/- 5.42\n",
      "Episode length: 242.80 +/- 5.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 243      |\n",
      "|    mean_reward     | 243      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=232.40 +/- 6.89\n",
      "Episode length: 232.40 +/- 6.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 232      |\n",
      "|    mean_reward     | 232      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 409   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 182   |\n",
      "|    total_timesteps | 74844 |\n",
      "------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=197.60 +/- 5.82\n",
      "Episode length: 197.60 +/- 5.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 198         |\n",
      "|    mean_reward          | 198         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 75000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034262624 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.385      |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.204       |\n",
      "|    n_updates            | 378         |\n",
      "|    policy_gradient_loss | 0.00787     |\n",
      "|    value_loss           | 0.402       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=203.00 +/- 7.46\n",
      "Episode length: 203.00 +/- 7.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 203      |\n",
      "|    mean_reward     | 203      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=200.20 +/- 2.40\n",
      "Episode length: 200.20 +/- 2.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=200.00 +/- 5.10\n",
      "Episode length: 200.00 +/- 5.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 200      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 411   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 186   |\n",
      "|    total_timesteps | 76626 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=228.80 +/- 11.37\n",
      "Episode length: 228.80 +/- 11.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 229         |\n",
      "|    mean_reward          | 229         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 77000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018041873 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0917      |\n",
      "|    n_updates            | 387         |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 0.203       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=243.80 +/- 4.92\n",
      "Episode length: 243.80 +/- 4.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 244      |\n",
      "|    mean_reward     | 244      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=231.40 +/- 10.65\n",
      "Episode length: 231.40 +/- 10.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 231      |\n",
      "|    mean_reward     | 231      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 413   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 189   |\n",
      "|    total_timesteps | 78408 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=259.80 +/- 12.51\n",
      "Episode length: 259.80 +/- 12.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 260         |\n",
      "|    mean_reward          | 260         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 78500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011810026 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.402      |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0406      |\n",
      "|    n_updates            | 396         |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=320.80 +/- 92.37\n",
      "Episode length: 320.80 +/- 92.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 321      |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=417.80 +/- 103.88\n",
      "Episode length: 417.80 +/- 103.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 418      |\n",
      "|    mean_reward     | 418      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=411.20 +/- 108.94\n",
      "Episode length: 411.20 +/- 108.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 411      |\n",
      "|    mean_reward     | 411      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 413   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 193   |\n",
      "|    total_timesteps | 80190 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=450.20 +/- 99.60\n",
      "Episode length: 450.20 +/- 99.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 450          |\n",
      "|    mean_reward          | 450          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070663034 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.168        |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.00732      |\n",
      "|    loss                 | 0.0234       |\n",
      "|    n_updates            | 405          |\n",
      "|    policy_gradient_loss | 0.00474      |\n",
      "|    value_loss           | 0.206        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=405.80 +/- 115.73\n",
      "Episode length: 405.80 +/- 115.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 406      |\n",
      "|    mean_reward     | 406      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 81500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 408   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 200   |\n",
      "|    total_timesteps | 81972 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016564123 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.168       |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00732     |\n",
      "|    loss                 | 0.0218      |\n",
      "|    n_updates            | 414         |\n",
      "|    policy_gradient_loss | -0.000543   |\n",
      "|    value_loss           | 0.168       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=454.40 +/- 91.20\n",
      "Episode length: 454.40 +/- 91.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 454      |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=411.40 +/- 108.53\n",
      "Episode length: 411.40 +/- 108.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 411      |\n",
      "|    mean_reward     | 411      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 83500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 403   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 207   |\n",
      "|    total_timesteps | 83754 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 84000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01983568 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.168      |\n",
      "|    entropy_loss         | -0.4       |\n",
      "|    explained_variance   | 0.916      |\n",
      "|    learning_rate        | 0.00732    |\n",
      "|    loss                 | 0.149      |\n",
      "|    n_updates            | 423        |\n",
      "|    policy_gradient_loss | 0.0055     |\n",
      "|    value_loss           | 0.486      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=457.40 +/- 85.20\n",
      "Episode length: 457.40 +/- 85.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 457      |\n",
      "|    mean_reward     | 457      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 400   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 213   |\n",
      "|    total_timesteps | 85536 |\n",
      "------------------------------\n",
      "Single environment training took 214.30 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>253.3</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clean-sweep-17</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/3nsyighl' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/3nsyighl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_135325-3nsyighl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7oqe6lm9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2924289164270875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.006625633589261951\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9817254868830296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9217850917543364\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009857731630951496\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 1.053873457159194\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 79389\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_135714-7oqe6lm9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/7oqe6lm9' target=\"_blank\">lilac-sweep-18</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/7oqe6lm9' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/7oqe6lm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1376`, after every 21 untruncated mini-batches, there will be a truncated mini-batch of size 32\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1376 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=11.80 +/- 2.32\n",
      "Episode length: 11.80 +/- 2.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=9.80 +/- 2.64\n",
      "Episode length: 9.80 +/- 2.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 9.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 665  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 1376 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=219.40 +/- 77.16\n",
      "Episode length: 219.40 +/- 77.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 219         |\n",
      "|    mean_reward          | 219         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015136882 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | 0.000914    |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 2.56        |\n",
      "|    n_updates            | 1           |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 19.8        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=217.20 +/- 76.26\n",
      "Episode length: 217.20 +/- 76.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 217      |\n",
      "|    mean_reward     | 217      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=229.00 +/- 87.42\n",
      "Episode length: 229.00 +/- 87.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 229      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 505  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2752 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=3000, episode_reward=60.00 +/- 13.74\n",
      "Episode length: 60.00 +/- 13.74\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 60         |\n",
      "|    mean_reward          | 60         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02517584 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.656     |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 1.89       |\n",
      "|    n_updates            | 2          |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    value_loss           | 6.86       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=65.20 +/- 16.28\n",
      "Episode length: 65.20 +/- 16.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 65.2     |\n",
      "|    mean_reward     | 65.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=56.40 +/- 7.96\n",
      "Episode length: 56.40 +/- 7.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 56.4     |\n",
      "|    mean_reward     | 56.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 578  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 4128 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4500, episode_reward=367.80 +/- 161.98\n",
      "Episode length: 367.80 +/- 161.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 368         |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039028708 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 6.6         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=228.00 +/- 142.59\n",
      "Episode length: 228.00 +/- 142.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | 228      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=189.20 +/- 68.85\n",
      "Episode length: 189.20 +/- 68.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 189      |\n",
      "|    mean_reward     | 189      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 545  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 5504 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=288.00 +/- 139.50\n",
      "Episode length: 288.00 +/- 139.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 288         |\n",
      "|    mean_reward          | 288         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041738655 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.455       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.008      |\n",
      "|    value_loss           | 2.54        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=201.40 +/- 93.65\n",
      "Episode length: 201.40 +/- 93.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 524  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 6880 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7000, episode_reward=331.60 +/- 168.58\n",
      "Episode length: 331.60 +/- 168.58\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 332        |\n",
      "|    mean_reward          | 332        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03419582 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.418     |\n",
      "|    explained_variance   | 0.772      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.0572     |\n",
      "|    n_updates            | 5          |\n",
      "|    policy_gradient_loss | -0.00778   |\n",
      "|    value_loss           | 1          |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=316.60 +/- 174.46\n",
      "Episode length: 316.60 +/- 174.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 317      |\n",
      "|    mean_reward     | 317      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=276.20 +/- 126.89\n",
      "Episode length: 276.20 +/- 126.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 276      |\n",
      "|    mean_reward     | 276      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 407  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 8256 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=68.20 +/- 8.38\n",
      "Episode length: 68.20 +/- 8.38\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 68.2      |\n",
      "|    mean_reward          | 68.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 8500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0775957 |\n",
      "|    clip_fraction        | 0.255     |\n",
      "|    clip_range           | 0.292     |\n",
      "|    entropy_loss         | -0.432    |\n",
      "|    explained_variance   | 0.607     |\n",
      "|    learning_rate        | 0.00986   |\n",
      "|    loss                 | 0.227     |\n",
      "|    n_updates            | 6         |\n",
      "|    policy_gradient_loss | 0.0146    |\n",
      "|    value_loss           | 1.2       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=81.60 +/- 22.28\n",
      "Episode length: 81.60 +/- 22.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 81.6     |\n",
      "|    mean_reward     | 81.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=69.80 +/- 15.10\n",
      "Episode length: 69.80 +/- 15.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 69.8     |\n",
      "|    mean_reward     | 69.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 428  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 22   |\n",
      "|    total_timesteps | 9632 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=182.00 +/- 93.99\n",
      "Episode length: 182.00 +/- 93.99\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 182         |\n",
      "|    mean_reward          | 182         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014460641 |\n",
      "|    clip_fraction        | 0.0795      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.438      |\n",
      "|    explained_variance   | 0.382       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.624       |\n",
      "|    n_updates            | 7           |\n",
      "|    policy_gradient_loss | 0.00257     |\n",
      "|    value_loss           | 2.71        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=92.60 +/- 12.91\n",
      "Episode length: 92.60 +/- 12.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 92.6     |\n",
      "|    mean_reward     | 92.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=83.00 +/- 6.90\n",
      "Episode length: 83.00 +/- 6.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 83       |\n",
      "|    mean_reward     | 83       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 413   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 11008 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=85.00 +/- 4.82\n",
      "Episode length: 85.00 +/- 4.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 85          |\n",
      "|    mean_reward          | 85          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019139677 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.701       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.341       |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    value_loss           | 1.54        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=111.60 +/- 34.67\n",
      "Episode length: 111.60 +/- 34.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 112      |\n",
      "|    mean_reward     | 112      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 427   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 12384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=91.80 +/- 16.62\n",
      "Episode length: 91.80 +/- 16.62\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 91.8        |\n",
      "|    mean_reward          | 91.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022602422 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.403      |\n",
      "|    explained_variance   | 0.716       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.906       |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | 0.00616     |\n",
      "|    value_loss           | 1.74        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=88.60 +/- 14.16\n",
      "Episode length: 88.60 +/- 14.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 88.6     |\n",
      "|    mean_reward     | 88.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=83.20 +/- 12.42\n",
      "Episode length: 83.20 +/- 12.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 83.2     |\n",
      "|    mean_reward     | 83.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 446   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 13760 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=100.40 +/- 9.29\n",
      "Episode length: 100.40 +/- 9.29\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 100        |\n",
      "|    mean_reward          | 100        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03760079 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.487     |\n",
      "|    explained_variance   | 0.582      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.632      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    value_loss           | 2.88       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=101.20 +/- 8.08\n",
      "Episode length: 101.20 +/- 8.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 101      |\n",
      "|    mean_reward     | 101      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=102.60 +/- 16.95\n",
      "Episode length: 102.60 +/- 16.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 103      |\n",
      "|    mean_reward     | 103      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 468   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 15136 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=189.40 +/- 17.78\n",
      "Episode length: 189.40 +/- 17.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 189         |\n",
      "|    mean_reward          | 189         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043167286 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.873       |\n",
      "|    n_updates            | 11          |\n",
      "|    policy_gradient_loss | -0.00797    |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=179.20 +/- 9.56\n",
      "Episode length: 179.20 +/- 9.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 179      |\n",
      "|    mean_reward     | 179      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=161.20 +/- 7.30\n",
      "Episode length: 161.20 +/- 7.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 161      |\n",
      "|    mean_reward     | 161      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 16512 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=123.40 +/- 17.21\n",
      "Episode length: 123.40 +/- 17.21\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 123        |\n",
      "|    mean_reward          | 123        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06814353 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.424     |\n",
      "|    explained_variance   | 0.429      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.374      |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | 0.0101     |\n",
      "|    value_loss           | 2.25       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=135.00 +/- 11.40\n",
      "Episode length: 135.00 +/- 11.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 135      |\n",
      "|    mean_reward     | 135      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 17888 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=132.20 +/- 11.51\n",
      "Episode length: 132.20 +/- 11.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 132         |\n",
      "|    mean_reward          | 132         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035067044 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.302       |\n",
      "|    n_updates            | 13          |\n",
      "|    policy_gradient_loss | 0.0347      |\n",
      "|    value_loss           | 0.947       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=135.00 +/- 12.82\n",
      "Episode length: 135.00 +/- 12.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 135      |\n",
      "|    mean_reward     | 135      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=136.20 +/- 18.17\n",
      "Episode length: 136.20 +/- 18.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 136      |\n",
      "|    mean_reward     | 136      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 492   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 19264 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=139.20 +/- 17.16\n",
      "Episode length: 139.20 +/- 17.16\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 139        |\n",
      "|    mean_reward          | 139        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12929036 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.382     |\n",
      "|    explained_variance   | 0.937      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.0311     |\n",
      "|    n_updates            | 14         |\n",
      "|    policy_gradient_loss | 0.0414     |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=133.20 +/- 10.65\n",
      "Episode length: 133.20 +/- 10.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 133      |\n",
      "|    mean_reward     | 133      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=149.20 +/- 15.10\n",
      "Episode length: 149.20 +/- 15.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 149      |\n",
      "|    mean_reward     | 149      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 505   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 20640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=123.00 +/- 6.66\n",
      "Episode length: 123.00 +/- 6.66\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 123        |\n",
      "|    mean_reward          | 123        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 21000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03400528 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.334     |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.0227     |\n",
      "|    n_updates            | 15         |\n",
      "|    policy_gradient_loss | 0.0137     |\n",
      "|    value_loss           | 0.238      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=127.00 +/- 8.05\n",
      "Episode length: 127.00 +/- 8.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 127      |\n",
      "|    mean_reward     | 127      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=135.20 +/- 11.05\n",
      "Episode length: 135.20 +/- 11.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 135      |\n",
      "|    mean_reward     | 135      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 514   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 22016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=121.60 +/- 12.80\n",
      "Episode length: 121.60 +/- 12.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 122         |\n",
      "|    mean_reward          | 122         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054299966 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.167       |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 0.0243      |\n",
      "|    value_loss           | 0.207       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=117.40 +/- 5.68\n",
      "Episode length: 117.40 +/- 5.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 117      |\n",
      "|    mean_reward     | 117      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 526   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 23392 |\n",
      "------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=175.60 +/- 29.53\n",
      "Episode length: 175.60 +/- 29.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 176        |\n",
      "|    mean_reward          | 176        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05292804 |\n",
      "|    clip_fraction        | 0.224      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.00767    |\n",
      "|    n_updates            | 17         |\n",
      "|    policy_gradient_loss | 0.0112     |\n",
      "|    value_loss           | 0.05       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=159.60 +/- 9.71\n",
      "Episode length: 159.60 +/- 9.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 160      |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=173.40 +/- 22.28\n",
      "Episode length: 173.40 +/- 22.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 173      |\n",
      "|    mean_reward     | 173      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 530   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 24768 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=179.60 +/- 9.22\n",
      "Episode length: 179.60 +/- 9.22\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 180        |\n",
      "|    mean_reward          | 180        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07998151 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.354     |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.0265     |\n",
      "|    n_updates            | 18         |\n",
      "|    policy_gradient_loss | 0.00856    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=171.00 +/- 12.52\n",
      "Episode length: 171.00 +/- 12.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 171      |\n",
      "|    mean_reward     | 171      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=190.40 +/- 28.40\n",
      "Episode length: 190.40 +/- 28.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | 190      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 521   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 26144 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=135.40 +/- 2.65\n",
      "Episode length: 135.40 +/- 2.65\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 135       |\n",
      "|    mean_reward          | 135       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 26500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0628251 |\n",
      "|    clip_fraction        | 0.213     |\n",
      "|    clip_range           | 0.292     |\n",
      "|    entropy_loss         | -0.336    |\n",
      "|    explained_variance   | 0.969     |\n",
      "|    learning_rate        | 0.00986   |\n",
      "|    loss                 | 0.0217    |\n",
      "|    n_updates            | 19        |\n",
      "|    policy_gradient_loss | 0.0215    |\n",
      "|    value_loss           | 0.0831    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=148.80 +/- 9.87\n",
      "Episode length: 148.80 +/- 9.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 149      |\n",
      "|    mean_reward     | 149      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=159.60 +/- 18.87\n",
      "Episode length: 159.60 +/- 18.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 160      |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 53    |\n",
      "|    total_timesteps | 27520 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=190.80 +/- 16.51\n",
      "Episode length: 190.80 +/- 16.51\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 191        |\n",
      "|    mean_reward          | 191        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09151825 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.45      |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | -0.0124    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.0188     |\n",
      "|    value_loss           | 0.0598     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=171.60 +/- 4.22\n",
      "Episode length: 171.60 +/- 4.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 172      |\n",
      "|    mean_reward     | 172      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 517   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 28896 |\n",
      "------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=314.80 +/- 96.29\n",
      "Episode length: 314.80 +/- 96.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 315         |\n",
      "|    mean_reward          | 315         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056805167 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.194       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | -0.00796    |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=338.40 +/- 128.83\n",
      "Episode length: 338.40 +/- 128.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 338      |\n",
      "|    mean_reward     | 338      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=321.00 +/- 106.33\n",
      "Episode length: 321.00 +/- 106.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 321      |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 492   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 30272 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=279.80 +/- 75.09\n",
      "Episode length: 279.80 +/- 75.09\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 280        |\n",
      "|    mean_reward          | 280        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01465551 |\n",
      "|    clip_fraction        | 0.093      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.494     |\n",
      "|    explained_variance   | 0.662      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.555      |\n",
      "|    n_updates            | 22         |\n",
      "|    policy_gradient_loss | 0.0146     |\n",
      "|    value_loss           | 0.622      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=179.60 +/- 16.48\n",
      "Episode length: 179.60 +/- 16.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 180      |\n",
      "|    mean_reward     | 180      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=200.60 +/- 26.66\n",
      "Episode length: 200.60 +/- 26.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 487   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 31648 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=162.60 +/- 13.81\n",
      "Episode length: 162.60 +/- 13.81\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 163         |\n",
      "|    mean_reward          | 163         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017240155 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.0545      |\n",
      "|    n_updates            | 23          |\n",
      "|    policy_gradient_loss | 0.00538     |\n",
      "|    value_loss           | 0.176       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=154.20 +/- 11.00\n",
      "Episode length: 154.20 +/- 11.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 154      |\n",
      "|    mean_reward     | 154      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=173.60 +/- 15.15\n",
      "Episode length: 173.60 +/- 15.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 174      |\n",
      "|    mean_reward     | 174      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 33024 |\n",
      "------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=158.20 +/- 9.54\n",
      "Episode length: 158.20 +/- 9.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 158        |\n",
      "|    mean_reward          | 158        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 33500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02151125 |\n",
      "|    clip_fraction        | 0.0973     |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.256     |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.1        |\n",
      "|    n_updates            | 24         |\n",
      "|    policy_gradient_loss | 0.0153     |\n",
      "|    value_loss           | 0.0225     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=215.40 +/- 107.17\n",
      "Episode length: 215.40 +/- 107.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 215      |\n",
      "|    mean_reward     | 215      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 486   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 34400 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=230.40 +/- 91.94\n",
      "Episode length: 230.40 +/- 91.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 230         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056564733 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.00722     |\n",
      "|    n_updates            | 25          |\n",
      "|    policy_gradient_loss | 0.0267      |\n",
      "|    value_loss           | 0.0182      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=189.00 +/- 19.89\n",
      "Episode length: 189.00 +/- 19.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 189      |\n",
      "|    mean_reward     | 189      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=218.60 +/- 92.07\n",
      "Episode length: 218.60 +/- 92.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 219      |\n",
      "|    mean_reward     | 219      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 490   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 35776 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=312.80 +/- 103.54\n",
      "Episode length: 312.80 +/- 103.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 313        |\n",
      "|    mean_reward          | 313        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05381531 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.344     |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.0423     |\n",
      "|    n_updates            | 26         |\n",
      "|    policy_gradient_loss | 0.0141     |\n",
      "|    value_loss           | 0.0136     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=372.20 +/- 81.20\n",
      "Episode length: 372.20 +/- 81.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 372      |\n",
      "|    mean_reward     | 372      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=37000, episode_reward=414.60 +/- 106.98\n",
      "Episode length: 414.60 +/- 106.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 415      |\n",
      "|    mean_reward     | 415      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 37000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 490   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 37152 |\n",
      "------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=444.00 +/- 112.00\n",
      "Episode length: 444.00 +/- 112.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 444         |\n",
      "|    mean_reward          | 444         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024099672 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | 0.00424     |\n",
      "|    value_loss           | 0.609       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=448.20 +/- 81.74\n",
      "Episode length: 448.20 +/- 81.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 448      |\n",
      "|    mean_reward     | 448      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38500, episode_reward=397.00 +/- 129.68\n",
      "Episode length: 397.00 +/- 129.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 397      |\n",
      "|    mean_reward     | 397      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 488   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 38528 |\n",
      "------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=377.80 +/- 149.87\n",
      "Episode length: 377.80 +/- 149.87\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 378        |\n",
      "|    mean_reward          | 378        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 39000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02063202 |\n",
      "|    clip_fraction        | 0.098      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.473     |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.353      |\n",
      "|    n_updates            | 28         |\n",
      "|    policy_gradient_loss | 0.0207     |\n",
      "|    value_loss           | 0.42       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=188.40 +/- 28.15\n",
      "Episode length: 188.40 +/- 28.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 188      |\n",
      "|    mean_reward     | 188      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 39500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 494   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 39904 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=229.40 +/- 81.39\n",
      "Episode length: 229.40 +/- 81.39\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 229        |\n",
      "|    mean_reward          | 229        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14543371 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.32      |\n",
      "|    explained_variance   | 0.569      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.688      |\n",
      "|    n_updates            | 29         |\n",
      "|    policy_gradient_loss | 0.00807    |\n",
      "|    value_loss           | 2.39       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=190.60 +/- 55.15\n",
      "Episode length: 190.60 +/- 55.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 191      |\n",
      "|    mean_reward     | 191      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=268.60 +/- 125.37\n",
      "Episode length: 268.60 +/- 125.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 269      |\n",
      "|    mean_reward     | 269      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 41000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 497   |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 83    |\n",
      "|    total_timesteps | 41280 |\n",
      "------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=298.20 +/- 155.33\n",
      "Episode length: 298.20 +/- 155.33\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 298          |\n",
      "|    mean_reward          | 298          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 41500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051677204 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.292        |\n",
      "|    entropy_loss         | -0.376       |\n",
      "|    explained_variance   | 0.535        |\n",
      "|    learning_rate        | 0.00986      |\n",
      "|    loss                 | 0.135        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 0.945        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=273.20 +/- 117.45\n",
      "Episode length: 273.20 +/- 117.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 273      |\n",
      "|    mean_reward     | 273      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=343.00 +/- 139.79\n",
      "Episode length: 343.00 +/- 139.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 499   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 85    |\n",
      "|    total_timesteps | 42656 |\n",
      "------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=445.20 +/- 67.19\n",
      "Episode length: 445.20 +/- 67.19\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 445        |\n",
      "|    mean_reward          | 445        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 43000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11871483 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.225     |\n",
      "|    explained_variance   | 0.798      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.017      |\n",
      "|    n_updates            | 31         |\n",
      "|    policy_gradient_loss | 0.0184     |\n",
      "|    value_loss           | 0.476      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=444.00 +/- 112.00\n",
      "Episode length: 444.00 +/- 112.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 444      |\n",
      "|    mean_reward     | 444      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 43500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=349.80 +/- 127.93\n",
      "Episode length: 349.80 +/- 127.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 350      |\n",
      "|    mean_reward     | 350      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 490   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 89    |\n",
      "|    total_timesteps | 44032 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=344.00 +/- 128.36\n",
      "Episode length: 344.00 +/- 128.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 344         |\n",
      "|    mean_reward          | 344         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012839246 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.299       |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | 0.00475     |\n",
      "|    value_loss           | 0.425       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=325.60 +/- 51.93\n",
      "Episode length: 325.60 +/- 51.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 326      |\n",
      "|    mean_reward     | 326      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 491   |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 92    |\n",
      "|    total_timesteps | 45408 |\n",
      "------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=315.80 +/- 56.53\n",
      "Episode length: 315.80 +/- 56.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 316         |\n",
      "|    mean_reward          | 316         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 45500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020881964 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.164      |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.0334      |\n",
      "|    n_updates            | 33          |\n",
      "|    policy_gradient_loss | 0.0121      |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=297.80 +/- 110.76\n",
      "Episode length: 297.80 +/- 110.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 298      |\n",
      "|    mean_reward     | 298      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=259.20 +/- 68.19\n",
      "Episode length: 259.20 +/- 68.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 259      |\n",
      "|    mean_reward     | 259      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 486   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 96    |\n",
      "|    total_timesteps | 46784 |\n",
      "------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=276.40 +/- 107.35\n",
      "Episode length: 276.40 +/- 107.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 276         |\n",
      "|    mean_reward          | 276         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 47000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022951085 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.211      |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.0939      |\n",
      "|    n_updates            | 34          |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    value_loss           | 0.485       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=225.20 +/- 53.34\n",
      "Episode length: 225.20 +/- 53.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 225      |\n",
      "|    mean_reward     | 225      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 47500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=265.80 +/- 110.11\n",
      "Episode length: 265.80 +/- 110.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 266      |\n",
      "|    mean_reward     | 266      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 100   |\n",
      "|    total_timesteps | 48160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=247.20 +/- 127.89\n",
      "Episode length: 247.20 +/- 127.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 247         |\n",
      "|    mean_reward          | 247         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008905213 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.126       |\n",
      "|    n_updates            | 35          |\n",
      "|    policy_gradient_loss | 0.00987     |\n",
      "|    value_loss           | 0.317       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=188.20 +/- 12.06\n",
      "Episode length: 188.20 +/- 12.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 188      |\n",
      "|    mean_reward     | 188      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=194.80 +/- 21.55\n",
      "Episode length: 194.80 +/- 21.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 195      |\n",
      "|    mean_reward     | 195      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 49500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 49536 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=371.00 +/- 109.39\n",
      "Episode length: 371.00 +/- 109.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 371         |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027871175 |\n",
      "|    clip_fraction        | 0.0831      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.216      |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.212       |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.032       |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=299.60 +/- 108.19\n",
      "Episode length: 299.60 +/- 108.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 300      |\n",
      "|    mean_reward     | 300      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 477   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 106   |\n",
      "|    total_timesteps | 50912 |\n",
      "------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=413.80 +/- 108.64\n",
      "Episode length: 413.80 +/- 108.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 414         |\n",
      "|    mean_reward          | 414         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 51000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011669699 |\n",
      "|    clip_fraction        | 0.0661      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 37          |\n",
      "|    policy_gradient_loss | 0.00246     |\n",
      "|    value_loss           | 0.174       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=385.20 +/- 101.38\n",
      "Episode length: 385.20 +/- 101.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 385      |\n",
      "|    mean_reward     | 385      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 51500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=464.80 +/- 55.39\n",
      "Episode length: 464.80 +/- 55.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 465      |\n",
      "|    mean_reward     | 465      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 112   |\n",
      "|    total_timesteps | 52288 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=286.20 +/- 62.71\n",
      "Episode length: 286.20 +/- 62.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 286        |\n",
      "|    mean_reward          | 286        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 52500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11614411 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.275     |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.0343     |\n",
      "|    n_updates            | 38         |\n",
      "|    policy_gradient_loss | 0.0512     |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=423.20 +/- 90.49\n",
      "Episode length: 423.20 +/- 90.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 423      |\n",
      "|    mean_reward     | 423      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=301.80 +/- 117.17\n",
      "Episode length: 301.80 +/- 117.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 302      |\n",
      "|    mean_reward     | 302      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 53500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 460   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 116   |\n",
      "|    total_timesteps | 53664 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=486.20 +/- 17.55\n",
      "Episode length: 486.20 +/- 17.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 486        |\n",
      "|    mean_reward          | 486        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 54000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03435161 |\n",
      "|    clip_fraction        | 0.0945     |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.2       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.208      |\n",
      "|    n_updates            | 39         |\n",
      "|    policy_gradient_loss | 0.00628    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54500, episode_reward=343.40 +/- 86.51\n",
      "Episode length: 343.40 +/- 86.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=374.80 +/- 103.86\n",
      "Episode length: 374.80 +/- 103.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 375      |\n",
      "|    mean_reward     | 375      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 459   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 119   |\n",
      "|    total_timesteps | 55040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=397.60 +/- 64.76\n",
      "Episode length: 397.60 +/- 64.76\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 398       |\n",
      "|    mean_reward          | 398       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 55500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2681568 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.292     |\n",
      "|    entropy_loss         | -0.239    |\n",
      "|    explained_variance   | 0.956     |\n",
      "|    learning_rate        | 0.00986   |\n",
      "|    loss                 | 5.56      |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | 0.344     |\n",
      "|    value_loss           | 0.0648    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=427.40 +/- 56.69\n",
      "Episode length: 427.40 +/- 56.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 427      |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 461   |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 122   |\n",
      "|    total_timesteps | 56416 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=9.80 +/- 0.40\n",
      "Episode length: 9.80 +/- 0.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.8        |\n",
      "|    mean_reward          | 9.8        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 56500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07259312 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.368     |\n",
      "|    explained_variance   | 0.508      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.287      |\n",
      "|    n_updates            | 41         |\n",
      "|    policy_gradient_loss | 0.034      |\n",
      "|    value_loss           | 0.47       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=8.80 +/- 0.75\n",
      "Episode length: 8.80 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 8.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=9.40 +/- 0.49\n",
      "Episode length: 9.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 57500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 468   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 123   |\n",
      "|    total_timesteps | 57792 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=9.60 +/- 1.02\n",
      "Episode length: 9.60 +/- 1.02\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 9.6      |\n",
      "|    mean_reward          | 9.6      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 58000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.181407 |\n",
      "|    clip_fraction        | 0.575    |\n",
      "|    clip_range           | 0.292    |\n",
      "|    entropy_loss         | -0.408   |\n",
      "|    explained_variance   | 0.01     |\n",
      "|    learning_rate        | 0.00986  |\n",
      "|    loss                 | 6.11     |\n",
      "|    n_updates            | 42       |\n",
      "|    policy_gradient_loss | 0.0889   |\n",
      "|    value_loss           | 21       |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=9.00 +/- 0.89\n",
      "Episode length: 9.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 9        |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=8.60 +/- 0.49\n",
      "Episode length: 8.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | 8.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 59000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 124   |\n",
      "|    total_timesteps | 59168 |\n",
      "------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=47.00 +/- 4.86\n",
      "Episode length: 47.00 +/- 4.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 47          |\n",
      "|    mean_reward          | 47          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 59500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035594806 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.277       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.615       |\n",
      "|    n_updates            | 43          |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=45.80 +/- 5.08\n",
      "Episode length: 45.80 +/- 5.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 45.8     |\n",
      "|    mean_reward     | 45.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=47.20 +/- 2.23\n",
      "Episode length: 47.20 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 47.2     |\n",
      "|    mean_reward     | 47.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 483   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 125   |\n",
      "|    total_timesteps | 60544 |\n",
      "------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=27.40 +/- 2.73\n",
      "Episode length: 27.40 +/- 2.73\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 27.4      |\n",
      "|    mean_reward          | 27.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 61000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1704639 |\n",
      "|    clip_fraction        | 0.512     |\n",
      "|    clip_range           | 0.292     |\n",
      "|    entropy_loss         | -0.448    |\n",
      "|    explained_variance   | 0.622     |\n",
      "|    learning_rate        | 0.00986   |\n",
      "|    loss                 | 0.573     |\n",
      "|    n_updates            | 44        |\n",
      "|    policy_gradient_loss | -0.0287   |\n",
      "|    value_loss           | 2.62      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=28.20 +/- 3.19\n",
      "Episode length: 28.20 +/- 3.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 28.2     |\n",
      "|    mean_reward     | 28.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 61500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 490   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 126   |\n",
      "|    total_timesteps | 61920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=103.00 +/- 5.18\n",
      "Episode length: 103.00 +/- 5.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 103        |\n",
      "|    mean_reward          | 103        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 62000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31583965 |\n",
      "|    clip_fraction        | 0.513      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.366     |\n",
      "|    explained_variance   | 0.794      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 1.12       |\n",
      "|    n_updates            | 45         |\n",
      "|    policy_gradient_loss | 0.00766    |\n",
      "|    value_loss           | 1.92       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=104.00 +/- 4.82\n",
      "Episode length: 104.00 +/- 4.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 104      |\n",
      "|    mean_reward     | 104      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=101.80 +/- 5.49\n",
      "Episode length: 101.80 +/- 5.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 102      |\n",
      "|    mean_reward     | 102      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 63000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 46    |\n",
      "|    time_elapsed    | 127   |\n",
      "|    total_timesteps | 63296 |\n",
      "------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=108.60 +/- 6.59\n",
      "Episode length: 108.60 +/- 6.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 109         |\n",
      "|    mean_reward          | 109         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 63500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011246264 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.0142      |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 3.27        |\n",
      "|    n_updates            | 46          |\n",
      "|    policy_gradient_loss | 0.00366     |\n",
      "|    value_loss           | 8.95        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=111.40 +/- 3.61\n",
      "Episode length: 111.40 +/- 3.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 111      |\n",
      "|    mean_reward     | 111      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=99.40 +/- 2.65\n",
      "Episode length: 99.40 +/- 2.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 99.4     |\n",
      "|    mean_reward     | 99.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 499   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 129   |\n",
      "|    total_timesteps | 64672 |\n",
      "------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=128.40 +/- 5.12\n",
      "Episode length: 128.40 +/- 5.12\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 128         |\n",
      "|    mean_reward          | 128         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 65000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043409176 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.282      |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 2.85        |\n",
      "|    n_updates            | 47          |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=132.60 +/- 7.23\n",
      "Episode length: 132.60 +/- 7.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 133      |\n",
      "|    mean_reward     | 133      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=141.40 +/- 18.68\n",
      "Episode length: 141.40 +/- 18.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 141      |\n",
      "|    mean_reward     | 141      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 131   |\n",
      "|    total_timesteps | 66048 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=175.40 +/- 22.86\n",
      "Episode length: 175.40 +/- 22.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 175         |\n",
      "|    mean_reward          | 175         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030488836 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.29       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.00422     |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | 0.00284     |\n",
      "|    value_loss           | 0.264       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=171.20 +/- 10.80\n",
      "Episode length: 171.20 +/- 10.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 171      |\n",
      "|    mean_reward     | 171      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 67000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 508   |\n",
      "|    iterations      | 49    |\n",
      "|    time_elapsed    | 132   |\n",
      "|    total_timesteps | 67424 |\n",
      "------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=234.00 +/- 41.36\n",
      "Episode length: 234.00 +/- 41.36\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 234        |\n",
      "|    mean_reward          | 234        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 67500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03254437 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.329     |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 49         |\n",
      "|    policy_gradient_loss | -0.00126   |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=279.00 +/- 24.92\n",
      "Episode length: 279.00 +/- 24.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 279      |\n",
      "|    mean_reward     | 279      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=248.80 +/- 13.93\n",
      "Episode length: 248.80 +/- 13.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 249      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 510   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 134   |\n",
      "|    total_timesteps | 68800 |\n",
      "------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=180.60 +/- 16.21\n",
      "Episode length: 180.60 +/- 16.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 181         |\n",
      "|    mean_reward          | 181         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 69000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037501257 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.769       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.0884      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    value_loss           | 0.692       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=195.80 +/- 29.25\n",
      "Episode length: 195.80 +/- 29.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 196      |\n",
      "|    mean_reward     | 196      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 69500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=200.80 +/- 20.83\n",
      "Episode length: 200.80 +/- 20.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 201      |\n",
      "|    mean_reward     | 201      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 137   |\n",
      "|    total_timesteps | 70176 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=356.20 +/- 38.55\n",
      "Episode length: 356.20 +/- 38.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 356         |\n",
      "|    mean_reward          | 356         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026831953 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.932       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | -0.00597    |\n",
      "|    n_updates            | 51          |\n",
      "|    policy_gradient_loss | 0.0069      |\n",
      "|    value_loss           | 0.251       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=404.60 +/- 60.05\n",
      "Episode length: 404.60 +/- 60.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 405      |\n",
      "|    mean_reward     | 405      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=362.80 +/- 33.84\n",
      "Episode length: 362.80 +/- 33.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 363      |\n",
      "|    mean_reward     | 363      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 71500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 509   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 140   |\n",
      "|    total_timesteps | 71552 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=420.80 +/- 43.92\n",
      "Episode length: 420.80 +/- 43.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 421         |\n",
      "|    mean_reward          | 421         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019126453 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.161      |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | 0.00799     |\n",
      "|    value_loss           | 0.257       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=424.40 +/- 58.34\n",
      "Episode length: 424.40 +/- 58.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 424      |\n",
      "|    mean_reward     | 424      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 510   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 142   |\n",
      "|    total_timesteps | 72928 |\n",
      "------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 73000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007081023 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.133      |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.02        |\n",
      "|    n_updates            | 53          |\n",
      "|    policy_gradient_loss | 0.00882     |\n",
      "|    value_loss           | 0.324       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=73500, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 73500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 493   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 150   |\n",
      "|    total_timesteps | 74304 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=389.80 +/- 56.98\n",
      "Episode length: 389.80 +/- 56.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 390         |\n",
      "|    mean_reward          | 390         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025164604 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.119      |\n",
      "|    explained_variance   | -25.2       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | -0.0019     |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | 0.00939     |\n",
      "|    value_loss           | 0.00982     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=372.40 +/- 54.29\n",
      "Episode length: 372.40 +/- 54.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 372      |\n",
      "|    mean_reward     | 372      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=422.80 +/- 58.96\n",
      "Episode length: 422.80 +/- 58.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 423      |\n",
      "|    mean_reward     | 423      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 75500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 488   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 154   |\n",
      "|    total_timesteps | 75680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=300.80 +/- 23.31\n",
      "Episode length: 300.80 +/- 23.31\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 301        |\n",
      "|    mean_reward          | 301        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 76000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07724127 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.203     |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.0291     |\n",
      "|    n_updates            | 55         |\n",
      "|    policy_gradient_loss | 0.0278     |\n",
      "|    value_loss           | 0.66       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=322.60 +/- 50.67\n",
      "Episode length: 322.60 +/- 50.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 323      |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 76500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=325.60 +/- 13.28\n",
      "Episode length: 325.60 +/- 13.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 326      |\n",
      "|    mean_reward     | 326      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 77000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 489   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 157   |\n",
      "|    total_timesteps | 77056 |\n",
      "------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=187.20 +/- 11.82\n",
      "Episode length: 187.20 +/- 11.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 187         |\n",
      "|    mean_reward          | 187         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 77500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042084273 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.292       |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.928       |\n",
      "|    learning_rate        | 0.00986     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | 0.00779     |\n",
      "|    value_loss           | 0.236       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=189.80 +/- 11.23\n",
      "Episode length: 189.80 +/- 11.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 190      |\n",
      "|    mean_reward     | 190      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 493   |\n",
      "|    iterations      | 57    |\n",
      "|    time_elapsed    | 159   |\n",
      "|    total_timesteps | 78432 |\n",
      "------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=144.80 +/- 17.55\n",
      "Episode length: 144.80 +/- 17.55\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 145        |\n",
      "|    mean_reward          | 145        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 78500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13637294 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.292      |\n",
      "|    entropy_loss         | -0.234     |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.00986    |\n",
      "|    loss                 | 0.301      |\n",
      "|    n_updates            | 57         |\n",
      "|    policy_gradient_loss | 0.0616     |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=144.80 +/- 16.85\n",
      "Episode length: 144.80 +/- 16.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 145      |\n",
      "|    mean_reward     | 145      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=150.40 +/- 25.27\n",
      "Episode length: 150.40 +/- 25.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 150      |\n",
      "|    mean_reward     | 150      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 79500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 496   |\n",
      "|    iterations      | 58    |\n",
      "|    time_elapsed    | 160   |\n",
      "|    total_timesteps | 79808 |\n",
      "------------------------------\n",
      "Single environment training took 160.77 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>174.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lilac-sweep-18</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/7oqe6lm9' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/7oqe6lm9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_135714-7oqe6lm9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p9pbrtbo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.17538614586038498\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.004948865621239801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9022874465577274\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9331839741723236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006928698267084203\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 8.137605341865102\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1181\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 32133\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_140009-p9pbrtbo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/p9pbrtbo' target=\"_blank\">ethereal-sweep-19</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/p9pbrtbo' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/p9pbrtbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1181`, after every 18 untruncated mini-batches, there will be a truncated mini-batch of size 29\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1181 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=125.60 +/- 62.26\n",
      "Episode length: 125.60 +/- 62.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 126      |\n",
      "|    mean_reward     | 126      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=136.80 +/- 66.04\n",
      "Episode length: 136.80 +/- 66.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 137      |\n",
      "|    mean_reward     | 137      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 911  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1181 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=1500, episode_reward=72.80 +/- 7.86\n",
      "Episode length: 72.80 +/- 7.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 72.8        |\n",
      "|    mean_reward          | 72.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014046804 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.00304     |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.54        |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 5.29        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=73.20 +/- 10.55\n",
      "Episode length: 73.20 +/- 10.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 73.2     |\n",
      "|    mean_reward     | 73.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 958  |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2362 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=2500, episode_reward=410.80 +/- 73.51\n",
      "Episode length: 410.80 +/- 73.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 411         |\n",
      "|    mean_reward          | 411         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020430489 |\n",
      "|    clip_fraction        | 0.434       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | 0.61        |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.794       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 2.08        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=268.20 +/- 135.92\n",
      "Episode length: 268.20 +/- 135.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 268      |\n",
      "|    mean_reward     | 268      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=291.40 +/- 164.24\n",
      "Episode length: 291.40 +/- 164.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 291      |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 655  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 3543 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=120.60 +/- 11.66\n",
      "Episode length: 120.60 +/- 11.66\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 121         |\n",
      "|    mean_reward          | 121         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022053054 |\n",
      "|    clip_fraction        | 0.404       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.64        |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 1.31        |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=147.60 +/- 41.15\n",
      "Episode length: 147.60 +/- 41.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 148      |\n",
      "|    mean_reward     | 148      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 698  |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 4724 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=5000, episode_reward=162.40 +/- 61.11\n",
      "Episode length: 162.40 +/- 61.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 162         |\n",
      "|    mean_reward          | 162         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019629287 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.579      |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.888       |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 2.66        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=206.00 +/- 104.19\n",
      "Episode length: 206.00 +/- 104.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 206      |\n",
      "|    mean_reward     | 206      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 709  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 5905 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=195.80 +/- 12.70\n",
      "Episode length: 195.80 +/- 12.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 196         |\n",
      "|    mean_reward          | 196         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027608253 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.559       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.801       |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=282.60 +/- 111.65\n",
      "Episode length: 282.60 +/- 111.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 283      |\n",
      "|    mean_reward     | 283      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=291.60 +/- 129.71\n",
      "Episode length: 291.60 +/- 129.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 292      |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 669  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 7086 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=7500, episode_reward=259.40 +/- 57.25\n",
      "Episode length: 259.40 +/- 57.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 259         |\n",
      "|    mean_reward          | 259         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016559726 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.465       |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 1.13        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=238.40 +/- 103.22\n",
      "Episode length: 238.40 +/- 103.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 238      |\n",
      "|    mean_reward     | 238      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 667  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 8267 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8500, episode_reward=361.80 +/- 88.01\n",
      "Episode length: 361.80 +/- 88.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 362         |\n",
      "|    mean_reward          | 362         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016489556 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    value_loss           | 0.557       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=310.40 +/- 58.61\n",
      "Episode length: 310.40 +/- 58.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 310      |\n",
      "|    mean_reward     | 310      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 644  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 9448 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=9500, episode_reward=391.80 +/- 76.47\n",
      "Episode length: 391.80 +/- 76.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 392         |\n",
      "|    mean_reward          | 392         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022735316 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | 0.00164     |\n",
      "|    value_loss           | 0.291       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=466.00 +/- 27.95\n",
      "Episode length: 466.00 +/- 27.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 466      |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10500, episode_reward=473.40 +/- 51.22\n",
      "Episode length: 473.40 +/- 51.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 473      |\n",
      "|    mean_reward     | 473      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 594   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 10629 |\n",
      "------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=378.40 +/- 81.70\n",
      "Episode length: 378.40 +/- 81.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 378         |\n",
      "|    mean_reward          | 378         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027251473 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | 0.00179     |\n",
      "|    value_loss           | 0.428       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=299.20 +/- 55.43\n",
      "Episode length: 299.20 +/- 55.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 299      |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 592   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 11810 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=296.20 +/- 67.39\n",
      "Episode length: 296.20 +/- 67.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 296         |\n",
      "|    mean_reward          | 296         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019645283 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.055       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00934     |\n",
      "|    value_loss           | 0.362       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=283.40 +/- 61.79\n",
      "Episode length: 283.40 +/- 61.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 283      |\n",
      "|    mean_reward     | 283      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 597   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 12991 |\n",
      "------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=266.20 +/- 108.18\n",
      "Episode length: 266.20 +/- 108.18\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 266        |\n",
      "|    mean_reward          | 266        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00943512 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.175      |\n",
      "|    entropy_loss         | -0.524     |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 0.00693    |\n",
      "|    loss                 | 0.191      |\n",
      "|    n_updates            | 33         |\n",
      "|    policy_gradient_loss | -0.000856  |\n",
      "|    value_loss           | 0.457      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=268.00 +/- 117.87\n",
      "Episode length: 268.00 +/- 117.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 268      |\n",
      "|    mean_reward     | 268      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=280.40 +/- 37.93\n",
      "Episode length: 280.40 +/- 37.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 592   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 14172 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=298.60 +/- 74.42\n",
      "Episode length: 298.60 +/- 74.42\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 299        |\n",
      "|    mean_reward          | 299        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01627488 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.175      |\n",
      "|    entropy_loss         | -0.544     |\n",
      "|    explained_variance   | 0.933      |\n",
      "|    learning_rate        | 0.00693    |\n",
      "|    loss                 | 0.203      |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    value_loss           | 0.305      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=269.80 +/- 50.52\n",
      "Episode length: 269.80 +/- 50.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 270      |\n",
      "|    mean_reward     | 270      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 597   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 15353 |\n",
      "------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=254.20 +/- 16.61\n",
      "Episode length: 254.20 +/- 16.61\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 254         |\n",
      "|    mean_reward          | 254         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 15500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015376749 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.058       |\n",
      "|    n_updates            | 39          |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    value_loss           | 0.272       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=256.20 +/- 15.46\n",
      "Episode length: 256.20 +/- 15.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 256      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=243.00 +/- 12.82\n",
      "Episode length: 243.00 +/- 12.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 243      |\n",
      "|    mean_reward     | 243      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 595   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 16534 |\n",
      "------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=251.00 +/- 29.68\n",
      "Episode length: 251.00 +/- 29.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 251         |\n",
      "|    mean_reward          | 251         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009582725 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.387       |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | 0.00325     |\n",
      "|    value_loss           | 0.568       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=259.20 +/- 15.48\n",
      "Episode length: 259.20 +/- 15.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 259      |\n",
      "|    mean_reward     | 259      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 17500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 601   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 17715 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=322.80 +/- 49.53\n",
      "Episode length: 322.80 +/- 49.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 323         |\n",
      "|    mean_reward          | 323         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016262641 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.454       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.00423     |\n",
      "|    value_loss           | 1.52        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=324.40 +/- 32.37\n",
      "Episode length: 324.40 +/- 32.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 324      |\n",
      "|    mean_reward     | 324      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 602   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 18896 |\n",
      "------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=385.80 +/- 98.49\n",
      "Episode length: 385.80 +/- 98.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 386         |\n",
      "|    mean_reward          | 386         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 19000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018073872 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.791       |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00562    |\n",
      "|    value_loss           | 0.788       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=399.20 +/- 94.34\n",
      "Episode length: 399.20 +/- 94.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 399      |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 19500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=422.40 +/- 105.11\n",
      "Episode length: 422.40 +/- 105.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 422      |\n",
      "|    mean_reward     | 422      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 585   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 20077 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=402.80 +/- 109.79\n",
      "Episode length: 402.80 +/- 109.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 403         |\n",
      "|    mean_reward          | 403         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015359736 |\n",
      "|    clip_fraction        | 0.323       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.017       |\n",
      "|    n_updates            | 51          |\n",
      "|    policy_gradient_loss | 0.00243     |\n",
      "|    value_loss           | 0.383       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=442.00 +/- 83.53\n",
      "Episode length: 442.00 +/- 83.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 442      |\n",
      "|    mean_reward     | 442      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 21000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 579   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 21258 |\n",
      "------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=473.40 +/- 37.00\n",
      "Episode length: 473.40 +/- 37.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 473         |\n",
      "|    mean_reward          | 473         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027900806 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.107       |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    value_loss           | 0.387       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=338.20 +/- 91.16\n",
      "Episode length: 338.20 +/- 91.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 338      |\n",
      "|    mean_reward     | 338      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 576   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 22439 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=387.80 +/- 91.69\n",
      "Episode length: 387.80 +/- 91.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 388         |\n",
      "|    mean_reward          | 388         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025011353 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.315       |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    value_loss           | 0.216       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=333.00 +/- 103.31\n",
      "Episode length: 333.00 +/- 103.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 333      |\n",
      "|    mean_reward     | 333      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=316.80 +/- 68.55\n",
      "Episode length: 316.80 +/- 68.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 317      |\n",
      "|    mean_reward     | 317      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 23500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 565   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 23620 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=349.40 +/- 118.00\n",
      "Episode length: 349.40 +/- 118.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 349         |\n",
      "|    mean_reward          | 349         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036383126 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0863      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.0209      |\n",
      "|    value_loss           | 0.232       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=342.60 +/- 92.24\n",
      "Episode length: 342.60 +/- 92.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 566   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 24801 |\n",
      "------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=343.20 +/- 129.64\n",
      "Episode length: 343.20 +/- 129.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 343         |\n",
      "|    mean_reward          | 343         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 25000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084137544 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0918      |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=297.20 +/- 71.39\n",
      "Episode length: 297.20 +/- 71.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 297      |\n",
      "|    mean_reward     | 297      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 569   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 25982 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=339.40 +/- 93.60\n",
      "Episode length: 339.40 +/- 93.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 339         |\n",
      "|    mean_reward          | 339         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016715791 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.00675     |\n",
      "|    n_updates            | 66          |\n",
      "|    policy_gradient_loss | 0.0154      |\n",
      "|    value_loss           | 0.111       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=344.60 +/- 105.10\n",
      "Episode length: 344.60 +/- 105.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 345      |\n",
      "|    mean_reward     | 345      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=294.40 +/- 70.20\n",
      "Episode length: 294.40 +/- 70.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 294      |\n",
      "|    mean_reward     | 294      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 564   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 27163 |\n",
      "------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=403.20 +/- 87.51\n",
      "Episode length: 403.20 +/- 87.51\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 403         |\n",
      "|    mean_reward          | 403         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 27500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029391618 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.447      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0687      |\n",
      "|    n_updates            | 69          |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 0.0953      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=412.80 +/- 106.81\n",
      "Episode length: 412.80 +/- 106.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 413      |\n",
      "|    mean_reward     | 413      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 546   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 28344 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=363.40 +/- 116.57\n",
      "Episode length: 363.40 +/- 116.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 363         |\n",
      "|    mean_reward          | 363         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018385354 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.337       |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.00693     |\n",
      "|    value_loss           | 0.409       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=323.00 +/- 100.63\n",
      "Episode length: 323.00 +/- 100.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 323      |\n",
      "|    mean_reward     | 323      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=479.00 +/- 18.24\n",
      "Episode length: 479.00 +/- 18.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 479      |\n",
      "|    mean_reward     | 479      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 29500    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 531   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 29525 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=380.80 +/- 136.67\n",
      "Episode length: 380.80 +/- 136.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 381         |\n",
      "|    mean_reward          | 381         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017794752 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0262      |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    value_loss           | 0.0759      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=270.00 +/- 56.76\n",
      "Episode length: 270.00 +/- 56.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 270      |\n",
      "|    mean_reward     | 270      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 530   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 30706 |\n",
      "------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=418.80 +/- 73.82\n",
      "Episode length: 418.80 +/- 73.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 419         |\n",
      "|    mean_reward          | 419         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 31000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007942537 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.175       |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00693     |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.00807     |\n",
      "|    value_loss           | 0.0575      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=390.60 +/- 112.89\n",
      "Episode length: 390.60 +/- 112.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 391      |\n",
      "|    mean_reward     | 391      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 31500    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 525   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 31887 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=285.00 +/- 78.27\n",
      "Episode length: 285.00 +/- 78.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 285          |\n",
      "|    mean_reward          | 285          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031730367 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.175        |\n",
      "|    entropy_loss         | -0.412       |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.00693      |\n",
      "|    loss                 | -0.00504     |\n",
      "|    n_updates            | 81           |\n",
      "|    policy_gradient_loss | 0.00565      |\n",
      "|    value_loss           | 0.0475       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=357.20 +/- 100.25\n",
      "Episode length: 357.20 +/- 100.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 357      |\n",
      "|    mean_reward     | 357      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32500    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=420.60 +/- 114.48\n",
      "Episode length: 420.60 +/- 114.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 421      |\n",
      "|    mean_reward     | 421      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 33000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 33068 |\n",
      "------------------------------\n",
      "Single environment training took 64.23 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>211.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-19</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/p9pbrtbo' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/p9pbrtbo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_140009-p9pbrtbo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2871tio1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.13800909172177428\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0006701640118988939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9937755798080838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9087862201061372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0025920242175827436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 8.878908130756182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 189\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 42435\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_140126-2871tio1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2871tio1' target=\"_blank\">zany-sweep-20</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/q4f7py8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2871tio1' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2871tio1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1171 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 189  |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 189`, after every 2 untruncated mini-batches, there will be a truncated mini-batch of size 61\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=189 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 378          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010510014 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | -0.025       |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 16.6         |\n",
      "|    n_updates            | 5            |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 42.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=500, episode_reward=36.80 +/- 9.58\n",
      "Episode length: 36.80 +/- 9.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 36.8        |\n",
      "|    mean_reward          | 36.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 500         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005213955 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.294      |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 8.06        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00713    |\n",
      "|    value_loss           | 27.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 820 |\n",
      "|    iterations      | 3   |\n",
      "|    time_elapsed    | 0   |\n",
      "|    total_timesteps | 567 |\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 856         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 756         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003568032 |\n",
      "|    clip_fraction        | 0.00963     |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | -0.107      |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 3.4         |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.00328    |\n",
      "|    value_loss           | 9.74        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 825         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 945         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003599421 |\n",
      "|    clip_fraction        | 0.049       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | -0.0724     |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 3.51        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    value_loss           | 8.57        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=175.40 +/- 13.97\n",
      "Episode length: 175.40 +/- 13.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 175         |\n",
      "|    mean_reward          | 175         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004206291 |\n",
      "|    clip_fraction        | 0.0722      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.0321      |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 3.21        |\n",
      "|    n_updates            | 25          |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    value_loss           | 6.94        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 652  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1134 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 697         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 1323        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009338786 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 3.35        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00873    |\n",
      "|    value_loss           | 6.17        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=184.60 +/- 55.65\n",
      "Episode length: 184.60 +/- 55.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 185         |\n",
      "|    mean_reward          | 185         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008128945 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 35          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 3.67        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 614  |\n",
      "|    iterations      | 8    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 1512 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 1701        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001735062 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | -1.14       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 5.74        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 672         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 1890        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028582752 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 1.86        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=130.20 +/- 8.54\n",
      "Episode length: 130.20 +/- 8.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 130          |\n",
      "|    mean_reward          | 130          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014792755 |\n",
      "|    clip_fraction        | 0.179        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.571       |\n",
      "|    explained_variance   | 0.328        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.601        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00688     |\n",
      "|    value_loss           | 1.41         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 630  |\n",
      "|    iterations      | 11   |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2079 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 655         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2268        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004858618 |\n",
      "|    clip_fraction        | 0.0584      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.358       |\n",
      "|    n_updates            | 55          |\n",
      "|    policy_gradient_loss | 0.000852    |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 679          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 2457         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065488312 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | -0.102       |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 2.45         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0092      |\n",
      "|    value_loss           | 7.15         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=168.40 +/- 14.39\n",
      "Episode length: 168.40 +/- 14.39\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 168          |\n",
      "|    mean_reward          | 168          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028934174 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | -0.223       |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.894        |\n",
      "|    n_updates            | 65           |\n",
      "|    policy_gradient_loss | 0.00059      |\n",
      "|    value_loss           | 7.05         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 623  |\n",
      "|    iterations      | 14   |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2646 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 642          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 2835         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049601295 |\n",
      "|    clip_fraction        | 0.0832       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.545       |\n",
      "|    explained_variance   | 0.485        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.208        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 1.36         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=324.00 +/- 129.34\n",
      "Episode length: 324.00 +/- 129.34\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 324          |\n",
      "|    mean_reward          | 324          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032938514 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.51        |\n",
      "|    explained_variance   | 0.661        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.251        |\n",
      "|    n_updates            | 75           |\n",
      "|    policy_gradient_loss | 0.00049      |\n",
      "|    value_loss           | 0.859        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 571  |\n",
      "|    iterations      | 16   |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 3024 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 587          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 3213         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031982295 |\n",
      "|    clip_fraction        | 0.0806       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.472       |\n",
      "|    explained_variance   | 0.565        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.345        |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.00121      |\n",
      "|    value_loss           | 0.982        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 3402        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004151841 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.628       |\n",
      "|    n_updates            | 85          |\n",
      "|    policy_gradient_loss | -0.00211    |\n",
      "|    value_loss           | 1.14        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=442.80 +/- 114.40\n",
      "Episode length: 442.80 +/- 114.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 443          |\n",
      "|    mean_reward          | 443          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030498088 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.305        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    value_loss           | 0.596        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 528  |\n",
      "|    iterations      | 19   |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 3591 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 3780         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029634656 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.531       |\n",
      "|    explained_variance   | 0.536        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.406        |\n",
      "|    n_updates            | 95           |\n",
      "|    policy_gradient_loss | 0.000125     |\n",
      "|    value_loss           | 1.12         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 555          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 3969         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005376846 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.515       |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.264        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    value_loss           | 0.898        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=279.60 +/- 115.86\n",
      "Episode length: 279.60 +/- 115.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 280          |\n",
      "|    mean_reward          | 280          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066215605 |\n",
      "|    clip_fraction        | 0.0903       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.361        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.312        |\n",
      "|    n_updates            | 105          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 1.18         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 517  |\n",
      "|    iterations      | 22   |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 4158 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 530         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4347        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004186581 |\n",
      "|    clip_fraction        | 0.0634      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.274       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    value_loss           | 0.761       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=386.00 +/- 113.02\n",
      "Episode length: 386.00 +/- 113.02\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 386          |\n",
      "|    mean_reward          | 386          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042844266 |\n",
      "|    clip_fraction        | 0.0783       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.545       |\n",
      "|    explained_variance   | 0.652        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.539        |\n",
      "|    n_updates            | 115          |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 0.899        |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 493  |\n",
      "|    iterations      | 24   |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 4536 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 505          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4725         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026292002 |\n",
      "|    clip_fraction        | 0.0909       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.57        |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.525        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00574     |\n",
      "|    value_loss           | 1.05         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4914        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005604734 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | -0.562      |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.449       |\n",
      "|    n_updates            | 125         |\n",
      "|    policy_gradient_loss | -0.00454    |\n",
      "|    value_loss           | 2.57        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=180.60 +/- 46.80\n",
      "Episode length: 180.60 +/- 46.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 181          |\n",
      "|    mean_reward          | 181          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020364167 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.525       |\n",
      "|    explained_variance   | 0.381        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0626       |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0035      |\n",
      "|    value_loss           | 0.292        |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 504  |\n",
      "|    iterations      | 27   |\n",
      "|    time_elapsed    | 10   |\n",
      "|    total_timesteps | 5103 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 5292         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025190401 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.549       |\n",
      "|    explained_variance   | 0.439        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.518        |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | -0.00715     |\n",
      "|    value_loss           | 1.38         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 524         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 5481        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008295398 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.365       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    value_loss           | 0.928       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=320.60 +/- 91.68\n",
      "Episode length: 320.60 +/- 91.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 321         |\n",
      "|    mean_reward          | 321         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004722873 |\n",
      "|    clip_fraction        | 0.0535      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.38        |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.204       |\n",
      "|    n_updates            | 145         |\n",
      "|    policy_gradient_loss | 0.00249     |\n",
      "|    value_loss           | 1.38        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 501  |\n",
      "|    iterations      | 30   |\n",
      "|    time_elapsed    | 11   |\n",
      "|    total_timesteps | 5670 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 5859         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038396353 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0.763        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.238        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00406     |\n",
      "|    value_loss           | 0.601        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=386.60 +/- 97.90\n",
      "Episode length: 386.60 +/- 97.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 387          |\n",
      "|    mean_reward          | 387          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042050737 |\n",
      "|    clip_fraction        | 0.131        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 0.775        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.262        |\n",
      "|    n_updates            | 155          |\n",
      "|    policy_gradient_loss | 0.00291      |\n",
      "|    value_loss           | 0.525        |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 484  |\n",
      "|    iterations      | 32   |\n",
      "|    time_elapsed    | 12   |\n",
      "|    total_timesteps | 6048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 491          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6237         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010865622 |\n",
      "|    clip_fraction        | 0.0935       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | -2.91        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00261      |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | 0.00111      |\n",
      "|    value_loss           | 0.0242       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 6426         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041166074 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.153        |\n",
      "|    n_updates            | 165          |\n",
      "|    policy_gradient_loss | 0.000612     |\n",
      "|    value_loss           | 0.435        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=274.80 +/- 83.26\n",
      "Episode length: 274.80 +/- 83.26\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 275          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043423176 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.502       |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.139        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 0.53         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 483  |\n",
      "|    iterations      | 35   |\n",
      "|    time_elapsed    | 13   |\n",
      "|    total_timesteps | 6615 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6804         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009598296 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.511       |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.121        |\n",
      "|    n_updates            | 175          |\n",
      "|    policy_gradient_loss | -0.00449     |\n",
      "|    value_loss           | 0.57         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 498          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 6993         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038212587 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.698        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00503      |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.0048      |\n",
      "|    value_loss           | 0.0103       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=242.40 +/- 85.43\n",
      "Episode length: 242.40 +/- 85.43\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 242         |\n",
      "|    mean_reward          | 242         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004288677 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 185         |\n",
      "|    policy_gradient_loss | 0.00467     |\n",
      "|    value_loss           | 0.503       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 488  |\n",
      "|    iterations      | 38   |\n",
      "|    time_elapsed    | 14   |\n",
      "|    total_timesteps | 7182 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 7371         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021532166 |\n",
      "|    clip_fraction        | 0.0892       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.491       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.241        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 0.396        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=275.00 +/- 72.56\n",
      "Episode length: 275.00 +/- 72.56\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 275          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004717867 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.222        |\n",
      "|    n_updates            | 195          |\n",
      "|    policy_gradient_loss | -0.000804    |\n",
      "|    value_loss           | 0.479        |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 483  |\n",
      "|    iterations      | 40   |\n",
      "|    time_elapsed    | 15   |\n",
      "|    total_timesteps | 7560 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 489          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 7749         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006526842 |\n",
      "|    clip_fraction        | 0.00531      |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.495       |\n",
      "|    explained_variance   | 0.846        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.119        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | 0.000878     |\n",
      "|    value_loss           | 0.379        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 7938         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023366993 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.183        |\n",
      "|    n_updates            | 205          |\n",
      "|    policy_gradient_loss | -0.00687     |\n",
      "|    value_loss           | 0.316        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=278.80 +/- 116.69\n",
      "Episode length: 278.80 +/- 116.69\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 279          |\n",
      "|    mean_reward          | 279          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040804925 |\n",
      "|    clip_fraction        | 0.165        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.727        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.49         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00309     |\n",
      "|    value_loss           | 0.592        |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 481  |\n",
      "|    iterations      | 43   |\n",
      "|    time_elapsed    | 16   |\n",
      "|    total_timesteps | 8127 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 8316        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002287349 |\n",
      "|    clip_fraction        | 0.0361      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.215       |\n",
      "|    n_updates            | 215         |\n",
      "|    policy_gradient_loss | 0.00176     |\n",
      "|    value_loss           | 0.286       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=321.80 +/- 96.45\n",
      "Episode length: 321.80 +/- 96.45\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 322        |\n",
      "|    mean_reward          | 322        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01379212 |\n",
      "|    clip_fraction        | 0.0978     |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.478     |\n",
      "|    explained_variance   | 0.87       |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.0855     |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.00587   |\n",
      "|    value_loss           | 0.336      |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 475  |\n",
      "|    iterations      | 45   |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 8505 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8694         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010563544 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.495       |\n",
      "|    explained_variance   | -0.103       |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.000252     |\n",
      "|    n_updates            | 225          |\n",
      "|    policy_gradient_loss | -0.00309     |\n",
      "|    value_loss           | 0.0181       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 485          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8883         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014286431 |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.482       |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.253        |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00317     |\n",
      "|    value_loss           | 0.438        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=280.20 +/- 69.67\n",
      "Episode length: 280.20 +/- 69.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 280          |\n",
      "|    mean_reward          | 280          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042366856 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.47        |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 235          |\n",
      "|    policy_gradient_loss | 0.000333     |\n",
      "|    value_loss           | 0.403        |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 475  |\n",
      "|    iterations      | 48   |\n",
      "|    time_elapsed    | 19   |\n",
      "|    total_timesteps | 9072 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 9261         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036450245 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.888        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000691    |\n",
      "|    value_loss           | 0.291        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 486          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 9450         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028195356 |\n",
      "|    clip_fraction        | 0.0477       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.504       |\n",
      "|    explained_variance   | 0.706        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0163       |\n",
      "|    n_updates            | 245          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 0.107        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=269.00 +/- 74.74\n",
      "Episode length: 269.00 +/- 74.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 269          |\n",
      "|    mean_reward          | 269          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024345627 |\n",
      "|    clip_fraction        | 0.0284       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.472       |\n",
      "|    explained_variance   | 0.744        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0533       |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000233    |\n",
      "|    value_loss           | 0.191        |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 476  |\n",
      "|    iterations      | 51   |\n",
      "|    time_elapsed    | 20   |\n",
      "|    total_timesteps | 9639 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 9828         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035693448 |\n",
      "|    clip_fraction        | 0.0431       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.494       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.358        |\n",
      "|    n_updates            | 255          |\n",
      "|    policy_gradient_loss | -2.85e-05    |\n",
      "|    value_loss           | 0.509        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=347.00 +/- 97.73\n",
      "Episode length: 347.00 +/- 97.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 347          |\n",
      "|    mean_reward          | 347          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014606161 |\n",
      "|    clip_fraction        | 0.0984       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.463       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.024        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00535     |\n",
      "|    value_loss           | 0.138        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 469   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 10017 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 10206        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013023192 |\n",
      "|    clip_fraction        | 0.0454       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.475       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.212        |\n",
      "|    n_updates            | 265          |\n",
      "|    policy_gradient_loss | -0.00488     |\n",
      "|    value_loss           | 0.383        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 10395        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021507153 |\n",
      "|    clip_fraction        | 0.0552       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.446       |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0896       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00336     |\n",
      "|    value_loss           | 0.231        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=274.40 +/- 24.74\n",
      "Episode length: 274.40 +/- 24.74\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 274          |\n",
      "|    mean_reward          | 274          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017364775 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.423        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00341     |\n",
      "|    n_updates            | 275          |\n",
      "|    policy_gradient_loss | -0.0037      |\n",
      "|    value_loss           | 0.00368      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 472   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 10584 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 476          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 10773        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036911468 |\n",
      "|    clip_fraction        | 0.0488       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.431       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0842       |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 0.297        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 10962       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002081132 |\n",
      "|    clip_fraction        | 0.0591      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.353       |\n",
      "|    n_updates            | 285         |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    value_loss           | 0.473       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=343.20 +/- 80.86\n",
      "Episode length: 343.20 +/- 80.86\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 343           |\n",
      "|    mean_reward          | 343           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 11000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00058707193 |\n",
      "|    clip_fraction        | 0.0409        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.445        |\n",
      "|    explained_variance   | 0.846         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.091         |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.00181      |\n",
      "|    value_loss           | 0.415         |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 471   |\n",
      "|    iterations      | 59    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 11151 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 476          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 11340        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046097445 |\n",
      "|    clip_fraction        | 0.0657       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.417       |\n",
      "|    explained_variance   | 0.648        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0211       |\n",
      "|    n_updates            | 295          |\n",
      "|    policy_gradient_loss | -0.00375     |\n",
      "|    value_loss           | 0.147        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=353.20 +/- 53.49\n",
      "Episode length: 353.20 +/- 53.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 353         |\n",
      "|    mean_reward          | 353         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 11500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005392432 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0488      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 464   |\n",
      "|    iterations      | 61    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 11529 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 11718       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001235551 |\n",
      "|    clip_fraction        | 0.00318     |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.441      |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.203       |\n",
      "|    n_updates            | 305         |\n",
      "|    policy_gradient_loss | -0.000275   |\n",
      "|    value_loss           | 0.494       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 473          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 11907        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015062526 |\n",
      "|    clip_fraction        | 0.0809       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.362        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00673     |\n",
      "|    value_loss           | 0.0215       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=447.40 +/- 66.57\n",
      "Episode length: 447.40 +/- 66.57\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 447          |\n",
      "|    mean_reward          | 447          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023799343 |\n",
      "|    clip_fraction        | 0.0754       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.394       |\n",
      "|    explained_variance   | 0.241        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.364        |\n",
      "|    n_updates            | 315          |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 1.78         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 457   |\n",
      "|    iterations      | 64    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 12096 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 12285       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002112345 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.39       |\n",
      "|    explained_variance   | -3.36       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.00304     |\n",
      "|    value_loss           | 0.0349      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 12474        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006087403 |\n",
      "|    clip_fraction        | 0.164        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | 0.566        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.282        |\n",
      "|    n_updates            | 325          |\n",
      "|    policy_gradient_loss | -0.00422     |\n",
      "|    value_loss           | 0.919        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=411.40 +/- 46.85\n",
      "Episode length: 411.40 +/- 46.85\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 411          |\n",
      "|    mean_reward          | 411          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017169868 |\n",
      "|    clip_fraction        | 0.0848       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.106        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0687       |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | 0.00344      |\n",
      "|    value_loss           | 0.219        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 454   |\n",
      "|    iterations      | 67    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 12663 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 458          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 12852        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022476602 |\n",
      "|    clip_fraction        | 0.0605       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.407       |\n",
      "|    explained_variance   | 0.679        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0213       |\n",
      "|    n_updates            | 335          |\n",
      "|    policy_gradient_loss | -0.000743    |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=423.60 +/- 89.71\n",
      "Episode length: 423.60 +/- 89.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 424         |\n",
      "|    mean_reward          | 424         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004638683 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.275       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000336   |\n",
      "|    value_loss           | 0.486       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 448   |\n",
      "|    iterations      | 69    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 13041 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 450         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 13230       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003508615 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 345         |\n",
      "|    policy_gradient_loss | -0.000271   |\n",
      "|    value_loss           | 1.06        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 454        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 13419      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01158834 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.4       |\n",
      "|    explained_variance   | -0.106     |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | -0.00831   |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.00895   |\n",
      "|    value_loss           | 0.0449     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=312.00 +/- 33.44\n",
      "Episode length: 312.00 +/- 33.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 312         |\n",
      "|    mean_reward          | 312         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 13500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007061674 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | 0.557       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.472       |\n",
      "|    n_updates            | 355         |\n",
      "|    policy_gradient_loss | -0.00443    |\n",
      "|    value_loss           | 1.05        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 448   |\n",
      "|    iterations      | 72    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 13608 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 452         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 13797       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006480023 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.836       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.143       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.0025      |\n",
      "|    value_loss           | 0.453       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 13986        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053503723 |\n",
      "|    clip_fraction        | 0.225        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.381       |\n",
      "|    explained_variance   | -2.69        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 365          |\n",
      "|    policy_gradient_loss | -0.00554     |\n",
      "|    value_loss           | 0.00715      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=345.20 +/- 14.57\n",
      "Episode length: 345.20 +/- 14.57\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 345           |\n",
      "|    mean_reward          | 345           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 14000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036432268 |\n",
      "|    clip_fraction        | 0.00531       |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.383        |\n",
      "|    explained_variance   | 0.824         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.191         |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | 0.00104       |\n",
      "|    value_loss           | 0.349         |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 447   |\n",
      "|    iterations      | 75    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 14175 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 14364        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034284105 |\n",
      "|    clip_fraction        | 0.0997       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.106        |\n",
      "|    n_updates            | 375          |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 0.328        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=301.60 +/- 32.49\n",
      "Episode length: 301.60 +/- 32.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 302         |\n",
      "|    mean_reward          | 302         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003799246 |\n",
      "|    clip_fraction        | 0.0384      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | -24.6       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0154      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00098    |\n",
      "|    value_loss           | 0.014       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 445   |\n",
      "|    iterations      | 77    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 14553 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 14742        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013891617 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.379       |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0796       |\n",
      "|    n_updates            | 385          |\n",
      "|    policy_gradient_loss | -0.00465     |\n",
      "|    value_loss           | 0.251        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 451         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 14931       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004969988 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.236       |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 0.482       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=370.40 +/- 63.80\n",
      "Episode length: 370.40 +/- 63.80\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 370           |\n",
      "|    mean_reward          | 370           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 15000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00088269875 |\n",
      "|    clip_fraction        | 0.0423        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.355        |\n",
      "|    explained_variance   | 0.786         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00109       |\n",
      "|    n_updates            | 395           |\n",
      "|    policy_gradient_loss | -0.000558     |\n",
      "|    value_loss           | 0.0249        |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 444   |\n",
      "|    iterations      | 80    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 15120 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 15309        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006320339 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.364       |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.11         |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00051     |\n",
      "|    value_loss           | 0.255        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 15498        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008336668 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.368       |\n",
      "|    explained_variance   | 0.82         |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.315        |\n",
      "|    n_updates            | 405          |\n",
      "|    policy_gradient_loss | 0.00142      |\n",
      "|    value_loss           | 0.487        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=265.60 +/- 34.98\n",
      "Episode length: 265.60 +/- 34.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 266          |\n",
      "|    mean_reward          | 266          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 15500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019987344 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.359       |\n",
      "|    explained_variance   | 0.212        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0409       |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | 9.16e-05     |\n",
      "|    value_loss           | 0.0885       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 446   |\n",
      "|    iterations      | 83    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 15687 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 449         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 15876       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005651128 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.37       |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.152       |\n",
      "|    n_updates            | 415         |\n",
      "|    policy_gradient_loss | 0.00319     |\n",
      "|    value_loss           | 0.634       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=264.80 +/- 29.01\n",
      "Episode length: 264.80 +/- 29.01\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 265           |\n",
      "|    mean_reward          | 265           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 16000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047038883 |\n",
      "|    clip_fraction        | 0.0327        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.356        |\n",
      "|    explained_variance   | 0.868         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.0918        |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | 0.00176       |\n",
      "|    value_loss           | 0.39          |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 445   |\n",
      "|    iterations      | 85    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 16065 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 16254        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008531268 |\n",
      "|    clip_fraction        | 0.09         |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.347       |\n",
      "|    explained_variance   | 0.921        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.177        |\n",
      "|    n_updates            | 425          |\n",
      "|    policy_gradient_loss | 0.00129      |\n",
      "|    value_loss           | 0.203        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 36            |\n",
      "|    total_timesteps      | 16443         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047870705 |\n",
      "|    clip_fraction        | 0.117         |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.375        |\n",
      "|    explained_variance   | 0.902         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.0914        |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.00223      |\n",
      "|    value_loss           | 0.263         |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=238.40 +/- 25.11\n",
      "Episode length: 238.40 +/- 25.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 238          |\n",
      "|    mean_reward          | 238          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041523087 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | -0.0417      |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00247      |\n",
      "|    n_updates            | 435          |\n",
      "|    policy_gradient_loss | -0.00366     |\n",
      "|    value_loss           | 0.00421      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 448   |\n",
      "|    iterations      | 88    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 16632 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 16821        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011075297 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.374       |\n",
      "|    explained_variance   | 0.914        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.139        |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.000669     |\n",
      "|    value_loss           | 0.242        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=211.60 +/- 22.01\n",
      "Episode length: 211.60 +/- 22.01\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 212           |\n",
      "|    mean_reward          | 212           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 17000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031939268 |\n",
      "|    clip_fraction        | 0.0677        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.379        |\n",
      "|    explained_variance   | 0.906         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.0451        |\n",
      "|    n_updates            | 445           |\n",
      "|    policy_gradient_loss | 0.000824      |\n",
      "|    value_loss           | 0.218         |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 449   |\n",
      "|    iterations      | 90    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 17010 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 17199        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010327158 |\n",
      "|    clip_fraction        | 0.0749       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.399       |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0401       |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    value_loss           | 0.105        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 455          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 17388        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034380017 |\n",
      "|    clip_fraction        | 0.0805       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.1          |\n",
      "|    n_updates            | 455          |\n",
      "|    policy_gradient_loss | -0.00305     |\n",
      "|    value_loss           | 0.354        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=262.00 +/- 25.93\n",
      "Episode length: 262.00 +/- 25.93\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 262         |\n",
      "|    mean_reward          | 262         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 17500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003672689 |\n",
      "|    clip_fraction        | 0.0808      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00415     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00487    |\n",
      "|    value_loss           | 0.0284      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 451   |\n",
      "|    iterations      | 93    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 17577 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 454         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 17766       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027095541 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.36       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.059       |\n",
      "|    n_updates            | 465         |\n",
      "|    policy_gradient_loss | -0.00665    |\n",
      "|    value_loss           | 0.291       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 457          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 17955        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018704217 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0559       |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 0.262        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=203.80 +/- 23.63\n",
      "Episode length: 203.80 +/- 23.63\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 204          |\n",
      "|    mean_reward          | 204          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017511925 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.373       |\n",
      "|    explained_variance   | 0.978        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.023        |\n",
      "|    n_updates            | 475          |\n",
      "|    policy_gradient_loss | 0.00106      |\n",
      "|    value_loss           | 0.0782       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 455   |\n",
      "|    iterations      | 96    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 18144 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 18333       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011032243 |\n",
      "|    clip_fraction        | 0.0931      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0462      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00372    |\n",
      "|    value_loss           | 0.0722      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=191.20 +/- 15.22\n",
      "Episode length: 191.20 +/- 15.22\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 191          |\n",
      "|    mean_reward          | 191          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013121465 |\n",
      "|    clip_fraction        | 0.0707       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.332       |\n",
      "|    explained_variance   | 0.926        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.107        |\n",
      "|    n_updates            | 485          |\n",
      "|    policy_gradient_loss | 0.00611      |\n",
      "|    value_loss           | 0.172        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 456   |\n",
      "|    iterations      | 98    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 18522 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 459           |\n",
      "|    iterations           | 99            |\n",
      "|    time_elapsed         | 40            |\n",
      "|    total_timesteps      | 18711         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042604085 |\n",
      "|    clip_fraction        | 0.0488        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.333        |\n",
      "|    explained_variance   | 0.923         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.0558        |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | -0.000319     |\n",
      "|    value_loss           | 0.203         |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 18900       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000429019 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.43        |\n",
      "|    n_updates            | 495         |\n",
      "|    policy_gradient_loss | 8.62e-06    |\n",
      "|    value_loss           | 0.58        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=179.40 +/- 16.14\n",
      "Episode length: 179.40 +/- 16.14\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 179        |\n",
      "|    mean_reward          | 179        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00946785 |\n",
      "|    clip_fraction        | 0.0715     |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.312     |\n",
      "|    explained_variance   | 0.896      |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.106      |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.00165   |\n",
      "|    value_loss           | 0.256      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 460   |\n",
      "|    iterations      | 101   |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 19089 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 102        |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 19278      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01210649 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.355     |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.0232     |\n",
      "|    n_updates            | 505        |\n",
      "|    policy_gradient_loss | -0.00757   |\n",
      "|    value_loss           | 0.0675     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 103        |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 19467      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00179341 |\n",
      "|    clip_fraction        | 0.0327     |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.373     |\n",
      "|    explained_variance   | 0.935      |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00139   |\n",
      "|    value_loss           | 0.101      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=187.00 +/- 25.51\n",
      "Episode length: 187.00 +/- 25.51\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 187          |\n",
      "|    mean_reward          | 187          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 19500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015608548 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0232       |\n",
      "|    n_updates            | 515          |\n",
      "|    policy_gradient_loss | -0.00069     |\n",
      "|    value_loss           | 0.0454       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 463   |\n",
      "|    iterations      | 104   |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 19656 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 19845       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003149613 |\n",
      "|    clip_fraction        | 0.0901      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00248    |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0034     |\n",
      "|    value_loss           | 0.143       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=181.20 +/- 18.54\n",
      "Episode length: 181.20 +/- 18.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 181          |\n",
      "|    mean_reward          | 181          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007316721 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.326       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00477      |\n",
      "|    n_updates            | 525          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 0.033        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 463   |\n",
      "|    iterations      | 106   |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 20034 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 20223        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010505852 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.313       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.000797    |\n",
      "|    value_loss           | 0.036        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 468          |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 20412        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023348301 |\n",
      "|    clip_fraction        | 0.0652       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00225     |\n",
      "|    n_updates            | 535          |\n",
      "|    policy_gradient_loss | -0.00365     |\n",
      "|    value_loss           | 0.0286       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=174.80 +/- 15.10\n",
      "Episode length: 174.80 +/- 15.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 175         |\n",
      "|    mean_reward          | 175         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001700821 |\n",
      "|    clip_fraction        | 0.0888      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00347    |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 109   |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 20601 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 469          |\n",
      "|    iterations           | 110          |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 20790        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019731831 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00708     |\n",
      "|    n_updates            | 545          |\n",
      "|    policy_gradient_loss | -0.00325     |\n",
      "|    value_loss           | 0.01         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 471          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 20979        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023278017 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.026        |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00475     |\n",
      "|    value_loss           | 0.0653       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=178.40 +/- 13.09\n",
      "Episode length: 178.40 +/- 13.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 178          |\n",
      "|    mean_reward          | 178          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 21000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024506345 |\n",
      "|    clip_fraction        | 0.0822       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0596       |\n",
      "|    n_updates            | 555          |\n",
      "|    policy_gradient_loss | -0.000791    |\n",
      "|    value_loss           | 0.249        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 470   |\n",
      "|    iterations      | 112   |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 21168 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 472           |\n",
      "|    iterations           | 113           |\n",
      "|    time_elapsed         | 45            |\n",
      "|    total_timesteps      | 21357         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053876144 |\n",
      "|    clip_fraction        | 0.017         |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.309        |\n",
      "|    explained_variance   | 0.972         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00703       |\n",
      "|    n_updates            | 560           |\n",
      "|    policy_gradient_loss | -0.000605     |\n",
      "|    value_loss           | 0.0762        |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=168.60 +/- 19.09\n",
      "Episode length: 168.60 +/- 19.09\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 169         |\n",
      "|    mean_reward          | 169         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 21500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005306182 |\n",
      "|    clip_fraction        | 0.0549      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.31       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0748      |\n",
      "|    n_updates            | 565         |\n",
      "|    policy_gradient_loss | -0.00634    |\n",
      "|    value_loss           | 0.413       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 471   |\n",
      "|    iterations      | 114   |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 21546 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 473          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 21735        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023878652 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.266       |\n",
      "|    explained_variance   | 0.811        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.11         |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 0.241        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 476          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 21924        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005390479 |\n",
      "|    clip_fraction        | 0.0064       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.291       |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0402       |\n",
      "|    n_updates            | 575          |\n",
      "|    policy_gradient_loss | 0.000706     |\n",
      "|    value_loss           | 0.0677       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=146.00 +/- 4.98\n",
      "Episode length: 146.00 +/- 4.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 146         |\n",
      "|    mean_reward          | 146         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004614976 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.268      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    value_loss           | 0.0882      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 475   |\n",
      "|    iterations      | 117   |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 22113 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 118         |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 22302       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008723733 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0103      |\n",
      "|    n_updates            | 585         |\n",
      "|    policy_gradient_loss | 0.00493     |\n",
      "|    value_loss           | 0.0216      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 119        |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 22491      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00235339 |\n",
      "|    clip_fraction        | 0.0509     |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.29      |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.061      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | 9.26e-06   |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=125.80 +/- 5.98\n",
      "Episode length: 125.80 +/- 5.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 126         |\n",
      "|    mean_reward          | 126         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008048383 |\n",
      "|    clip_fraction        | 0.069       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.302      |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0437      |\n",
      "|    n_updates            | 595         |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    value_loss           | 0.113       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 120   |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 22680 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 22869        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010937607 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.328       |\n",
      "|    explained_variance   | 0.965        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0394       |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000236    |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=137.60 +/- 14.55\n",
      "Episode length: 137.60 +/- 14.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 138          |\n",
      "|    mean_reward          | 138          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014835028 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0137       |\n",
      "|    n_updates            | 605          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 0.0388       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 478   |\n",
      "|    iterations      | 122   |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 23058 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 23247        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030583579 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.362       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00346      |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00439     |\n",
      "|    value_loss           | 0.0237       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 124        |\n",
      "|    time_elapsed         | 48         |\n",
      "|    total_timesteps      | 23436      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02796246 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.384     |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.0387     |\n",
      "|    n_updates            | 615        |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    value_loss           | 0.0857     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=145.20 +/- 11.67\n",
      "Episode length: 145.20 +/- 11.67\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 145          |\n",
      "|    mean_reward          | 145          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 23500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012821871 |\n",
      "|    clip_fraction        | 0.135        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.421       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00869      |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    value_loss           | 0.038        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 481   |\n",
      "|    iterations      | 125   |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 23625 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 482          |\n",
      "|    iterations           | 126          |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 23814        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009435156 |\n",
      "|    clip_fraction        | 0.0603       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.399       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0136       |\n",
      "|    n_updates            | 625          |\n",
      "|    policy_gradient_loss | -0.00381     |\n",
      "|    value_loss           | 0.0578       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=127.60 +/- 9.46\n",
      "Episode length: 127.60 +/- 9.46\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 128        |\n",
      "|    mean_reward          | 128        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01814652 |\n",
      "|    clip_fraction        | 0.23       |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.00478    |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.00888   |\n",
      "|    value_loss           | 0.0277     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 127   |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 24003 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 24192        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042255134 |\n",
      "|    clip_fraction        | 0.0946       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.412       |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00827      |\n",
      "|    n_updates            | 635          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 0.0352       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 24381       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006460478 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00137     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | 0.00227     |\n",
      "|    value_loss           | 0.00902     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=144.60 +/- 14.05\n",
      "Episode length: 144.60 +/- 14.05\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 145          |\n",
      "|    mean_reward          | 145          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028802662 |\n",
      "|    clip_fraction        | 0.0634       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.337       |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0312       |\n",
      "|    n_updates            | 645          |\n",
      "|    policy_gradient_loss | 0.00144      |\n",
      "|    value_loss           | 0.0492       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 130   |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 24570 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 475          |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 24759        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030452788 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.298       |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0183       |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.000174    |\n",
      "|    value_loss           | 0.0246       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 132         |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 24948       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008757488 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00209    |\n",
      "|    n_updates            | 655         |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    value_loss           | 0.0199      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=161.40 +/- 13.50\n",
      "Episode length: 161.40 +/- 13.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 161          |\n",
      "|    mean_reward          | 161          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015006772 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.34        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.0102      |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000445    |\n",
      "|    value_loss           | 0.0155       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 133   |\n",
      "|    time_elapsed    | 53    |\n",
      "|    total_timesteps | 25137 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 25326       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004163514 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.31       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.000818    |\n",
      "|    n_updates            | 665         |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    value_loss           | 0.00383     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=166.00 +/- 14.04\n",
      "Episode length: 166.00 +/- 14.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 166          |\n",
      "|    mean_reward          | 166          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013064328 |\n",
      "|    clip_fraction        | 0.0223       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.335       |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0164       |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000418    |\n",
      "|    value_loss           | 0.0454       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 459   |\n",
      "|    iterations      | 135   |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 25515 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 25704        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007130401 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.322       |\n",
      "|    explained_variance   | 0.979        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0325       |\n",
      "|    n_updates            | 675          |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 0.0375       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 137         |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 25893       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008570786 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.304      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 0.0158      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=166.40 +/- 23.21\n",
      "Episode length: 166.40 +/- 23.21\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 166          |\n",
      "|    mean_reward          | 166          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009731036 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.27        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00679      |\n",
      "|    n_updates            | 685          |\n",
      "|    policy_gradient_loss | 0.00161      |\n",
      "|    value_loss           | 0.0086       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 462   |\n",
      "|    iterations      | 138   |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 26082 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 26271       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000813535 |\n",
      "|    clip_fraction        | 0.00432     |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00162     |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.00072     |\n",
      "|    value_loss           | 0.00691     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 26460       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006523041 |\n",
      "|    clip_fraction        | 0.0476      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.262      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00134     |\n",
      "|    n_updates            | 695         |\n",
      "|    policy_gradient_loss | 0.00202     |\n",
      "|    value_loss           | 0.0168      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=145.60 +/- 10.29\n",
      "Episode length: 145.60 +/- 10.29\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 146       |\n",
      "|    mean_reward          | 146       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 26500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0165765 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.138     |\n",
      "|    entropy_loss         | -0.271    |\n",
      "|    explained_variance   | 0.977     |\n",
      "|    learning_rate        | 0.00259   |\n",
      "|    loss                 | -0.0107   |\n",
      "|    n_updates            | 700       |\n",
      "|    policy_gradient_loss | -0.00898  |\n",
      "|    value_loss           | 0.0396    |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 463   |\n",
      "|    iterations      | 141   |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 26649 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 142          |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 26838        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014922217 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.284       |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 7.47e-05     |\n",
      "|    n_updates            | 705          |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 0.0197       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=148.80 +/- 11.89\n",
      "Episode length: 148.80 +/- 11.89\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 149           |\n",
      "|    mean_reward          | 149           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 27000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032306928 |\n",
      "|    clip_fraction        | 0.0222        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.268        |\n",
      "|    explained_variance   | 0.998         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00577       |\n",
      "|    n_updates            | 710           |\n",
      "|    policy_gradient_loss | 0.00206       |\n",
      "|    value_loss           | 0.00504       |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 464   |\n",
      "|    iterations      | 143   |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 27027 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 465          |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 27216        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008579131 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.255       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00643      |\n",
      "|    n_updates            | 715          |\n",
      "|    policy_gradient_loss | 0.000708     |\n",
      "|    value_loss           | 0.014        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 27405       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012117314 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.248      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00973    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00561    |\n",
      "|    value_loss           | 0.0261      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=144.40 +/- 6.65\n",
      "Episode length: 144.40 +/- 6.65\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 144          |\n",
      "|    mean_reward          | 144          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 27500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019383496 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.214       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00457      |\n",
      "|    n_updates            | 725          |\n",
      "|    policy_gradient_loss | 0.00112      |\n",
      "|    value_loss           | 0.00395      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 466   |\n",
      "|    iterations      | 146   |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 27594 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 147         |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 27783       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018061467 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.212      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    value_loss           | 0.00823     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 27972        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051019234 |\n",
      "|    clip_fraction        | 0.0688       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.207       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0161       |\n",
      "|    n_updates            | 735          |\n",
      "|    policy_gradient_loss | 0.000522     |\n",
      "|    value_loss           | 0.0576       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=143.20 +/- 9.28\n",
      "Episode length: 143.20 +/- 9.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 143          |\n",
      "|    mean_reward          | 143          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015089818 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.229       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00156     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | 0.000236     |\n",
      "|    value_loss           | 0.0165       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 469   |\n",
      "|    iterations      | 149   |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 28161 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 28350       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053146034 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.221      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 745         |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    value_loss           | 0.00749     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=142.20 +/- 9.83\n",
      "Episode length: 142.20 +/- 9.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 142          |\n",
      "|    mean_reward          | 142          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029155149 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0759       |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00859     |\n",
      "|    value_loss           | 0.0843       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 470   |\n",
      "|    iterations      | 151   |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 28539 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 471           |\n",
      "|    iterations           | 152           |\n",
      "|    time_elapsed         | 60            |\n",
      "|    total_timesteps      | 28728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00070871087 |\n",
      "|    clip_fraction        | 0.0169        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.235        |\n",
      "|    explained_variance   | 0.994         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | -0.000106     |\n",
      "|    n_updates            | 755           |\n",
      "|    policy_gradient_loss | -0.000681     |\n",
      "|    value_loss           | 0.014         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 473          |\n",
      "|    iterations           | 153          |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 28917        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011748271 |\n",
      "|    clip_fraction        | 0.0812       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0327       |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.000328    |\n",
      "|    value_loss           | 0.0627       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=142.40 +/- 12.47\n",
      "Episode length: 142.40 +/- 12.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 142         |\n",
      "|    mean_reward          | 142         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 29000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008965414 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.23       |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.002       |\n",
      "|    n_updates            | 765         |\n",
      "|    policy_gradient_loss | -0.00941    |\n",
      "|    value_loss           | 0.00635     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 473   |\n",
      "|    iterations      | 154   |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 29106 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 474           |\n",
      "|    iterations           | 155           |\n",
      "|    time_elapsed         | 61            |\n",
      "|    total_timesteps      | 29295         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025827257 |\n",
      "|    clip_fraction        | 0.0361        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.222        |\n",
      "|    explained_variance   | 0.995         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.0012        |\n",
      "|    n_updates            | 770           |\n",
      "|    policy_gradient_loss | 0.00179       |\n",
      "|    value_loss           | 0.0116        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 476          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 29484        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017105807 |\n",
      "|    clip_fraction        | 0.0759       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0066       |\n",
      "|    n_updates            | 775          |\n",
      "|    policy_gradient_loss | 0.000795     |\n",
      "|    value_loss           | 0.0175       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=139.80 +/- 11.30\n",
      "Episode length: 139.80 +/- 11.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 140          |\n",
      "|    mean_reward          | 140          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 29500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052690455 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.204       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00144      |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 0.00543      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 474   |\n",
      "|    iterations      | 157   |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 29673 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 476          |\n",
      "|    iterations           | 158          |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 29862        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015306808 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00144     |\n",
      "|    n_updates            | 785          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 0.00461      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=137.60 +/- 2.58\n",
      "Episode length: 137.60 +/- 2.58\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 138           |\n",
      "|    mean_reward          | 138           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 30000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031156666 |\n",
      "|    clip_fraction        | 0.0296        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.195        |\n",
      "|    explained_variance   | 0.997         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | -0.00244      |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | 0.00131       |\n",
      "|    value_loss           | 0.00389       |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 476   |\n",
      "|    iterations      | 159   |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 30051 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 477           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 63            |\n",
      "|    total_timesteps      | 30240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032484098 |\n",
      "|    clip_fraction        | 0.0804        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.221        |\n",
      "|    explained_variance   | 0.995         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.0171        |\n",
      "|    n_updates            | 795           |\n",
      "|    policy_gradient_loss | -0.00328      |\n",
      "|    value_loss           | 0.022         |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 30429       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001165691 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.213      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00224     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | 0.00136     |\n",
      "|    value_loss           | 0.00287     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=135.60 +/- 13.98\n",
      "Episode length: 135.60 +/- 13.98\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 136         |\n",
      "|    mean_reward          | 136         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014787421 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.208      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 805         |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    value_loss           | 0.00814     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 479   |\n",
      "|    iterations      | 162   |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 30618 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 163          |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 30807        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032628868 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.215       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00449     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | 0.000541     |\n",
      "|    value_loss           | 0.00611      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 481          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 30996        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025737921 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0122       |\n",
      "|    n_updates            | 815          |\n",
      "|    policy_gradient_loss | 0.000708     |\n",
      "|    value_loss           | 0.0126       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=127.80 +/- 19.19\n",
      "Episode length: 127.80 +/- 19.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 128          |\n",
      "|    mean_reward          | 128          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015554368 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00223      |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | 0.000233     |\n",
      "|    value_loss           | 0.00195      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 481   |\n",
      "|    iterations      | 165   |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 31185 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 482          |\n",
      "|    iterations           | 166          |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 31374        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011497464 |\n",
      "|    clip_fraction        | 0.0544       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.19        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00913     |\n",
      "|    n_updates            | 825          |\n",
      "|    policy_gradient_loss | 0.000819     |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=135.00 +/- 11.52\n",
      "Episode length: 135.00 +/- 11.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 135          |\n",
      "|    mean_reward          | 135          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 31500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014688849 |\n",
      "|    clip_fraction        | 0.0604       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.177       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00978      |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | 0.00153      |\n",
      "|    value_loss           | 0.00734      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 482   |\n",
      "|    iterations      | 167   |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 31563 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 484          |\n",
      "|    iterations           | 168          |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 31752        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009552506 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.17        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00606     |\n",
      "|    n_updates            | 835          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    value_loss           | 0.00157      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 485           |\n",
      "|    iterations           | 169           |\n",
      "|    time_elapsed         | 65            |\n",
      "|    total_timesteps      | 31941         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00077480706 |\n",
      "|    clip_fraction        | 0.0234        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.163        |\n",
      "|    explained_variance   | 0.998         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00416       |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -0.00339      |\n",
      "|    value_loss           | 0.0097        |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=141.20 +/- 18.29\n",
      "Episode length: 141.20 +/- 18.29\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 141          |\n",
      "|    mean_reward          | 141          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017203201 |\n",
      "|    clip_fraction        | 0.0616       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.0166      |\n",
      "|    n_updates            | 845          |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    value_loss           | 0.00124      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 485   |\n",
      "|    iterations      | 170   |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 32130 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 486         |\n",
      "|    iterations           | 171         |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 32319       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001435238 |\n",
      "|    clip_fraction        | 0.0689      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.17       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00195     |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.000205   |\n",
      "|    value_loss           | 0.00186     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=149.60 +/- 19.65\n",
      "Episode length: 149.60 +/- 19.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 150         |\n",
      "|    mean_reward          | 150         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005181891 |\n",
      "|    clip_fraction        | 0.0235      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.156      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0846      |\n",
      "|    n_updates            | 855         |\n",
      "|    policy_gradient_loss | 0.00683     |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 486   |\n",
      "|    iterations      | 172   |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 32508 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 487         |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 32697       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001609746 |\n",
      "|    clip_fraction        | 0.0255      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.152      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.00219     |\n",
      "|    value_loss           | 0.00416     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 32886        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048463717 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.13        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00723      |\n",
      "|    n_updates            | 865          |\n",
      "|    policy_gradient_loss | 0.000355     |\n",
      "|    value_loss           | 0.00362      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=135.40 +/- 9.05\n",
      "Episode length: 135.40 +/- 9.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 135         |\n",
      "|    mean_reward          | 135         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 33000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003619299 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00221    |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    value_loss           | 0.00361     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 487   |\n",
      "|    iterations      | 175   |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 33075 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 489          |\n",
      "|    iterations           | 176          |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 33264        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033817962 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.117       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0089       |\n",
      "|    n_updates            | 875          |\n",
      "|    policy_gradient_loss | 0.0018       |\n",
      "|    value_loss           | 0.00599      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 490          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 33453        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033505575 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.105       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -6.52e-05    |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | 0.000235     |\n",
      "|    value_loss           | 0.00893      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=131.60 +/- 5.75\n",
      "Episode length: 131.60 +/- 5.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 132          |\n",
      "|    mean_reward          | 132          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 33500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022473552 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00673     |\n",
      "|    n_updates            | 885          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 0.000969     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 490   |\n",
      "|    iterations      | 178   |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 33642 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 491           |\n",
      "|    iterations           | 179           |\n",
      "|    time_elapsed         | 68            |\n",
      "|    total_timesteps      | 33831         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020159963 |\n",
      "|    clip_fraction        | 0.019         |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.105        |\n",
      "|    explained_variance   | 0.998         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00307       |\n",
      "|    n_updates            | 890           |\n",
      "|    policy_gradient_loss | -0.00113      |\n",
      "|    value_loss           | 0.00469       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=142.60 +/- 11.32\n",
      "Episode length: 142.60 +/- 11.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 143         |\n",
      "|    mean_reward          | 143         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012533441 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.108      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00121     |\n",
      "|    n_updates            | 895         |\n",
      "|    policy_gradient_loss | -0.00153    |\n",
      "|    value_loss           | 0.0022      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 491   |\n",
      "|    iterations      | 180   |\n",
      "|    time_elapsed    | 69    |\n",
      "|    total_timesteps | 34020 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 492          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 34209        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009775618 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0207       |\n",
      "|    n_updates            | 900          |\n",
      "|    policy_gradient_loss | -1.29e-05    |\n",
      "|    value_loss           | 0.0149       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 34398       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010425106 |\n",
      "|    clip_fraction        | 0.0532      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0298      |\n",
      "|    n_updates            | 905         |\n",
      "|    policy_gradient_loss | 0.00883     |\n",
      "|    value_loss           | 0.0042      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=148.40 +/- 24.69\n",
      "Episode length: 148.40 +/- 24.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 148         |\n",
      "|    mean_reward          | 148         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002008524 |\n",
      "|    clip_fraction        | 0.0309      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0193      |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | 0.00564     |\n",
      "|    value_loss           | 0.00505     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 493   |\n",
      "|    iterations      | 183   |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 34587 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 494         |\n",
      "|    iterations           | 184         |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 34776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000885926 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.128      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00581    |\n",
      "|    n_updates            | 915         |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 0.00262     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 34965        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007468487 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.14        |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00444      |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -0.000519    |\n",
      "|    value_loss           | 0.00393      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=131.60 +/- 10.93\n",
      "Episode length: 131.60 +/- 10.93\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 132          |\n",
      "|    mean_reward          | 132          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 35000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007437254 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00182      |\n",
      "|    n_updates            | 925          |\n",
      "|    policy_gradient_loss | 0.000143     |\n",
      "|    value_loss           | 0.00393      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 495   |\n",
      "|    iterations      | 186   |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 35154 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 497         |\n",
      "|    iterations           | 187         |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 35343       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003340494 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00763    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00259    |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=121.60 +/- 7.39\n",
      "Episode length: 121.60 +/- 7.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 122         |\n",
      "|    mean_reward          | 122         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 35500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026644537 |\n",
      "|    clip_fraction        | 0.0901      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.161      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 935         |\n",
      "|    policy_gradient_loss | 0.00038     |\n",
      "|    value_loss           | 0.00299     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 496   |\n",
      "|    iterations      | 188   |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 35532 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 189          |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 35721        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034812018 |\n",
      "|    clip_fraction        | 0.0467       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.141       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00898     |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 0.00559      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 190          |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 35910        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019531167 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.142       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0313       |\n",
      "|    n_updates            | 945          |\n",
      "|    policy_gradient_loss | 0.00384      |\n",
      "|    value_loss           | 0.00723      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=135.40 +/- 9.46\n",
      "Episode length: 135.40 +/- 9.46\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 135           |\n",
      "|    mean_reward          | 135           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 36000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097069144 |\n",
      "|    clip_fraction        | 0.0127        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.163        |\n",
      "|    explained_variance   | 1             |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00123       |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | 0.000835      |\n",
      "|    value_loss           | 0.00162       |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 499   |\n",
      "|    iterations      | 191   |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 36099 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 192          |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 36288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012071995 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.159       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00196     |\n",
      "|    n_updates            | 955          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    value_loss           | 0.00238      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 501           |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 72            |\n",
      "|    total_timesteps      | 36477         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090841576 |\n",
      "|    clip_fraction        | 0.033         |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.17         |\n",
      "|    explained_variance   | 1             |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | -0.0003       |\n",
      "|    n_updates            | 960           |\n",
      "|    policy_gradient_loss | 0.00358       |\n",
      "|    value_loss           | 0.000696      |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=142.20 +/- 20.19\n",
      "Episode length: 142.20 +/- 20.19\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 142          |\n",
      "|    mean_reward          | 142          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 36500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060374006 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.168       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0203       |\n",
      "|    n_updates            | 965          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 0.00902      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 501   |\n",
      "|    iterations      | 194   |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 36666 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 36855        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013072621 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.0026       |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | 0.000234     |\n",
      "|    value_loss           | 0.00257      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=183.20 +/- 43.53\n",
      "Episode length: 183.20 +/- 43.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 183         |\n",
      "|    mean_reward          | 183         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 37000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008769986 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.163      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0106      |\n",
      "|    n_updates            | 975         |\n",
      "|    policy_gradient_loss | 0.0017      |\n",
      "|    value_loss           | 0.000928    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 501   |\n",
      "|    iterations      | 196   |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 37044 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 37233        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030952322 |\n",
      "|    clip_fraction        | 0.037        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.18        |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00604      |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | 0.00046      |\n",
      "|    value_loss           | 0.00121      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 504          |\n",
      "|    iterations           | 198          |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 37422        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012831799 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.152       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.000736    |\n",
      "|    n_updates            | 985          |\n",
      "|    policy_gradient_loss | 0.000272     |\n",
      "|    value_loss           | 0.00145      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=172.00 +/- 19.54\n",
      "Episode length: 172.00 +/- 19.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 172          |\n",
      "|    mean_reward          | 172          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 37500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052086557 |\n",
      "|    clip_fraction        | 0.0341       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.164       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00789     |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | 0.00166      |\n",
      "|    value_loss           | 0.00994      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 503   |\n",
      "|    iterations      | 199   |\n",
      "|    time_elapsed    | 74    |\n",
      "|    total_timesteps | 37611 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 504           |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 74            |\n",
      "|    total_timesteps      | 37800         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029188886 |\n",
      "|    clip_fraction        | 0.0286        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.153        |\n",
      "|    explained_variance   | 0.999         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00188       |\n",
      "|    n_updates            | 995           |\n",
      "|    policy_gradient_loss | 0.000336      |\n",
      "|    value_loss           | 0.00186       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 505         |\n",
      "|    iterations           | 201         |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 37989       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001698895 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.132      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00544    |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=140.80 +/- 11.97\n",
      "Episode length: 140.80 +/- 11.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 141         |\n",
      "|    mean_reward          | 141         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005572094 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.117      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00361     |\n",
      "|    n_updates            | 1005        |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    value_loss           | 0.0132      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 505   |\n",
      "|    iterations      | 202   |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 38178 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 506         |\n",
      "|    iterations           | 203         |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 38367       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078441136 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.122      |\n",
      "|    explained_variance   | 1           |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | 0.00267     |\n",
      "|    value_loss           | 0.000534    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=142.80 +/- 23.58\n",
      "Episode length: 142.80 +/- 23.58\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 143           |\n",
      "|    mean_reward          | 143           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 38500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065541797 |\n",
      "|    clip_fraction        | 0.0276        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.129        |\n",
      "|    explained_variance   | 0.999         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.0051        |\n",
      "|    n_updates            | 1015          |\n",
      "|    policy_gradient_loss | 0.000791      |\n",
      "|    value_loss           | 0.00228       |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 505   |\n",
      "|    iterations      | 204   |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 38556 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 38745        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058106943 |\n",
      "|    clip_fraction        | 0.0435       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.138       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.0112      |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    value_loss           | 0.00324      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 508         |\n",
      "|    iterations           | 206         |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 38934       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001348149 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.14       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.005       |\n",
      "|    n_updates            | 1025        |\n",
      "|    policy_gradient_loss | 0.000442    |\n",
      "|    value_loss           | 0.0137      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=152.00 +/- 19.82\n",
      "Episode length: 152.00 +/- 19.82\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 152          |\n",
      "|    mean_reward          | 152          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 39000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007419866 |\n",
      "|    clip_fraction        | 0.00937      |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.126       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00273      |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    value_loss           | 0.00202      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 507   |\n",
      "|    iterations      | 207   |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 39123 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 508           |\n",
      "|    iterations           | 208           |\n",
      "|    time_elapsed         | 77            |\n",
      "|    total_timesteps      | 39312         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028621525 |\n",
      "|    clip_fraction        | 0.0116        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.117        |\n",
      "|    explained_variance   | 0.997         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00126       |\n",
      "|    n_updates            | 1035          |\n",
      "|    policy_gradient_loss | -0.00134      |\n",
      "|    value_loss           | 0.00467       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=133.60 +/- 15.63\n",
      "Episode length: 133.60 +/- 15.63\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 134           |\n",
      "|    mean_reward          | 134           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 39500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013154498 |\n",
      "|    clip_fraction        | 0.0213        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.101        |\n",
      "|    explained_variance   | 0.998         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | -0.00209      |\n",
      "|    n_updates            | 1040          |\n",
      "|    policy_gradient_loss | 8.79e-05      |\n",
      "|    value_loss           | 0.00456       |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 508   |\n",
      "|    iterations      | 209   |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 39501 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 509         |\n",
      "|    iterations           | 210         |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 39690       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001630158 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.101      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00182     |\n",
      "|    n_updates            | 1045        |\n",
      "|    policy_gradient_loss | -0.00363    |\n",
      "|    value_loss           | 0.000977    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 211          |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 39879        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018307073 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.121       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.0044      |\n",
      "|    n_updates            | 1050         |\n",
      "|    policy_gradient_loss | -0.000172    |\n",
      "|    value_loss           | 0.00332      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=136.80 +/- 14.55\n",
      "Episode length: 136.80 +/- 14.55\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 137          |\n",
      "|    mean_reward          | 137          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014313022 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.0959      |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00136      |\n",
      "|    n_updates            | 1055         |\n",
      "|    policy_gradient_loss | 0.00231      |\n",
      "|    value_loss           | 0.00779      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 510   |\n",
      "|    iterations      | 212   |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 40068 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 213          |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 40257        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025725574 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.104       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00393      |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    value_loss           | 0.00779      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 512           |\n",
      "|    iterations           | 214           |\n",
      "|    time_elapsed         | 78            |\n",
      "|    total_timesteps      | 40446         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051338895 |\n",
      "|    clip_fraction        | 0.0338        |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.0985       |\n",
      "|    explained_variance   | 0.999         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | -0.00696      |\n",
      "|    n_updates            | 1065          |\n",
      "|    policy_gradient_loss | -0.0027       |\n",
      "|    value_loss           | 0.00302       |\n",
      "-------------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=138.60 +/- 9.54\n",
      "Episode length: 138.60 +/- 9.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 139         |\n",
      "|    mean_reward          | 139         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013486832 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.0878     |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00588    |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.000263    |\n",
      "|    value_loss           | 0.014       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 215   |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 40635 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 513        |\n",
      "|    iterations           | 216        |\n",
      "|    time_elapsed         | 79         |\n",
      "|    total_timesteps      | 40824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00181395 |\n",
      "|    clip_fraction        | 0.00854    |\n",
      "|    clip_range           | 0.138      |\n",
      "|    entropy_loss         | -0.0903    |\n",
      "|    explained_variance   | 1          |\n",
      "|    learning_rate        | 0.00259    |\n",
      "|    loss                 | 0.00422    |\n",
      "|    n_updates            | 1075       |\n",
      "|    policy_gradient_loss | -0.000138  |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=119.40 +/- 6.80\n",
      "Episode length: 119.40 +/- 6.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 119         |\n",
      "|    mean_reward          | 119         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036019187 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.0833     |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00547    |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    value_loss           | 0.00241     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 511   |\n",
      "|    iterations      | 217   |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 41013 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 41202       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009833592 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.0926     |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.0081      |\n",
      "|    n_updates            | 1085        |\n",
      "|    policy_gradient_loss | 0.00696     |\n",
      "|    value_loss           | 0.0198      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 41391        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015140254 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.0777      |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | -0.00637     |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 0.00262      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=142.40 +/- 19.45\n",
      "Episode length: 142.40 +/- 19.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 142         |\n",
      "|    mean_reward          | 142         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 41500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002018505 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.0864     |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00015    |\n",
      "|    n_updates            | 1095        |\n",
      "|    policy_gradient_loss | -0.000546   |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 513   |\n",
      "|    iterations      | 220   |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 41580 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 221         |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 41769       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018779777 |\n",
      "|    clip_fraction        | 0.0677      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.075      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | 0.00433     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 0.00244     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 516         |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 41958       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026610218 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.0852     |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00457    |\n",
      "|    n_updates            | 1105        |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 0.0034      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=124.60 +/- 11.07\n",
      "Episode length: 124.60 +/- 11.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 125         |\n",
      "|    mean_reward          | 125         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009024811 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.138       |\n",
      "|    entropy_loss         | -0.0655     |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00259     |\n",
      "|    loss                 | -0.00225    |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    value_loss           | 0.00911     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 223   |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 42147 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 224          |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 42336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021651897 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.138        |\n",
      "|    entropy_loss         | -0.0823      |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.00259      |\n",
      "|    loss                 | 0.00454      |\n",
      "|    n_updates            | 1115         |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 0.016        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=132.60 +/- 13.03\n",
      "Episode length: 132.60 +/- 13.03\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 133           |\n",
      "|    mean_reward          | 133           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 42500         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055784563 |\n",
      "|    clip_fraction        | 0.00739       |\n",
      "|    clip_range           | 0.138         |\n",
      "|    entropy_loss         | -0.0762       |\n",
      "|    explained_variance   | 0.999         |\n",
      "|    learning_rate        | 0.00259       |\n",
      "|    loss                 | 0.00108       |\n",
      "|    n_updates            | 1120          |\n",
      "|    policy_gradient_loss | -0.000476     |\n",
      "|    value_loss           | 0.0038        |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 516   |\n",
      "|    iterations      | 225   |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 42525 |\n",
      "------------------------------\n",
      "Single environment training took 82.38 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>128.9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-sweep-20</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2871tio1' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2871tio1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_140126-2871tio1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1bahrtuk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.198388751321804\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0035612128843734293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9935998995645532\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.909121931564672\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0049397506384296136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 6.175786490883611\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 95409\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_140301-1bahrtuk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/1bahrtuk' target=\"_blank\">serene-sweep-1</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/1bahrtuk' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/1bahrtuk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-4:\n",
      "Process ForkServerProcess-3:\n",
      "Process ForkServerProcess-2:\n",
      "Process ForkServerProcess-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 1229298042\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 1734018814\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 2103710000\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 456337976\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-sweep-1</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/1bahrtuk' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/1bahrtuk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_140301-1bahrtuk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1bahrtuk errored: EOFError()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1bahrtuk errored: EOFError()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5jnaseew with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.27941671028997983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.008131621745903757\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9674789546052766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9263553900546296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007256660913341136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 2.3836737242784025\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1741\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 97049\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_140316-5jnaseew</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/5jnaseew' target=\"_blank\">avid-sweep-2</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/5jnaseew' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/5jnaseew</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 3922265521\n",
      "Process ForkServerProcess-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 1255004200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 1001855517\n",
      "Process ForkServerProcess-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 3996730458\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-sweep-2</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/5jnaseew' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/5jnaseew</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_140316-5jnaseew/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5jnaseew errored: EOFError()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5jnaseew errored: EOFError()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m7u27qj0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.1548381238349878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.00863732323016807\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9040056905994543\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9354049976651054\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007499545892770019\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 7.435089408020336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 590\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 51464\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_140331-m7u27qj0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/m7u27qj0' target=\"_blank\">dazzling-sweep-3</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/wtg12j8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/m7u27qj0' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/m7u27qj0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-10:\n",
      "Process ForkServerProcess-9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 2008496853\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 4214016148\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 2752014004\n",
      "Process ForkServerProcess-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 25, in _worker\n",
      "    env = env_fn_wrapper.var()\n",
      "  File \"/var/folders/w3/204l8n9n6g1c3vgzxxhhh7500000gn/T/ipykernel_87305/3773973735.py\", line 4, in _init\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/core.py\", line 301, in seed\n",
      "    return self.env.seed(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py\", line 100, in seed\n",
      "    self.np_random, seed = seeding.np_random(seed)\n",
      "  File \"/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/gym/utils/seeding.py\", line 13, in np_random\n",
      "    raise error.Error(\n",
      "gym.error.Error: Seed must be a non-negative integer or omitted, not 2177871239\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-sweep-3</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/m7u27qj0' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/m7u27qj0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_140331-m7u27qj0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run m7u27qj0 errored: EOFError()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run m7u27qj0 errored: EOFError()\n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "# Single\n",
    "wandb.agent(single_sweep_id, function=sweep_agent_single, count=20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9353c688",
   "metadata": {},
   "source": [
    "# Run the sweep: Multi env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "656dfdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cl7g958b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2511996313412903\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.008940710676994899\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.948714940686066\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9043587061935928\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004970508894585218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 7.593603572713287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 179\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 56851\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_142920-cl7g958b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cl7g958b' target=\"_blank\">silver-sweep-1</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cl7g958b' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cl7g958b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3719 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 716  |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 716`, after every 11 untruncated mini-batches, there will be a truncated mini-batch of size 12\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=179 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x14267b520> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x14263c310>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2002        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 0           |\n",
      "|    total_timesteps      | 1432        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008591862 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | -0.00978    |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 6.15        |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.00606    |\n",
      "|    value_loss           | 22.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=45.80 +/- 6.34\n",
      "Episode length: 45.80 +/- 6.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 45.8        |\n",
      "|    mean_reward          | 45.8        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017778661 |\n",
      "|    clip_fraction        | 0.0608      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | -0.139      |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 6.58        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1615 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2148 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1585        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 2864        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010666304 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.0852      |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 4.54        |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    value_loss           | 4.06        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1528        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 3580        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016964925 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.466       |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=119.80 +/- 2.71\n",
      "Episode length: 119.80 +/- 2.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 120         |\n",
      "|    mean_reward          | 120         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020310247 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1333 |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 4296 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1212        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 5012        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008943422 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.946       |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 1.27        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1186        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 5728        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013920051 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.689       |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=210.80 +/- 33.87\n",
      "Episode length: 210.80 +/- 33.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 211         |\n",
      "|    mean_reward          | 211         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013400591 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 0.61        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1112 |\n",
      "|    iterations      | 9    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 6444 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1136        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 7160        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012135637 |\n",
      "|    clip_fraction        | 0.076       |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.433       |\n",
      "|    n_updates            | 81          |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.666       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1145        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 7876        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009071361 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.691       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0842      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    value_loss           | 0.484       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=362.80 +/- 18.32\n",
      "Episode length: 362.80 +/- 18.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 363         |\n",
      "|    mean_reward          | 363         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005725249 |\n",
      "|    clip_fraction        | 0.0259      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.437       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 99          |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1069 |\n",
      "|    iterations      | 12   |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 8592 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1090        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 9308        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007361173 |\n",
      "|    clip_fraction        | 0.0298      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0549      |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    value_loss           | 0.333       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=281.00 +/- 27.03\n",
      "Episode length: 281.00 +/- 27.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 281         |\n",
      "|    mean_reward          | 281         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006715544 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.138       |\n",
      "|    n_updates            | 117         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 0.841       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1020  |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 10024 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1038         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 10740        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045444183 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | 0.592        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.00698      |\n",
      "|    n_updates            | 126          |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    value_loss           | 0.423        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1059        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 11456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010549924 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.456      |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 135         |\n",
      "|    policy_gradient_loss | 0.00101     |\n",
      "|    value_loss           | 0.545       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=388.40 +/- 91.33\n",
      "Episode length: 388.40 +/- 91.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 388         |\n",
      "|    mean_reward          | 388         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008423855 |\n",
      "|    clip_fraction        | 0.0279      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0334      |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.00386    |\n",
      "|    value_loss           | 0.347       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 969   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 12172 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 973          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 12888        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065387315 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.478       |\n",
      "|    explained_variance   | 0.732        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.121        |\n",
      "|    n_updates            | 153          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 0.387        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 986          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 13604        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034275113 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | 0.725        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.355        |\n",
      "|    n_updates            | 162          |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    value_loss           | 0.594        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=355.00 +/- 45.04\n",
      "Episode length: 355.00 +/- 45.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 355          |\n",
      "|    mean_reward          | 355          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048504677 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.801        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.467        |\n",
      "|    n_updates            | 171          |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 0.328        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 945   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 15    |\n",
      "|    total_timesteps | 14320 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 956         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 15036       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003243368 |\n",
      "|    clip_fraction        | 0.0316      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.325       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 0.279       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 961         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 15752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005227022 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0949      |\n",
      "|    n_updates            | 189         |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    value_loss           | 0.231       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=233.20 +/- 28.59\n",
      "Episode length: 233.20 +/- 28.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 233         |\n",
      "|    mean_reward          | 233         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007004073 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 198         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 0.31        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 944   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 16468 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 957          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 17184        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031251453 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.0586       |\n",
      "|    n_updates            | 207          |\n",
      "|    policy_gradient_loss | -0.00523     |\n",
      "|    value_loss           | 0.523        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 959         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 17900       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011268544 |\n",
      "|    clip_fraction        | 0.0454      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    value_loss           | 0.171       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=260.40 +/- 61.44\n",
      "Episode length: 260.40 +/- 61.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 260         |\n",
      "|    mean_reward          | 260         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007962773 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.882       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0768      |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    value_loss           | 0.307       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 922   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 18616 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 928         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 19332       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016207749 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0478      |\n",
      "|    n_updates            | 234         |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    value_loss           | 0.27        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=455.60 +/- 88.80\n",
      "Episode length: 455.60 +/- 88.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 456          |\n",
      "|    mean_reward          | 456          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055242567 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.5         |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 243          |\n",
      "|    policy_gradient_loss | -0.012       |\n",
      "|    value_loss           | 0.218        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 893   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 20048 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 20764        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069458256 |\n",
      "|    clip_fraction        | 0.00593      |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.68         |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.0289       |\n",
      "|    n_updates            | 252          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 0.362        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 852          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 21480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053775976 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0138      |\n",
      "|    n_updates            | 261          |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    value_loss           | 0.0593       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=371.60 +/- 99.95\n",
      "Episode length: 371.60 +/- 99.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 372          |\n",
      "|    mean_reward          | 372          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042460533 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.442       |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.476        |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00516     |\n",
      "|    value_loss           | 0.166        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 835   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 22196 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 843          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 22912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052266526 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | 0.637        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 1.79         |\n",
      "|    n_updates            | 279          |\n",
      "|    policy_gradient_loss | -0.00808     |\n",
      "|    value_loss           | 0.538        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 837         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 23628       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005012413 |\n",
      "|    clip_fraction        | 0.00178     |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0332      |\n",
      "|    n_updates            | 288         |\n",
      "|    policy_gradient_loss | 0.00129     |\n",
      "|    value_loss           | 0.766       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=422.60 +/- 97.87\n",
      "Episode length: 422.60 +/- 97.87\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 423          |\n",
      "|    mean_reward          | 423          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047372063 |\n",
      "|    clip_fraction        | 0.00989      |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.458       |\n",
      "|    explained_variance   | 0.834        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.265        |\n",
      "|    n_updates            | 297          |\n",
      "|    policy_gradient_loss | -7.63e-05    |\n",
      "|    value_loss           | 0.0575       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 805   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 24344 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 815         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 25060       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010484579 |\n",
      "|    clip_fraction        | 0.029       |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0176      |\n",
      "|    n_updates            | 306         |\n",
      "|    policy_gradient_loss | -0.00219    |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 825        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 25776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00229992 |\n",
      "|    clip_fraction        | 0.00584    |\n",
      "|    clip_range           | 0.251      |\n",
      "|    entropy_loss         | -0.485     |\n",
      "|    explained_variance   | 0.899      |\n",
      "|    learning_rate        | 0.000497   |\n",
      "|    loss                 | 0.186      |\n",
      "|    n_updates            | 315        |\n",
      "|    policy_gradient_loss | -0.000859  |\n",
      "|    value_loss           | 0.0655     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=386.80 +/- 93.67\n",
      "Episode length: 386.80 +/- 93.67\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 387         |\n",
      "|    mean_reward          | 387         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006378956 |\n",
      "|    clip_fraction        | 0.0169      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.104       |\n",
      "|    n_updates            | 324         |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    value_loss           | 0.25        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 814   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 26492 |\n",
      "------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 821           |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 33            |\n",
      "|    total_timesteps      | 27208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054461556 |\n",
      "|    clip_fraction        | 0.00159       |\n",
      "|    clip_range           | 0.251         |\n",
      "|    entropy_loss         | -0.503        |\n",
      "|    explained_variance   | 0.238         |\n",
      "|    learning_rate        | 0.000497      |\n",
      "|    loss                 | -0.0149       |\n",
      "|    n_updates            | 333           |\n",
      "|    policy_gradient_loss | 0.00054       |\n",
      "|    value_loss           | 0.0181        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 826          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 27924        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050759926 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | 0.71         |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.337        |\n",
      "|    n_updates            | 342          |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 0.383        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=408.60 +/- 78.59\n",
      "Episode length: 408.60 +/- 78.59\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 409          |\n",
      "|    mean_reward          | 409          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 28000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060248445 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.511       |\n",
      "|    explained_variance   | 0.616        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.396        |\n",
      "|    n_updates            | 351          |\n",
      "|    policy_gradient_loss | -0.00916     |\n",
      "|    value_loss           | 0.283        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 800   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 28640 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 29356       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008673909 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.118       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    value_loss           | 0.167       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=471.00 +/- 58.00\n",
      "Episode length: 471.00 +/- 58.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 471          |\n",
      "|    mean_reward          | 471          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022844707 |\n",
      "|    clip_fraction        | 0.000579     |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.502       |\n",
      "|    explained_variance   | 0.689        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.133        |\n",
      "|    n_updates            | 369          |\n",
      "|    policy_gradient_loss | -6.54e-06    |\n",
      "|    value_loss           | 0.616        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 777   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 30072 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 786          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 30788        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039669825 |\n",
      "|    clip_fraction        | 0.00318      |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.508       |\n",
      "|    explained_variance   | 0.642        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.00526      |\n",
      "|    n_updates            | 378          |\n",
      "|    policy_gradient_loss | -0.000437    |\n",
      "|    value_loss           | 0.215        |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 31504      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01355214 |\n",
      "|    clip_fraction        | 0.0867     |\n",
      "|    clip_range           | 0.251      |\n",
      "|    entropy_loss         | -0.484     |\n",
      "|    explained_variance   | 0.809      |\n",
      "|    learning_rate        | 0.000497   |\n",
      "|    loss                 | -0.01      |\n",
      "|    n_updates            | 387        |\n",
      "|    policy_gradient_loss | -0.00659   |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=237.60 +/- 37.82\n",
      "Episode length: 237.60 +/- 37.82\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 238        |\n",
      "|    mean_reward          | 238        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00451945 |\n",
      "|    clip_fraction        | 0.0113     |\n",
      "|    clip_range           | 0.251      |\n",
      "|    entropy_loss         | -0.449     |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.000497   |\n",
      "|    loss                 | 0.00784    |\n",
      "|    n_updates            | 396        |\n",
      "|    policy_gradient_loss | -0.00154   |\n",
      "|    value_loss           | 0.0811     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 795   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 32220 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 803          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 32936        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043784995 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.443       |\n",
      "|    explained_variance   | 0.948        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.0575       |\n",
      "|    n_updates            | 405          |\n",
      "|    policy_gradient_loss | -0.00526     |\n",
      "|    value_loss           | 0.0783       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 812          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 33652        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013026517 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.448       |\n",
      "|    explained_variance   | 0.915        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.0376       |\n",
      "|    n_updates            | 414          |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    value_loss           | 0.191        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=224.00 +/- 57.25\n",
      "Episode length: 224.00 +/- 57.25\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 224          |\n",
      "|    mean_reward          | 224          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035687268 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.459       |\n",
      "|    explained_variance   | 0.956        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0554      |\n",
      "|    n_updates            | 423          |\n",
      "|    policy_gradient_loss | -0.000379    |\n",
      "|    value_loss           | 0.0975       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 803   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 34368 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 35084        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042340476 |\n",
      "|    clip_fraction        | 0.00395      |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.00575      |\n",
      "|    n_updates            | 432          |\n",
      "|    policy_gradient_loss | 0.000691     |\n",
      "|    value_loss           | 0.0738       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 817         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 35800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011377948 |\n",
      "|    clip_fraction        | 0.0496      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | -0.0316     |\n",
      "|    n_updates            | 441         |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    value_loss           | 0.0448      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=351.20 +/- 55.38\n",
      "Episode length: 351.20 +/- 55.38\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 351          |\n",
      "|    mean_reward          | 351          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 36000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046957503 |\n",
      "|    clip_fraction        | 0.0683       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.505       |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.0307       |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00957     |\n",
      "|    value_loss           | 0.0389       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 809   |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 36516 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 814         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 37232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009319795 |\n",
      "|    clip_fraction        | 0.0339      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 459         |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    value_loss           | 0.0399      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 818          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 37948        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081993425 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0355      |\n",
      "|    n_updates            | 468          |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    value_loss           | 0.0318       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=335.00 +/- 83.64\n",
      "Episode length: 335.00 +/- 83.64\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 335          |\n",
      "|    mean_reward          | 335          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030861606 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.47        |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.084       |\n",
      "|    n_updates            | 477          |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    value_loss           | 0.0605       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 804   |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 38664 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 39380        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005468966 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.487       |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.000594    |\n",
      "|    n_updates            | 486          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 0.0476       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=315.40 +/- 38.76\n",
      "Episode length: 315.40 +/- 38.76\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 315          |\n",
      "|    mean_reward          | 315          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029985653 |\n",
      "|    clip_fraction        | 0.0458       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.495       |\n",
      "|    explained_variance   | 0.951        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0246      |\n",
      "|    n_updates            | 495          |\n",
      "|    policy_gradient_loss | -0.00371     |\n",
      "|    value_loss           | 0.052        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 800   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 40096 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 805         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 40812       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003730297 |\n",
      "|    clip_fraction        | 0.0186      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 504         |\n",
      "|    policy_gradient_loss | 0.000147    |\n",
      "|    value_loss           | 0.0636      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 811          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 41528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024247388 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.542       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0138      |\n",
      "|    n_updates            | 513          |\n",
      "|    policy_gradient_loss | 0.000339     |\n",
      "|    value_loss           | 0.0269       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=342.20 +/- 35.73\n",
      "Episode length: 342.20 +/- 35.73\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 342         |\n",
      "|    mean_reward          | 342         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009709717 |\n",
      "|    clip_fraction        | 0.0431      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 522         |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    value_loss           | 0.0497      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 803   |\n",
      "|    iterations      | 59    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 42244 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 42960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035364942 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 531          |\n",
      "|    policy_gradient_loss | -4.37e-05    |\n",
      "|    value_loss           | 0.0496       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 812         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 43676       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009234687 |\n",
      "|    clip_fraction        | 0.0312      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.038       |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    value_loss           | 0.0716      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=270.20 +/- 15.00\n",
      "Episode length: 270.20 +/- 15.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 270          |\n",
      "|    mean_reward          | 270          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019312156 |\n",
      "|    clip_fraction        | 0.0134       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.0237       |\n",
      "|    n_updates            | 549          |\n",
      "|    policy_gradient_loss | -3.7e-05     |\n",
      "|    value_loss           | 0.0264       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 804   |\n",
      "|    iterations      | 62    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 44392 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 808          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 45108        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035146174 |\n",
      "|    clip_fraction        | 0.000434     |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.0493       |\n",
      "|    n_updates            | 558          |\n",
      "|    policy_gradient_loss | 0.00162      |\n",
      "|    value_loss           | 0.0613       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 808         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 45824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004130326 |\n",
      "|    clip_fraction        | 0.0401      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | -0.0319     |\n",
      "|    n_updates            | 567         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    value_loss           | 0.0446      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=228.40 +/- 6.41\n",
      "Episode length: 228.40 +/- 6.41\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 228         |\n",
      "|    mean_reward          | 228         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005165627 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.38        |\n",
      "|    n_updates            | 576         |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    value_loss           | 0.0816      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 796   |\n",
      "|    iterations      | 65    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 46540 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 800          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 47256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026857697 |\n",
      "|    clip_fraction        | 0.0122       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.477       |\n",
      "|    explained_variance   | 0.969        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.00703     |\n",
      "|    n_updates            | 585          |\n",
      "|    policy_gradient_loss | 0.000273     |\n",
      "|    value_loss           | 0.058        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 803          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 47972        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072027724 |\n",
      "|    clip_fraction        | 0.0388       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.468       |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | 0.00711      |\n",
      "|    n_updates            | 594          |\n",
      "|    policy_gradient_loss | -0.00495     |\n",
      "|    value_loss           | 0.036        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=204.20 +/- 17.53\n",
      "Episode length: 204.20 +/- 17.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 204         |\n",
      "|    mean_reward          | 204         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008065852 |\n",
      "|    clip_fraction        | 0.0523      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 603         |\n",
      "|    policy_gradient_loss | -0.00811    |\n",
      "|    value_loss           | 0.0288      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 801   |\n",
      "|    iterations      | 68    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 48688 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 49404        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035767646 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0355      |\n",
      "|    n_updates            | 612          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    value_loss           | 0.0112       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=194.20 +/- 14.54\n",
      "Episode length: 194.20 +/- 14.54\n",
      "-------------------------------------------\n",
      "| eval/                   |               |\n",
      "|    mean_ep_length       | 194           |\n",
      "|    mean_reward          | 194           |\n",
      "| time/                   |               |\n",
      "|    total_timesteps      | 50000         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00095439743 |\n",
      "|    clip_fraction        | 0.0255        |\n",
      "|    clip_range           | 0.251         |\n",
      "|    entropy_loss         | -0.476        |\n",
      "|    explained_variance   | 0.989         |\n",
      "|    learning_rate        | 0.000497      |\n",
      "|    loss                 | 0.0353        |\n",
      "|    n_updates            | 621           |\n",
      "|    policy_gradient_loss | 0.00127       |\n",
      "|    value_loss           | 0.0121        |\n",
      "-------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 805   |\n",
      "|    iterations      | 70    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 50120 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 806          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 50836        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025214874 |\n",
      "|    clip_fraction        | 0.0173       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.482       |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0644      |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00372     |\n",
      "|    value_loss           | 0.00977      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 803          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 51552        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039568376 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.495       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0153      |\n",
      "|    n_updates            | 639          |\n",
      "|    policy_gradient_loss | -0.00519     |\n",
      "|    value_loss           | 0.0249       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=177.40 +/- 8.16\n",
      "Episode length: 177.40 +/- 8.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 177         |\n",
      "|    mean_reward          | 177         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006058326 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.00325     |\n",
      "|    n_updates            | 648         |\n",
      "|    policy_gradient_loss | 0.000245    |\n",
      "|    value_loss           | 0.00545     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 789   |\n",
      "|    iterations      | 73    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 52268 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 52984        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020813432 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.497       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.00764     |\n",
      "|    n_updates            | 657          |\n",
      "|    policy_gradient_loss | 0.000716     |\n",
      "|    value_loss           | 0.0107       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 793          |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 53700        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029068536 |\n",
      "|    clip_fraction        | 0.0538       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.483       |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.00274     |\n",
      "|    n_updates            | 666          |\n",
      "|    policy_gradient_loss | -0.00821     |\n",
      "|    value_loss           | 0.116        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=174.40 +/- 8.01\n",
      "Episode length: 174.40 +/- 8.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 174         |\n",
      "|    mean_reward          | 174         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006277057 |\n",
      "|    clip_fraction        | 0.00637     |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.00709     |\n",
      "|    n_updates            | 675         |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 0.0138      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 788   |\n",
      "|    iterations      | 76    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 54416 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 793          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 55132        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050310516 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.446       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0113      |\n",
      "|    n_updates            | 684          |\n",
      "|    policy_gradient_loss | -0.000402    |\n",
      "|    value_loss           | 0.00896      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 798          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 55848        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043933056 |\n",
      "|    clip_fraction        | 0.027        |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0094      |\n",
      "|    n_updates            | 693          |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 0.00601      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=162.40 +/- 12.24\n",
      "Episode length: 162.40 +/- 12.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 162         |\n",
      "|    mean_reward          | 162         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012793057 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.251       |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.000497    |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 702         |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 0.0174      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 799   |\n",
      "|    iterations      | 79    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 56564 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 803          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 57280        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046340306 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.251        |\n",
      "|    entropy_loss         | -0.502       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.000497     |\n",
      "|    loss                 | -0.0202      |\n",
      "|    n_updates            | 711          |\n",
      "|    policy_gradient_loss | 0.000334     |\n",
      "|    value_loss           | 0.0106       |\n",
      "------------------------------------------\n",
      "Multi-environment training took 75.55 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>164.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sweep-1</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cl7g958b' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/cl7g958b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_142920-cl7g958b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: htf8wkb2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.22142595193361375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.004019576161769689\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9939615430120788\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.914968521952961\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007455090939542673\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 0.8560975048503298\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 363\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 31322\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143049-htf8wkb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/htf8wkb2' target=\"_blank\">lucky-sweep-2</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/htf8wkb2' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/htf8wkb2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1452`, after every 22 untruncated mini-batches, there will be a truncated mini-batch of size 44\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=363 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1426db790> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x1426899d0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3322 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1452 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=242.80 +/- 34.03\n",
      "Episode length: 242.80 +/- 34.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 243         |\n",
      "|    mean_reward          | 243         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022166245 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | -0.00129    |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.884       |\n",
      "|    n_updates            | 5           |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 7.89        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1806 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2904 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=190.60 +/- 71.48\n",
      "Episode length: 190.60 +/- 71.48\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 191        |\n",
      "|    mean_reward          | 191        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02750365 |\n",
      "|    clip_fraction        | 0.434      |\n",
      "|    clip_range           | 0.221      |\n",
      "|    entropy_loss         | -0.648     |\n",
      "|    explained_variance   | 0.585      |\n",
      "|    learning_rate        | 0.00746    |\n",
      "|    loss                 | 1.88       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 3.05       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1625 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 4356 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1730        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 5808        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019737797 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=333.20 +/- 128.75\n",
      "Episode length: 333.20 +/- 128.75\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 333         |\n",
      "|    mean_reward          | 333         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028971983 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.511       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.27        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1569 |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 7260 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=282.60 +/- 58.44\n",
      "Episode length: 282.60 +/- 58.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 283         |\n",
      "|    mean_reward          | 283         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021004405 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 25          |\n",
      "|    policy_gradient_loss | 0.00561     |\n",
      "|    value_loss           | 0.347       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1497 |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 8712 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=297.20 +/- 96.49\n",
      "Episode length: 297.20 +/- 96.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 297         |\n",
      "|    mean_reward          | 297         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013912785 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0582      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.00645     |\n",
      "|    value_loss           | 0.327       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1448  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 10164 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1496       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 11616      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01821731 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.221      |\n",
      "|    entropy_loss         | -0.543     |\n",
      "|    explained_variance   | 0.671      |\n",
      "|    learning_rate        | 0.00746    |\n",
      "|    loss                 | 0.165      |\n",
      "|    n_updates            | 35         |\n",
      "|    policy_gradient_loss | -0.00632   |\n",
      "|    value_loss           | 0.476      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=293.40 +/- 106.36\n",
      "Episode length: 293.40 +/- 106.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 293         |\n",
      "|    mean_reward          | 293         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013783285 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.403       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    value_loss           | 0.475       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1457  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 8     |\n",
      "|    total_timesteps | 13068 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=364.20 +/- 59.87\n",
      "Episode length: 364.20 +/- 59.87\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 364         |\n",
      "|    mean_reward          | 364         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016875075 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.00628     |\n",
      "|    value_loss           | 0.322       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1397  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 14520 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1442        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 15972       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024846587 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    value_loss           | 0.519       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=414.80 +/- 99.60\n",
      "Episode length: 414.80 +/- 99.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 415         |\n",
      "|    mean_reward          | 415         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011890853 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.244       |\n",
      "|    n_updates            | 55          |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 0.396       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1336  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 13    |\n",
      "|    total_timesteps | 17424 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=472.60 +/- 33.74\n",
      "Episode length: 472.60 +/- 33.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 473         |\n",
      "|    mean_reward          | 473         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016144129 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0375      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00184     |\n",
      "|    value_loss           | 0.202       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1277  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 18876 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=474.20 +/- 49.14\n",
      "Episode length: 474.20 +/- 49.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 474         |\n",
      "|    mean_reward          | 474         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015472177 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | -0.0598     |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0459      |\n",
      "|    n_updates            | 65          |\n",
      "|    policy_gradient_loss | 0.000206    |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1232  |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 20328 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1248        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 21780       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022421148 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00879     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=283.60 +/- 29.71\n",
      "Episode length: 283.60 +/- 29.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 284         |\n",
      "|    mean_reward          | 284         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010805195 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    value_loss           | 0.0952      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1246  |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 23232 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=268.80 +/- 16.14\n",
      "Episode length: 268.80 +/- 16.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 269         |\n",
      "|    mean_reward          | 269         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017936744 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | -0.000662   |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.0049      |\n",
      "|    value_loss           | 0.0591      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1246  |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 24684 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=414.40 +/- 75.15\n",
      "Episode length: 414.40 +/- 75.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 414         |\n",
      "|    mean_reward          | 414         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018252477 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.018       |\n",
      "|    n_updates            | 85          |\n",
      "|    policy_gradient_loss | 0.0114      |\n",
      "|    value_loss           | 0.0345      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1228  |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 26136 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1256        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 27588       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010417533 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00772     |\n",
      "|    value_loss           | 0.027       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=193.60 +/- 28.57\n",
      "Episode length: 193.60 +/- 28.57\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 194         |\n",
      "|    mean_reward          | 194         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026951885 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0342      |\n",
      "|    n_updates            | 95          |\n",
      "|    policy_gradient_loss | 0.0163      |\n",
      "|    value_loss           | 0.0845      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1263  |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 29040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=297.60 +/- 104.02\n",
      "Episode length: 297.60 +/- 104.02\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 298         |\n",
      "|    mean_reward          | 298         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051654447 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.675       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00552     |\n",
      "|    value_loss           | 0.889       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1259  |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 30492 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1281        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 31944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029290127 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.221       |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.304       |\n",
      "|    learning_rate        | 0.00746     |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 105         |\n",
      "|    policy_gradient_loss | 0.00222     |\n",
      "|    value_loss           | 0.892       |\n",
      "-----------------------------------------\n",
      "Multi-environment training took 29.84 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>334.1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-sweep-2</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/htf8wkb2' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/htf8wkb2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143049-htf8wkb2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lt3ngz64 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.26826561422371387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.00045131284383028103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9190931991195735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9425235444069028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009254711792238631\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 5.925643982366641\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 357\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 76208\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143140-lt3ngz64</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/lt3ngz64' target=\"_blank\">silver-sweep-3</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/lt3ngz64' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/lt3ngz64</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1428`, after every 22 untruncated mini-batches, there will be a truncated mini-batch of size 20\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=357 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1427192b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x14271e400>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1463 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1428 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=78.20 +/- 6.49\n",
      "Episode length: 78.20 +/- 6.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 78.2        |\n",
      "|    mean_reward          | 78.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025846608 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | -0.00587    |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.717       |\n",
      "|    n_updates            | 2           |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    value_loss           | 7.54        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1191 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2856 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=483.20 +/- 33.60\n",
      "Episode length: 483.20 +/- 33.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040624123 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.985       |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    value_loss           | 4.57        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 893  |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 4284 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1064        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 5712        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036103606 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.223       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=455.00 +/- 64.67\n",
      "Episode length: 455.00 +/- 64.67\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 455        |\n",
      "|    mean_reward          | 455        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04759426 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.447     |\n",
      "|    explained_variance   | 0.784      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.886      |\n",
      "|    n_updates            | 8          |\n",
      "|    policy_gradient_loss | -0.00483   |\n",
      "|    value_loss           | 0.97       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 945  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 7140 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=442.60 +/- 96.06\n",
      "Episode length: 442.60 +/- 96.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 443         |\n",
      "|    mean_reward          | 443         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026878087 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.435      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.047       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00578    |\n",
      "|    value_loss           | 0.805       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 899  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 8568 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 986         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 9996        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058409005 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.184       |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | 0.00408     |\n",
      "|    value_loss           | 0.979       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=497.20 +/- 5.60\n",
      "Episode length: 497.20 +/- 5.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 497         |\n",
      "|    mean_reward          | 497         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030484166 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.139       |\n",
      "|    n_updates            | 14          |\n",
      "|    policy_gradient_loss | 0.0174      |\n",
      "|    value_loss           | 0.704       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 961   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 11424 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=450.40 +/- 99.20\n",
      "Episode length: 450.40 +/- 99.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 450        |\n",
      "|    mean_reward          | 450        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00998281 |\n",
      "|    clip_fraction        | 0.0876     |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.128      |\n",
      "|    n_updates            | 16         |\n",
      "|    policy_gradient_loss | 0.00858    |\n",
      "|    value_loss           | 0.615      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 956   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 13    |\n",
      "|    total_timesteps | 12852 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=254.80 +/- 80.48\n",
      "Episode length: 254.80 +/- 80.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 255         |\n",
      "|    mean_reward          | 255         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021499282 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.116       |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | 0.00522     |\n",
      "|    value_loss           | 0.389       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 979   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 14280 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1032        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 15708       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010600941 |\n",
      "|    clip_fraction        | 0.0969      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.00649     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00912     |\n",
      "|    value_loss           | 0.809       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=479.00 +/- 42.00\n",
      "Episode length: 479.00 +/- 42.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 479         |\n",
      "|    mean_reward          | 479         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055639002 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.145       |\n",
      "|    n_updates            | 22          |\n",
      "|    policy_gradient_loss | 0.0224      |\n",
      "|    value_loss           | 0.905       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1011  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 17136 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=426.40 +/- 114.64\n",
      "Episode length: 426.40 +/- 114.64\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 426        |\n",
      "|    mean_reward          | 426        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08280915 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.202     |\n",
      "|    explained_variance   | 0.947      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.0314     |\n",
      "|    n_updates            | 24         |\n",
      "|    policy_gradient_loss | 0.0202     |\n",
      "|    value_loss           | 0.371      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1007  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 18564 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1048       |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 19992      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18265492 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.0981    |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | -0.00111   |\n",
      "|    n_updates            | 26         |\n",
      "|    policy_gradient_loss | 0.00603    |\n",
      "|    value_loss           | 0.233      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=477.60 +/- 44.80\n",
      "Episode length: 477.60 +/- 44.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 478        |\n",
      "|    mean_reward          | 478        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04336744 |\n",
      "|    clip_fraction        | 0.105      |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.0941    |\n",
      "|    explained_variance   | 0.782      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.0443     |\n",
      "|    n_updates            | 28         |\n",
      "|    policy_gradient_loss | 0.0134     |\n",
      "|    value_loss           | 0.628      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 974   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 21420 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=419.20 +/- 98.96\n",
      "Episode length: 419.20 +/- 98.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 419         |\n",
      "|    mean_reward          | 419         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034231145 |\n",
      "|    clip_fraction        | 0.0865      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.159      |\n",
      "|    explained_variance   | 0.582       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.296       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0109      |\n",
      "|    value_loss           | 0.338       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 963   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 22848 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=273.60 +/- 102.02\n",
      "Episode length: 273.60 +/- 102.02\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 274        |\n",
      "|    mean_reward          | 274        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50330997 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.152     |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.00607    |\n",
      "|    n_updates            | 32         |\n",
      "|    policy_gradient_loss | 0.0116     |\n",
      "|    value_loss           | 0.31       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 936   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 24276 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 969        |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 25704      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10855118 |\n",
      "|    clip_fraction        | 0.0836     |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.034      |\n",
      "|    n_updates            | 34         |\n",
      "|    policy_gradient_loss | 0.015      |\n",
      "|    value_loss           | 0.0551     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=362.00 +/- 143.30\n",
      "Episode length: 362.00 +/- 143.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 362         |\n",
      "|    mean_reward          | 362         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011760964 |\n",
      "|    clip_fraction        | 0.0578      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.109      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.117       |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.00677     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 970   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 27132 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=272.40 +/- 115.01\n",
      "Episode length: 272.40 +/- 115.01\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 272        |\n",
      "|    mean_reward          | 272        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04864248 |\n",
      "|    clip_fraction        | 0.0753     |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.119     |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.00502    |\n",
      "|    n_updates            | 38         |\n",
      "|    policy_gradient_loss | 0.0126     |\n",
      "|    value_loss           | 0.0328     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 986   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 28560 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1017        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 29988       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016398879 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.00348     |\n",
      "|    value_loss           | 0.0548      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=469.60 +/- 38.13\n",
      "Episode length: 469.60 +/- 38.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 470         |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038725283 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.155      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0822      |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    value_loss           | 0.0701      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1018  |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 31416 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=446.00 +/- 66.44\n",
      "Episode length: 446.00 +/- 66.44\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 446        |\n",
      "|    mean_reward          | 446        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09976063 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.147     |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.25       |\n",
      "|    n_updates            | 44         |\n",
      "|    policy_gradient_loss | 0.00648    |\n",
      "|    value_loss           | 0.284      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1015  |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 32844 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=466.40 +/- 67.20\n",
      "Episode length: 466.40 +/- 67.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 466         |\n",
      "|    mean_reward          | 466         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080318615 |\n",
      "|    clip_fraction        | 0.0575      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.147      |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.227       |\n",
      "|    n_updates            | 46          |\n",
      "|    policy_gradient_loss | 0.00913     |\n",
      "|    value_loss           | 0.334       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1016  |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 34272 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1042        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 35700       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024225593 |\n",
      "|    clip_fraction        | 0.0575      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.143      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.032       |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.000185   |\n",
      "|    value_loss           | 0.148       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=357.40 +/- 141.63\n",
      "Episode length: 357.40 +/- 141.63\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 357         |\n",
      "|    mean_reward          | 357         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007417318 |\n",
      "|    clip_fraction        | 0.0303      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.000392   |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00505     |\n",
      "|    value_loss           | 0.0867      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1049  |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 37128 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=311.20 +/- 144.37\n",
      "Episode length: 311.20 +/- 144.37\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 311         |\n",
      "|    mean_reward          | 311         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009273111 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.16       |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 52          |\n",
      "|    policy_gradient_loss | 0.013       |\n",
      "|    value_loss           | 0.141       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1033  |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 38556 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1045        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 39984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056417394 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.179      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0111      |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | 0.00141     |\n",
      "|    value_loss           | 0.109       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=184.20 +/- 45.05\n",
      "Episode length: 184.20 +/- 45.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 184         |\n",
      "|    mean_reward          | 184         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009714624 |\n",
      "|    clip_fraction        | 0.0594      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.182      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | 0.00504     |\n",
      "|    value_loss           | 0.038       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1043  |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 41412 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=278.60 +/- 143.83\n",
      "Episode length: 278.60 +/- 143.83\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 279          |\n",
      "|    mean_reward          | 279          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046301936 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.268        |\n",
      "|    entropy_loss         | -0.137       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00925      |\n",
      "|    loss                 | 0.0294       |\n",
      "|    n_updates            | 58           |\n",
      "|    policy_gradient_loss | -1.57e-05    |\n",
      "|    value_loss           | 0.0371       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1036  |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 42840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=200.60 +/- 40.28\n",
      "Episode length: 200.60 +/- 40.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 201        |\n",
      "|    mean_reward          | 201        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 44000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39458326 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.0974    |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.0152     |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | 0.0241     |\n",
      "|    value_loss           | 0.0485     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1044  |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 44268 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1065       |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 45696      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31937978 |\n",
      "|    clip_fraction        | 0.0851     |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.0574    |\n",
      "|    explained_variance   | 0.997      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 62         |\n",
      "|    policy_gradient_loss | 0.0142     |\n",
      "|    value_loss           | 0.0727     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=305.80 +/- 109.26\n",
      "Episode length: 305.80 +/- 109.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 306         |\n",
      "|    mean_reward          | 306         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010997537 |\n",
      "|    clip_fraction        | 0.0382      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0502     |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0747      |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | 0.00669     |\n",
      "|    value_loss           | 0.753       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1055  |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 47124 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=383.80 +/- 110.70\n",
      "Episode length: 383.80 +/- 110.70\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 384        |\n",
      "|    mean_reward          | 384        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 48000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06610113 |\n",
      "|    clip_fraction        | 0.0695     |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.0682    |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.16       |\n",
      "|    n_updates            | 66         |\n",
      "|    policy_gradient_loss | -0.00645   |\n",
      "|    value_loss           | 0.245      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1051  |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 48552 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1068        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 49980       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049570642 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0649     |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 68          |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    value_loss           | 0.256       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=373.40 +/- 83.57\n",
      "Episode length: 373.40 +/- 83.57\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 373       |\n",
      "|    mean_reward          | 373       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 50000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0373934 |\n",
      "|    clip_fraction        | 0.0738    |\n",
      "|    clip_range           | 0.268     |\n",
      "|    entropy_loss         | -0.0866   |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.00925   |\n",
      "|    loss                 | 0.0635    |\n",
      "|    n_updates            | 70        |\n",
      "|    policy_gradient_loss | 0.017     |\n",
      "|    value_loss           | 0.102     |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1070  |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 51408 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 52000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02753175 |\n",
      "|    clip_fraction        | 0.0736     |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.0996    |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.127      |\n",
      "|    n_updates            | 72         |\n",
      "|    policy_gradient_loss | 0.0221     |\n",
      "|    value_loss           | 0.25       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1059  |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 52836 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=489.60 +/- 20.80\n",
      "Episode length: 489.60 +/- 20.80\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 490      |\n",
      "|    mean_reward          | 490      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 54000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.051903 |\n",
      "|    clip_fraction        | 0.0536   |\n",
      "|    clip_range           | 0.268    |\n",
      "|    entropy_loss         | -0.0721  |\n",
      "|    explained_variance   | 0.969    |\n",
      "|    learning_rate        | 0.00925  |\n",
      "|    loss                 | 0.41     |\n",
      "|    n_updates            | 74       |\n",
      "|    policy_gradient_loss | 0.0253   |\n",
      "|    value_loss           | 0.255    |\n",
      "--------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1050  |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 54264 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1067        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 55692       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004810967 |\n",
      "|    clip_fraction        | 0.0465      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0801     |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.000625   |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | 0.00257     |\n",
      "|    value_loss           | 0.000473    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021734351 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0755     |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.258       |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.00747     |\n",
      "|    value_loss           | 0.0639      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1057  |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 57120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=452.40 +/- 50.03\n",
      "Episode length: 452.40 +/- 50.03\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 452         |\n",
      "|    mean_reward          | 452         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021277856 |\n",
      "|    clip_fraction        | 0.0308      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.052      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0411      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00723     |\n",
      "|    value_loss           | 0.0942      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1053  |\n",
      "|    iterations      | 41    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 58548 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1068        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 59976       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014194164 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.075      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0089      |\n",
      "|    n_updates            | 82          |\n",
      "|    policy_gradient_loss | 0.00138     |\n",
      "|    value_loss           | 1.26        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=454.60 +/- 90.80\n",
      "Episode length: 454.60 +/- 90.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 455         |\n",
      "|    mean_reward          | 455         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005826511 |\n",
      "|    clip_fraction        | 0.0378      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0741     |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.548       |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | 0.0043      |\n",
      "|    value_loss           | 0.436       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1068  |\n",
      "|    iterations      | 43    |\n",
      "|    time_elapsed    | 57    |\n",
      "|    total_timesteps | 61404 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022247488 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0847     |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 2.47        |\n",
      "|    n_updates            | 86          |\n",
      "|    policy_gradient_loss | 0.00525     |\n",
      "|    value_loss           | 0.511       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1065  |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 62832 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002994914 |\n",
      "|    clip_fraction        | 0.0306      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0909     |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.128       |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | 0.00108     |\n",
      "|    value_loss           | 0.676       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1057  |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 64260 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1069        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 65688       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007022599 |\n",
      "|    clip_fraction        | 0.0297      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0794     |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.000171    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00356     |\n",
      "|    value_loss           | 0.0107      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=487.00 +/- 26.00\n",
      "Episode length: 487.00 +/- 26.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 487         |\n",
      "|    mean_reward          | 487         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007853512 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.112      |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.00117    |\n",
      "|    n_updates            | 92          |\n",
      "|    policy_gradient_loss | 0.0006      |\n",
      "|    value_loss           | 0.000409    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1058  |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 67116 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=450.80 +/- 64.19\n",
      "Episode length: 450.80 +/- 64.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 451         |\n",
      "|    mean_reward          | 451         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034672916 |\n",
      "|    clip_fraction        | 0.0878      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.114      |\n",
      "|    explained_variance   | 0.737       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 94          |\n",
      "|    policy_gradient_loss | 0.00257     |\n",
      "|    value_loss           | 0.000116    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1058  |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 68544 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1071        |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 69972       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012131956 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.118      |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | 0.00153     |\n",
      "|    value_loss           | 0.155       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060833298 |\n",
      "|    clip_fraction        | 0.0783      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.0978     |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | 0.0774      |\n",
      "|    n_updates            | 98          |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1069  |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 71400 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058171563 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.268        |\n",
      "|    entropy_loss         | -0.123       |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 0.00925      |\n",
      "|    loss                 | -0.0173      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 0.000982     |\n",
      "|    value_loss           | 0.00227      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1062  |\n",
      "|    iterations      | 51    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 72828 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=490.20 +/- 19.60\n",
      "Episode length: 490.20 +/- 19.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 490         |\n",
      "|    mean_reward          | 490         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013899793 |\n",
      "|    clip_fraction        | 0.0418      |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.122      |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | 0.00137     |\n",
      "|    value_loss           | 0.000284    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1057  |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 74256 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1070        |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 75684       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011667158 |\n",
      "|    clip_fraction        | 0.044       |\n",
      "|    clip_range           | 0.268       |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.00925     |\n",
      "|    loss                 | -0.00207    |\n",
      "|    n_updates            | 104         |\n",
      "|    policy_gradient_loss | 0.00339     |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 76000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03797425 |\n",
      "|    clip_fraction        | 0.0478     |\n",
      "|    clip_range           | 0.268      |\n",
      "|    entropy_loss         | -0.0817    |\n",
      "|    explained_variance   | 0.683      |\n",
      "|    learning_rate        | 0.00925    |\n",
      "|    loss                 | 0.156      |\n",
      "|    n_updates            | 106        |\n",
      "|    policy_gradient_loss | 0.00349    |\n",
      "|    value_loss           | 0.426      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1057  |\n",
      "|    iterations      | 54    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 77112 |\n",
      "------------------------------\n",
      "Multi-environment training took 78.27 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-sweep-3</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/lt3ngz64' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/lt3ngz64</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143140-lt3ngz64/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j4f9y0h0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.28679502905891463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.003492115143496844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.969664421079512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9447246576943796\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008435304788672422\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 1.2858542687842636\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 39522\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143313-j4f9y0h0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/j4f9y0h0' target=\"_blank\">comfy-sweep-4</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/j4f9y0h0' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/j4f9y0h0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4312`, after every 67 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1078 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x14277d940> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x14277dc70>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=9.80 +/- 0.40\n",
      "Episode length: 9.80 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 9.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=10.00 +/- 0.63\n",
      "Episode length: 10.00 +/- 0.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 10       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3697 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 4312 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=294.20 +/- 111.05\n",
      "Episode length: 294.20 +/- 111.05\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 294         |\n",
      "|    mean_reward          | 294         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036308046 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.00242     |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 0.657       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    value_loss           | 4.7         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=361.20 +/- 102.21\n",
      "Episode length: 361.20 +/- 102.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 361      |\n",
      "|    mean_reward     | 361      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1582 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 8624 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=324.20 +/- 90.48\n",
      "Episode length: 324.20 +/- 90.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | 324         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044468086 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 3.14        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0474     |\n",
      "|    value_loss           | 6.08        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=341.20 +/- 96.50\n",
      "Episode length: 341.20 +/- 96.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 341      |\n",
      "|    mean_reward     | 341      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1297  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 12936 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=374.60 +/- 88.07\n",
      "Episode length: 374.60 +/- 88.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 375         |\n",
      "|    mean_reward          | 375         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032823108 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 0.24        |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    value_loss           | 2.75        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=343.20 +/- 101.90\n",
      "Episode length: 343.20 +/- 101.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 343      |\n",
      "|    mean_reward     | 343      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1274  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 13    |\n",
      "|    total_timesteps | 17248 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=367.80 +/- 68.36\n",
      "Episode length: 367.80 +/- 68.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 368         |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054719098 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 0.133       |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    value_loss           | 0.462       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=342.20 +/- 84.26\n",
      "Episode length: 342.20 +/- 84.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 342      |\n",
      "|    mean_reward     | 342      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1255  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 21560 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=435.80 +/- 62.71\n",
      "Episode length: 435.80 +/- 62.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 436         |\n",
      "|    mean_reward          | 436         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102200896 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 0.0358      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0208      |\n",
      "|    value_loss           | 0.0533      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=382.00 +/- 91.31\n",
      "Episode length: 382.00 +/- 91.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 382      |\n",
      "|    mean_reward     | 382      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1233  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 25872 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=419.00 +/- 76.14\n",
      "Episode length: 419.00 +/- 76.14\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 419         |\n",
      "|    mean_reward          | 419         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039533287 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.283      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 0.0848      |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.0189      |\n",
      "|    value_loss           | 0.05        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=432.60 +/- 85.38\n",
      "Episode length: 432.60 +/- 85.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 433      |\n",
      "|    mean_reward     | 433      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=456.20 +/- 87.60\n",
      "Episode length: 456.20 +/- 87.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 456      |\n",
      "|    mean_reward     | 456      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1173  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 30184 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=484.60 +/- 30.80\n",
      "Episode length: 484.60 +/- 30.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 485         |\n",
      "|    mean_reward          | 485         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040609032 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.324      |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | 0.0134      |\n",
      "|    value_loss           | 0.0934      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=34000, episode_reward=470.00 +/- 60.00\n",
      "Episode length: 470.00 +/- 60.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 470      |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1159  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 34496 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=489.00 +/- 22.00\n",
      "Episode length: 489.00 +/- 22.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 489         |\n",
      "|    mean_reward          | 489         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023296883 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.287       |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00844     |\n",
      "|    loss                 | 0.0212      |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | 0.00958     |\n",
      "|    value_loss           | 0.266       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1143  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 38808 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03340922 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.287      |\n",
      "|    entropy_loss         | -0.308     |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.00844    |\n",
      "|    loss                 | 0.024      |\n",
      "|    n_updates            | 54         |\n",
      "|    policy_gradient_loss | 0.00783    |\n",
      "|    value_loss           | 0.0773     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1085  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 43120 |\n",
      "------------------------------\n",
      "Multi-environment training took 45.47 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>492.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">comfy-sweep-4</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/j4f9y0h0' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/j4f9y0h0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143313-j4f9y0h0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qfezt17m with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.10160280177355592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.004913550107822112\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9267262165397676\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9301180889797372\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003544989206473057\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 7.1837055551636935\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 480\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 60222\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143412-qfezt17m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/qfezt17m' target=\"_blank\">peach-sweep-5</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/qfezt17m' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/qfezt17m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x14280e190> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x1427ff550>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2734 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1920 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=290.00 +/- 20.95\n",
      "Episode length: 290.00 +/- 20.95\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 290          |\n",
      "|    mean_reward          | 290          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024749634 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.691       |\n",
      "|    explained_variance   | 0.00328      |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 1.76         |\n",
      "|    n_updates            | 1            |\n",
      "|    policy_gradient_loss | -0.00428     |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1885 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 3840 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018519996 |\n",
      "|    clip_fraction        | 0.0964       |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.688       |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 2.34         |\n",
      "|    n_updates            | 2            |\n",
      "|    policy_gradient_loss | -0.00636     |\n",
      "|    value_loss           | 5.94         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1413 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 5760 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004044175 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.372       |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 2.28        |\n",
      "|    n_updates            | 3           |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 5.19        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1254 |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 7680 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=420.00 +/- 99.92\n",
      "Episode length: 420.00 +/- 99.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 420          |\n",
      "|    mean_reward          | 420          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036425397 |\n",
      "|    clip_fraction        | 0.234        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | 0.461        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 2.73         |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.0126      |\n",
      "|    value_loss           | 4.86         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1254 |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 9600 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=392.00 +/- 93.92\n",
      "Episode length: 392.00 +/- 93.92\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 392          |\n",
      "|    mean_reward          | 392          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046753897 |\n",
      "|    clip_fraction        | 0.281        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.636       |\n",
      "|    explained_variance   | 0.544        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 5            |\n",
      "|    policy_gradient_loss | -0.0181      |\n",
      "|    value_loss           | 4.26         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1232  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 11520 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=388.40 +/- 129.96\n",
      "Episode length: 388.40 +/- 129.96\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 388         |\n",
      "|    mean_reward          | 388         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009942878 |\n",
      "|    clip_fraction        | 0.385       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 3.46        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1227  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 13440 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=251.00 +/- 94.52\n",
      "Episode length: 251.00 +/- 94.52\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 251          |\n",
      "|    mean_reward          | 251          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 14000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069630523 |\n",
      "|    clip_fraction        | 0.251        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.584       |\n",
      "|    explained_variance   | 0.41         |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.433        |\n",
      "|    n_updates            | 7            |\n",
      "|    policy_gradient_loss | -0.0072      |\n",
      "|    value_loss           | 2.33         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1269  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 15360 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=369.20 +/- 80.78\n",
      "Episode length: 369.20 +/- 80.78\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 369          |\n",
      "|    mean_reward          | 369          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070420597 |\n",
      "|    clip_fraction        | 0.248        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.33         |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.00493     |\n",
      "|    value_loss           | 1.35         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1290  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 13    |\n",
      "|    total_timesteps | 17280 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=400.40 +/- 105.75\n",
      "Episode length: 400.40 +/- 105.75\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 400          |\n",
      "|    mean_reward          | 400          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025318682 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.548       |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.509        |\n",
      "|    n_updates            | 9            |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 0.924        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1307  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 19200 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=379.60 +/- 125.84\n",
      "Episode length: 379.60 +/- 125.84\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 380          |\n",
      "|    mean_reward          | 380          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044451305 |\n",
      "|    clip_fraction        | 0.169        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.557       |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.5          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00441     |\n",
      "|    value_loss           | 0.803        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1325  |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 15    |\n",
      "|    total_timesteps | 21120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=343.40 +/- 129.09\n",
      "Episode length: 343.40 +/- 129.09\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | 343          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042998013 |\n",
      "|    clip_fraction        | 0.227        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.541       |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.859        |\n",
      "|    n_updates            | 11           |\n",
      "|    policy_gradient_loss | -0.000813    |\n",
      "|    value_loss           | 0.858        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1343  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 23040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=311.00 +/- 92.44\n",
      "Episode length: 311.00 +/- 92.44\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 311        |\n",
      "|    mean_reward          | 311        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01197796 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.522     |\n",
      "|    explained_variance   | 0.865      |\n",
      "|    learning_rate        | 0.00354    |\n",
      "|    loss                 | 0.252      |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | 0.0074     |\n",
      "|    value_loss           | 0.603      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1355  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 24960 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=275.40 +/- 62.43\n",
      "Episode length: 275.40 +/- 62.43\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 275          |\n",
      "|    mean_reward          | 275          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054948824 |\n",
      "|    clip_fraction        | 0.254        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.541       |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.19         |\n",
      "|    n_updates            | 13           |\n",
      "|    policy_gradient_loss | 0.00144      |\n",
      "|    value_loss           | 0.568        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1364  |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 26880 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=431.20 +/- 95.89\n",
      "Episode length: 431.20 +/- 95.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 431         |\n",
      "|    mean_reward          | 431         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007638725 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 0.368       |\n",
      "|    n_updates            | 14          |\n",
      "|    policy_gradient_loss | 0.00684     |\n",
      "|    value_loss           | 0.601       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1355  |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 28800 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=470.00 +/- 36.83\n",
      "Episode length: 470.00 +/- 36.83\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 470         |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003303854 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.523      |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 0.0756      |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | 0.00364     |\n",
      "|    value_loss           | 0.455       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1350  |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=411.80 +/- 108.08\n",
      "Episode length: 411.80 +/- 108.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 412         |\n",
      "|    mean_reward          | 412         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002327979 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 0.0975      |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    value_loss           | 0.23        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1343  |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 24    |\n",
      "|    total_timesteps | 32640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=497.20 +/- 5.60\n",
      "Episode length: 497.20 +/- 5.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 497          |\n",
      "|    mean_reward          | 497          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020390865 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.901        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.184        |\n",
      "|    n_updates            | 17           |\n",
      "|    policy_gradient_loss | -0.000303    |\n",
      "|    value_loss           | 0.336        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1335  |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 34560 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=376.60 +/- 93.89\n",
      "Episode length: 376.60 +/- 93.89\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 377          |\n",
      "|    mean_reward          | 377          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 36000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038467748 |\n",
      "|    clip_fraction        | 0.157        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.503       |\n",
      "|    explained_variance   | 0.845        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.459        |\n",
      "|    n_updates            | 18           |\n",
      "|    policy_gradient_loss | 0.00775      |\n",
      "|    value_loss           | 0.435        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1333  |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 36480 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=416.60 +/- 102.36\n",
      "Episode length: 416.60 +/- 102.36\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 417          |\n",
      "|    mean_reward          | 417          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029489351 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | 0.956        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.0701       |\n",
      "|    n_updates            | 19           |\n",
      "|    policy_gradient_loss | 0.00788      |\n",
      "|    value_loss           | 0.172        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1331  |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 38400 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=463.20 +/- 73.60\n",
      "Episode length: 463.20 +/- 73.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 463          |\n",
      "|    mean_reward          | 463          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045331395 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | 0.889        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.0727       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.00544      |\n",
      "|    value_loss           | 0.42         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1319  |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 40320 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=426.60 +/- 89.90\n",
      "Episode length: 426.60 +/- 89.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 427          |\n",
      "|    mean_reward          | 427          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024565228 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.532       |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.0698       |\n",
      "|    n_updates            | 21           |\n",
      "|    policy_gradient_loss | -0.000108    |\n",
      "|    value_loss           | 0.47         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1315  |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 32    |\n",
      "|    total_timesteps | 42240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=342.80 +/- 79.42\n",
      "Episode length: 342.80 +/- 79.42\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 343          |\n",
      "|    mean_reward          | 343          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015876243 |\n",
      "|    clip_fraction        | 0.0839       |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.513       |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.048        |\n",
      "|    n_updates            | 22           |\n",
      "|    policy_gradient_loss | 0.00105      |\n",
      "|    value_loss           | 0.254        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1322  |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 44160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=404.20 +/- 77.66\n",
      "Episode length: 404.20 +/- 77.66\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 404          |\n",
      "|    mean_reward          | 404          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 46000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024085378 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.496       |\n",
      "|    explained_variance   | 0.879        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.0157       |\n",
      "|    n_updates            | 23           |\n",
      "|    policy_gradient_loss | 0.00107      |\n",
      "|    value_loss           | 0.25         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1319  |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 46080 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=484.00 +/- 32.00\n",
      "Episode length: 484.00 +/- 32.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 484          |\n",
      "|    mean_reward          | 484          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010845216 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.498       |\n",
      "|    explained_variance   | 0.935        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.0631       |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | 0.00109      |\n",
      "|    value_loss           | 0.241        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1294  |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 48000 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1303         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 49920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020721997 |\n",
      "|    clip_fraction        | 0.0875       |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.907        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.199        |\n",
      "|    n_updates            | 25           |\n",
      "|    policy_gradient_loss | 0.00444      |\n",
      "|    value_loss           | 0.273        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=462.00 +/- 76.00\n",
      "Episode length: 462.00 +/- 76.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 462          |\n",
      "|    mean_reward          | 462          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025294605 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.0341       |\n",
      "|    n_updates            | 26           |\n",
      "|    policy_gradient_loss | 0.00381      |\n",
      "|    value_loss           | 0.184        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1284  |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 51840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=435.80 +/- 85.60\n",
      "Episode length: 435.80 +/- 85.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 436          |\n",
      "|    mean_reward          | 436          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024282334 |\n",
      "|    clip_fraction        | 0.0984       |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.485       |\n",
      "|    explained_variance   | 0.858        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.144        |\n",
      "|    n_updates            | 27           |\n",
      "|    policy_gradient_loss | 0.000456     |\n",
      "|    value_loss           | 0.318        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1275  |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 53760 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=414.00 +/- 77.11\n",
      "Episode length: 414.00 +/- 77.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 414         |\n",
      "|    mean_reward          | 414         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002590925 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.00354     |\n",
      "|    loss                 | 0.0346      |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | 0.00402     |\n",
      "|    value_loss           | 0.163       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1257  |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 55680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=391.60 +/- 89.81\n",
      "Episode length: 391.60 +/- 89.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 392          |\n",
      "|    mean_reward          | 392          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042218943 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.00393      |\n",
      "|    n_updates            | 29           |\n",
      "|    policy_gradient_loss | 0.00857      |\n",
      "|    value_loss           | 0.0981       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1250  |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 57600 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=474.60 +/- 50.80\n",
      "Episode length: 474.60 +/- 50.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 475          |\n",
      "|    mean_reward          | 475          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031540657 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.495       |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.0429       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.00314      |\n",
      "|    value_loss           | 0.371        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1239  |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 59520 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031697203 |\n",
      "|    clip_fraction        | 0.139        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | 0.912        |\n",
      "|    learning_rate        | 0.00354      |\n",
      "|    loss                 | 0.00609      |\n",
      "|    n_updates            | 31           |\n",
      "|    policy_gradient_loss | 0.00564      |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1235  |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "Multi-environment training took 53.56 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>488.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peach-sweep-5</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/qfezt17m' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/qfezt17m</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143412-qfezt17m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l5fce2w5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.15913465986628403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.008312410652083501\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9308009965780228\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9259713195649272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.003006980532605324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 8.199661090328787\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 94998\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143518-l5fce2w5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/l5fce2w5' target=\"_blank\">vibrant-sweep-6</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/l5fce2w5' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/l5fce2w5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2616`, after every 40 untruncated mini-batches, there will be a truncated mini-batch of size 56\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=654 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1428463a0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x142846460>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=8.80 +/- 0.75\n",
      "Episode length: 8.80 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 8.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3180 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2616 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=103.00 +/- 14.44\n",
      "Episode length: 103.00 +/- 14.44\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 103         |\n",
      "|    mean_reward          | 103         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010460696 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.0114     |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.279       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1971 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 5232 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=302.60 +/- 105.18\n",
      "Episode length: 302.60 +/- 105.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 303         |\n",
      "|    mean_reward          | 303         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011728723 |\n",
      "|    clip_fraction        | 0.363       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 1.32        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    value_loss           | 2.07        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1656 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 7848 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=318.20 +/- 54.54\n",
      "Episode length: 318.20 +/- 54.54\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 318         |\n",
      "|    mean_reward          | 318         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017430713 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.915       |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.0372     |\n",
      "|    value_loss           | 2.38        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=327.40 +/- 59.17\n",
      "Episode length: 327.40 +/- 59.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 327      |\n",
      "|    mean_reward     | 327      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1402  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 10464 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=398.40 +/- 83.89\n",
      "Episode length: 398.40 +/- 83.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 398         |\n",
      "|    mean_reward          | 398         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014497381 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.183       |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    value_loss           | 1.29        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1351  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 13080 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=368.00 +/- 115.94\n",
      "Episode length: 368.00 +/- 115.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 368         |\n",
      "|    mean_reward          | 368         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020944387 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.221       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 0.862       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1319  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 15696 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=397.80 +/- 125.26\n",
      "Episode length: 397.80 +/- 125.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 398         |\n",
      "|    mean_reward          | 398         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007517781 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | 0.000483    |\n",
      "|    value_loss           | 0.259       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=405.60 +/- 78.29\n",
      "Episode length: 405.60 +/- 78.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 406      |\n",
      "|    mean_reward     | 406      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1226  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 18312 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=385.20 +/- 40.93\n",
      "Episode length: 385.20 +/- 40.93\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 385        |\n",
      "|    mean_reward          | 385        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00542976 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.159      |\n",
      "|    entropy_loss         | -0.512     |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.00301    |\n",
      "|    loss                 | 0.0915     |\n",
      "|    n_updates            | 42         |\n",
      "|    policy_gradient_loss | 0.0016     |\n",
      "|    value_loss           | 0.171      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1223  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 20928 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=298.60 +/- 41.45\n",
      "Episode length: 298.60 +/- 41.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 299          |\n",
      "|    mean_reward          | 299          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065501877 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | 0.0248       |\n",
      "|    n_updates            | 48           |\n",
      "|    policy_gradient_loss | 0.00295      |\n",
      "|    value_loss           | 0.0493       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1228  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 23544 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=198.60 +/- 10.15\n",
      "Episode length: 198.60 +/- 10.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 199         |\n",
      "|    mean_reward          | 199         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004023944 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0133      |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | 0.000143    |\n",
      "|    value_loss           | 0.0456      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=207.40 +/- 27.60\n",
      "Episode length: 207.40 +/- 27.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 207      |\n",
      "|    mean_reward     | 207      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1221  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 26160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=253.80 +/- 48.38\n",
      "Episode length: 253.80 +/- 48.38\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 254         |\n",
      "|    mean_reward          | 254         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009792753 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0199      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00251     |\n",
      "|    value_loss           | 0.0249      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1227  |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 28776 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=240.60 +/- 47.45\n",
      "Episode length: 240.60 +/- 47.45\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 241          |\n",
      "|    mean_reward          | 241          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032790997 |\n",
      "|    clip_fraction        | 0.0824       |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.515       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | 0.0147       |\n",
      "|    n_updates            | 66           |\n",
      "|    policy_gradient_loss | 0.00362      |\n",
      "|    value_loss           | 0.0173       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1236  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 31392 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=202.00 +/- 9.86\n",
      "Episode length: 202.00 +/- 9.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 202         |\n",
      "|    mean_reward          | 202         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006816171 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0381      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.000317   |\n",
      "|    value_loss           | 0.0733      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=205.40 +/- 21.55\n",
      "Episode length: 205.40 +/- 21.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 205      |\n",
      "|    mean_reward     | 205      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1228  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 27    |\n",
      "|    total_timesteps | 34008 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=194.80 +/- 14.82\n",
      "Episode length: 194.80 +/- 14.82\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 195         |\n",
      "|    mean_reward          | 195         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004736384 |\n",
      "|    clip_fraction        | 0.0824      |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0688      |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | 0.000418    |\n",
      "|    value_loss           | 0.38        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1233  |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 36624 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=258.80 +/- 50.89\n",
      "Episode length: 258.80 +/- 50.89\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 259         |\n",
      "|    mean_reward          | 259         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009385575 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.00376    |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1236  |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 39240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=378.40 +/- 103.97\n",
      "Episode length: 378.40 +/- 103.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 378         |\n",
      "|    mean_reward          | 378         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008518583 |\n",
      "|    clip_fraction        | 0.0932      |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    value_loss           | 0.0373      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1232  |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 41856 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=408.00 +/- 112.98\n",
      "Episode length: 408.00 +/- 112.98\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 408          |\n",
      "|    mean_reward          | 408          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061949543 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | 0.0477       |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | 0.00381      |\n",
      "|    value_loss           | 0.0142       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=44000, episode_reward=425.00 +/- 91.98\n",
      "Episode length: 425.00 +/- 91.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 425      |\n",
      "|    mean_reward     | 425      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1190  |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 44472 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=393.00 +/- 85.24\n",
      "Episode length: 393.00 +/- 85.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 393         |\n",
      "|    mean_reward          | 393         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005336688 |\n",
      "|    clip_fraction        | 0.0742      |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | -0.000254   |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | 0.000561    |\n",
      "|    value_loss           | 0.0285      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1184  |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 47088 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=449.60 +/- 42.39\n",
      "Episode length: 449.60 +/- 42.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 450         |\n",
      "|    mean_reward          | 450         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011904169 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.893       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0752      |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | -0.00486    |\n",
      "|    value_loss           | 0.27        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1170  |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 49704 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075200624 |\n",
      "|    clip_fraction        | 0.138        |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.521       |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | -0.00913     |\n",
      "|    n_updates            | 114          |\n",
      "|    policy_gradient_loss | -0.000958    |\n",
      "|    value_loss           | 0.12         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=52000, episode_reward=471.20 +/- 57.60\n",
      "Episode length: 471.20 +/- 57.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1133  |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 52320 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092257755 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0.937        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | 0.0267       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.00207      |\n",
      "|    value_loss           | 0.0909       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1108  |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 54936 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=483.40 +/- 23.78\n",
      "Episode length: 483.40 +/- 23.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 483         |\n",
      "|    mean_reward          | 483         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005350061 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.136       |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | 0.00259     |\n",
      "|    value_loss           | 0.0589      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1089  |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 57552 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=447.00 +/- 70.26\n",
      "Episode length: 447.00 +/- 70.26\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 447         |\n",
      "|    mean_reward          | 447         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006534044 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 0.0519      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1062  |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 60168 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=484.60 +/- 30.80\n",
      "Episode length: 484.60 +/- 30.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 485         |\n",
      "|    mean_reward          | 485         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010716152 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.00177     |\n",
      "|    n_updates            | 138         |\n",
      "|    policy_gradient_loss | 0.00647     |\n",
      "|    value_loss           | 0.0121      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1059  |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 62784 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=480.60 +/- 38.80\n",
      "Episode length: 480.60 +/- 38.80\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 481         |\n",
      "|    mean_reward          | 481         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004911657 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | -0.00154    |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | 0.00343     |\n",
      "|    value_loss           | 0.179       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1057  |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 65400 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=453.40 +/- 57.39\n",
      "Episode length: 453.40 +/- 57.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 453         |\n",
      "|    mean_reward          | 453         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009173021 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.906       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.000183   |\n",
      "|    value_loss           | 0.0789      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1036  |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 65    |\n",
      "|    total_timesteps | 68016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=488.00 +/- 24.00\n",
      "Episode length: 488.00 +/- 24.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | 488         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012379293 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.017       |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 0.0368      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1037  |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 70632 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=483.00 +/- 34.00\n",
      "Episode length: 483.00 +/- 34.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | 483          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062275105 |\n",
      "|    clip_fraction        | 0.0902       |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | 0.00746      |\n",
      "|    n_updates            | 162          |\n",
      "|    policy_gradient_loss | 0.00415      |\n",
      "|    value_loss           | 0.0211       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1038  |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 73248 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009411479 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.499      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.00313     |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1039  |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 75864 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056647933 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | -0.018       |\n",
      "|    n_updates            | 174          |\n",
      "|    policy_gradient_loss | 0.000243     |\n",
      "|    value_loss           | 0.015        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=486.60 +/- 26.80\n",
      "Episode length: 486.60 +/- 26.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 487      |\n",
      "|    mean_reward     | 487      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1027  |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 78480 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=455.20 +/- 40.37\n",
      "Episode length: 455.20 +/- 40.37\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 455          |\n",
      "|    mean_reward          | 455          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 80000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039362116 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | -0.00772     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | 0.00371      |\n",
      "|    value_loss           | 0.0103       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1028  |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 78    |\n",
      "|    total_timesteps | 81096 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008032519 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.523      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 186         |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    value_loss           | 0.00508     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1030  |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 83712 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 84000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040798383 |\n",
      "|    clip_fraction        | 0.0897       |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | 0.00509      |\n",
      "|    n_updates            | 192          |\n",
      "|    policy_gradient_loss | 0.00403      |\n",
      "|    value_loss           | 0.00183      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1018  |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 84    |\n",
      "|    total_timesteps | 86328 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=494.00 +/- 12.00\n",
      "Episode length: 494.00 +/- 12.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 494          |\n",
      "|    mean_reward          | 494          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 88000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072461455 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | -0.116       |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | -0.00459     |\n",
      "|    n_updates            | 198          |\n",
      "|    policy_gradient_loss | 0.000375     |\n",
      "|    value_loss           | 1.17e-05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1019  |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 87    |\n",
      "|    total_timesteps | 88944 |\n",
      "------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 90000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014878771 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | 0.000303    |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | -0.00506    |\n",
      "|    value_loss           | 2.24e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1019  |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 89    |\n",
      "|    total_timesteps | 91560 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 92000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107745845 |\n",
      "|    clip_fraction        | 0.156        |\n",
      "|    clip_range           | 0.159        |\n",
      "|    entropy_loss         | -0.517       |\n",
      "|    explained_variance   | -0.321       |\n",
      "|    learning_rate        | 0.00301      |\n",
      "|    loss                 | -0.0232      |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000769    |\n",
      "|    value_loss           | 4.46e-07     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1009  |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 93    |\n",
      "|    total_timesteps | 94176 |\n",
      "------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=479.00 +/- 42.00\n",
      "Episode length: 479.00 +/- 42.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 479         |\n",
      "|    mean_reward          | 479         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008012497 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.159       |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | -2.55       |\n",
      "|    learning_rate        | 0.00301     |\n",
      "|    loss                 | -0.00535    |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 6.45e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1011  |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 95    |\n",
      "|    total_timesteps | 96792 |\n",
      "------------------------------\n",
      "Multi-environment training took 101.32 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sweep-6</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/l5fce2w5' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/l5fce2w5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143518-l5fce2w5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ketp8uju with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.273765101423342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.00948908942503587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9577863453207472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.92706750717221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0030432410046592\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 6.504317384180399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 74874\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143721-ketp8uju</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ketp8uju' target=\"_blank\">vivid-sweep-7</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ketp8uju' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ketp8uju</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2780`, after every 43 untruncated mini-batches, there will be a truncated mini-batch of size 28\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=695 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1426db5b0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x1426b1dc0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=308.80 +/- 98.99\n",
      "Episode length: 308.80 +/- 98.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 309      |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1984 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2780 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=305.20 +/- 132.91\n",
      "Episode length: 305.20 +/- 132.91\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 305        |\n",
      "|    mean_reward          | 305        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02632936 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.274      |\n",
      "|    entropy_loss         | -0.674     |\n",
      "|    explained_variance   | -0.00604   |\n",
      "|    learning_rate        | 0.00304    |\n",
      "|    loss                 | 0.988      |\n",
      "|    n_updates            | 3          |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 8.23       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1728 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 5560 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=347.40 +/- 93.17\n",
      "Episode length: 347.40 +/- 93.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 347         |\n",
      "|    mean_reward          | 347         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042959817 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 1.83        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    value_loss           | 3.61        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=274.00 +/- 83.29\n",
      "Episode length: 274.00 +/- 83.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 274      |\n",
      "|    mean_reward     | 274      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1491 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 8340 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=349.40 +/- 95.69\n",
      "Episode length: 349.40 +/- 95.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 349         |\n",
      "|    mean_reward          | 349         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033660963 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.612       |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1495  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 11120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=329.60 +/- 114.53\n",
      "Episode length: 329.60 +/- 114.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 330         |\n",
      "|    mean_reward          | 330         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012581282 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0897      |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1499  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 13900 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=487.20 +/- 25.60\n",
      "Episode length: 487.20 +/- 25.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 487         |\n",
      "|    mean_reward          | 487         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016433364 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.476      |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0218      |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.00712    |\n",
      "|    value_loss           | 0.47        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=388.40 +/- 94.01\n",
      "Episode length: 388.40 +/- 94.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 388      |\n",
      "|    mean_reward     | 388      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1361  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 16680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=353.80 +/- 75.81\n",
      "Episode length: 353.80 +/- 75.81\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 354          |\n",
      "|    mean_reward          | 354          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066555813 |\n",
      "|    clip_fraction        | 0.049        |\n",
      "|    clip_range           | 0.274        |\n",
      "|    entropy_loss         | -0.47        |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.00304      |\n",
      "|    loss                 | 0.0319       |\n",
      "|    n_updates            | 18           |\n",
      "|    policy_gradient_loss | 0.00238      |\n",
      "|    value_loss           | 0.396        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1382  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 19460 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004558763 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 21          |\n",
      "|    policy_gradient_loss | 0.0051      |\n",
      "|    value_loss           | 0.183       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=405.80 +/- 112.10\n",
      "Episode length: 405.80 +/- 112.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 406      |\n",
      "|    mean_reward     | 406      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1307  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 22240 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019133715 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0358      |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00348    |\n",
      "|    value_loss           | 0.368       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1305  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 25020 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=371.00 +/- 77.49\n",
      "Episode length: 371.00 +/- 77.49\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 371         |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019090412 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.241       |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | 0.00107     |\n",
      "|    value_loss           | 0.384       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1321  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 27800 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=476.80 +/- 46.40\n",
      "Episode length: 476.80 +/- 46.40\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 477         |\n",
      "|    mean_reward          | 477         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012450527 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0804      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    value_loss           | 0.918       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=415.80 +/- 68.92\n",
      "Episode length: 415.80 +/- 68.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 416      |\n",
      "|    mean_reward     | 416      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1164  |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 30580 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 32000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0109048495 |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.274        |\n",
      "|    entropy_loss         | -0.496       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.00304      |\n",
      "|    loss                 | 0.0734       |\n",
      "|    n_updates            | 33           |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 0.132        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1055  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 33360 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=410.60 +/- 64.97\n",
      "Episode length: 410.60 +/- 64.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 411         |\n",
      "|    mean_reward          | 411         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009505182 |\n",
      "|    clip_fraction        | 0.0641      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | -0.0182     |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    value_loss           | 0.255       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=453.60 +/- 60.78\n",
      "Episode length: 453.60 +/- 60.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 454      |\n",
      "|    mean_reward     | 454      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 798   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 36140 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=218.20 +/- 34.74\n",
      "Episode length: 218.20 +/- 34.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 218         |\n",
      "|    mean_reward          | 218         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005395265 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0493      |\n",
      "|    n_updates            | 39          |\n",
      "|    policy_gradient_loss | -0.00069    |\n",
      "|    value_loss           | 0.0942      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 794   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 38920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=195.20 +/- 19.48\n",
      "Episode length: 195.20 +/- 19.48\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 195         |\n",
      "|    mean_reward          | 195         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008914109 |\n",
      "|    clip_fraction        | 0.0641      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.00266     |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | 0.00216     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 809   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 41700 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=229.60 +/- 32.19\n",
      "Episode length: 229.60 +/- 32.19\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 230         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008627946 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.00426     |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.00147     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=218.60 +/- 24.10\n",
      "Episode length: 218.60 +/- 24.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 219      |\n",
      "|    mean_reward     | 219      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 816   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 44480 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=275.40 +/- 86.29\n",
      "Episode length: 275.40 +/- 86.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 275         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021371584 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.909       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.249       |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    value_loss           | 0.376       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 840   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 47260 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=200.40 +/- 20.60\n",
      "Episode length: 200.40 +/- 20.60\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 200        |\n",
      "|    mean_reward          | 200        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 48000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01145007 |\n",
      "|    clip_fraction        | 0.0922     |\n",
      "|    clip_range           | 0.274      |\n",
      "|    entropy_loss         | -0.49      |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.00304    |\n",
      "|    loss                 | 0.102      |\n",
      "|    n_updates            | 51         |\n",
      "|    policy_gradient_loss | -0.00831   |\n",
      "|    value_loss           | 0.218      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=185.00 +/- 17.05\n",
      "Episode length: 185.00 +/- 17.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 185      |\n",
      "|    mean_reward     | 185      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 861   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 50040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=250.40 +/- 20.45\n",
      "Episode length: 250.40 +/- 20.45\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 250         |\n",
      "|    mean_reward          | 250         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012552027 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0236      |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | -0.0042     |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 883   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 52820 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=344.20 +/- 94.30\n",
      "Episode length: 344.20 +/- 94.30\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 344         |\n",
      "|    mean_reward          | 344         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009057809 |\n",
      "|    clip_fraction        | 0.0913      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0625      |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | 0.00781     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 903   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 55600 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=407.60 +/- 54.85\n",
      "Episode length: 407.60 +/- 54.85\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 408         |\n",
      "|    mean_reward          | 408         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006547807 |\n",
      "|    clip_fraction        | 0.0394      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | -0.0175     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.000143    |\n",
      "|    value_loss           | 0.0591      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=342.40 +/- 74.99\n",
      "Episode length: 342.40 +/- 74.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 342      |\n",
      "|    mean_reward     | 342      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 907   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 58380 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=430.00 +/- 61.53\n",
      "Episode length: 430.00 +/- 61.53\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 430         |\n",
      "|    mean_reward          | 430         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010338437 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | 0.00219     |\n",
      "|    value_loss           | 0.0429      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 922   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 61160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=429.00 +/- 69.11\n",
      "Episode length: 429.00 +/- 69.11\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 429          |\n",
      "|    mean_reward          | 429          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 62000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083413925 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.274        |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.00304      |\n",
      "|    loss                 | 0.0019       |\n",
      "|    n_updates            | 66           |\n",
      "|    policy_gradient_loss | 0.000335     |\n",
      "|    value_loss           | 0.0235       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 935   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 63940 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=358.00 +/- 117.00\n",
      "Episode length: 358.00 +/- 117.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 358          |\n",
      "|    mean_reward          | 358          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039024286 |\n",
      "|    clip_fraction        | 0.0525       |\n",
      "|    clip_range           | 0.274        |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00304      |\n",
      "|    loss                 | 0.00646      |\n",
      "|    n_updates            | 69           |\n",
      "|    policy_gradient_loss | 0.00423      |\n",
      "|    value_loss           | 0.0295       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=412.40 +/- 62.97\n",
      "Episode length: 412.40 +/- 62.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 412      |\n",
      "|    mean_reward     | 412      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 941   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 66720 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=329.40 +/- 88.33\n",
      "Episode length: 329.40 +/- 88.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 329         |\n",
      "|    mean_reward          | 329         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 68000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009985727 |\n",
      "|    clip_fraction        | 0.0779      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | 0.0584      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00737    |\n",
      "|    value_loss           | 0.983       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 956   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 69500 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=425.80 +/- 94.08\n",
      "Episode length: 425.80 +/- 94.08\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 426         |\n",
      "|    mean_reward          | 426         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012771964 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.274       |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.00304     |\n",
      "|    loss                 | -0.00491    |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 0.026       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=394.00 +/- 86.90\n",
      "Episode length: 394.00 +/- 86.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 394      |\n",
      "|    mean_reward     | 394      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 960   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 75    |\n",
      "|    total_timesteps | 72280 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=470.00 +/- 60.00\n",
      "Episode length: 470.00 +/- 60.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 470          |\n",
      "|    mean_reward          | 470          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 74000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051612956 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.274        |\n",
      "|    entropy_loss         | -0.458       |\n",
      "|    explained_variance   | 0.986        |\n",
      "|    learning_rate        | 0.00304      |\n",
      "|    loss                 | -0.0123      |\n",
      "|    n_updates            | 78           |\n",
      "|    policy_gradient_loss | 0.00148      |\n",
      "|    value_loss           | 0.0464       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 971   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 75060 |\n",
      "------------------------------\n",
      "Multi-environment training took 81.06 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>350.6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-7</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ketp8uju' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ketp8uju</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143721-ketp8uju/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2fnqjy2r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.18351907492643563\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.005240068358000306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9145273563275678\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9217249690439528\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00789566652468114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 1.35229940196693\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 437\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 13545\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143856-2fnqjy2r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2fnqjy2r' target=\"_blank\">eternal-sweep-8</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2fnqjy2r' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2fnqjy2r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1748`, after every 27 untruncated mini-batches, there will be a truncated mini-batch of size 20\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=437 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1425f9a00> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x142638d00>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2768 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1748 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=87.20 +/- 9.74\n",
      "Episode length: 87.20 +/- 9.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 87.2        |\n",
      "|    mean_reward          | 87.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011893997 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.184       |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.0356      |\n",
      "|    learning_rate        | 0.0079      |\n",
      "|    loss                 | 0.099       |\n",
      "|    n_updates            | 2           |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 4.74        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1997 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 3496 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=152.40 +/- 44.35\n",
      "Episode length: 152.40 +/- 44.35\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 152         |\n",
      "|    mean_reward          | 152         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014691306 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.184       |\n",
      "|    entropy_loss         | -0.649      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0079      |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.0263     |\n",
      "|    value_loss           | 2.53        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1789 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 5244 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=422.20 +/- 95.31\n",
      "Episode length: 422.20 +/- 95.31\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 422         |\n",
      "|    mean_reward          | 422         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019750956 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.184       |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0079      |\n",
      "|    loss                 | 0.984       |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1375 |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 6992 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=496.60 +/- 6.80\n",
      "Episode length: 496.60 +/- 6.80\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 497        |\n",
      "|    mean_reward          | 497        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04094169 |\n",
      "|    clip_fraction        | 0.433      |\n",
      "|    clip_range           | 0.184      |\n",
      "|    entropy_loss         | -0.543     |\n",
      "|    explained_variance   | 0.727      |\n",
      "|    learning_rate        | 0.0079     |\n",
      "|    loss                 | 0.108      |\n",
      "|    n_updates            | 8          |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    value_loss           | 0.911      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1180 |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 7    |\n",
      "|    total_timesteps | 8740 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=392.60 +/- 89.72\n",
      "Episode length: 392.60 +/- 89.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 393         |\n",
      "|    mean_reward          | 393         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023372468 |\n",
      "|    clip_fraction        | 0.314       |\n",
      "|    clip_range           | 0.184       |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0079      |\n",
      "|    loss                 | 0.00981     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    value_loss           | 0.379       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1136  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 10488 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=221.40 +/- 21.15\n",
      "Episode length: 221.40 +/- 21.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 221         |\n",
      "|    mean_reward          | 221         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022753505 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.184       |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.638       |\n",
      "|    learning_rate        | 0.0079      |\n",
      "|    loss                 | 0.22        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    value_loss           | 0.679       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1076  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 12236 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1098       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 13984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04284956 |\n",
      "|    clip_fraction        | 0.383      |\n",
      "|    clip_range           | 0.184      |\n",
      "|    entropy_loss         | -0.481     |\n",
      "|    explained_variance   | 0.856      |\n",
      "|    learning_rate        | 0.0079     |\n",
      "|    loss                 | 0.0827     |\n",
      "|    n_updates            | 14         |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    value_loss           | 0.455      |\n",
      "----------------------------------------\n",
      "Multi-environment training took 18.40 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>234.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eternal-sweep-8</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2fnqjy2r' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/2fnqjy2r</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143856-2fnqjy2r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ve275hep with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.12762246823316145\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0022087445784009785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.965095559154621\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9718520958571006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00657375422189462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 5.906243614399937\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1302\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 59091\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_143926-ve275hep</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ve275hep' target=\"_blank\">stilted-sweep-9</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ve275hep' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ve275hep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5208`, after every 81 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1302 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1424b35e0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x142689670>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=11.80 +/- 1.47\n",
      "Episode length: 11.80 +/- 1.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.8     |\n",
      "|    mean_reward     | 11.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=11.60 +/- 1.36\n",
      "Episode length: 11.60 +/- 1.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 11.6     |\n",
      "|    mean_reward     | 11.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3335 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 5208 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=227.60 +/- 64.54\n",
      "Episode length: 227.60 +/- 64.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 228          |\n",
      "|    mean_reward          | 228          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069551235 |\n",
      "|    clip_fraction        | 0.34         |\n",
      "|    clip_range           | 0.128        |\n",
      "|    entropy_loss         | -0.687       |\n",
      "|    explained_variance   | -0.0047      |\n",
      "|    learning_rate        | 0.00657      |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.02        |\n",
      "|    value_loss           | 7.33         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=276.20 +/- 103.11\n",
      "Episode length: 276.20 +/- 103.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 276      |\n",
      "|    mean_reward     | 276      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=230.80 +/- 56.16\n",
      "Episode length: 230.80 +/- 56.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 231      |\n",
      "|    mean_reward     | 231      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1640  |\n",
      "|    iterations      | 2     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 10416 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=325.80 +/- 57.07\n",
      "Episode length: 325.80 +/- 57.07\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 326          |\n",
      "|    mean_reward          | 326          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075823367 |\n",
      "|    clip_fraction        | 0.369        |\n",
      "|    clip_range           | 0.128        |\n",
      "|    entropy_loss         | -0.667       |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.00657      |\n",
      "|    loss                 | 9.95         |\n",
      "|    n_updates            | 16           |\n",
      "|    policy_gradient_loss | -0.0296      |\n",
      "|    value_loss           | 17.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=291.80 +/- 59.97\n",
      "Episode length: 291.80 +/- 59.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 292      |\n",
      "|    mean_reward     | 292      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1464  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 15624 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=370.60 +/- 99.11\n",
      "Episode length: 370.60 +/- 99.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 371         |\n",
      "|    mean_reward          | 371         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007973429 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.128       |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.00657     |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 25          |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=384.00 +/- 68.83\n",
      "Episode length: 384.00 +/- 68.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 384      |\n",
      "|    mean_reward     | 384      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=331.40 +/- 98.89\n",
      "Episode length: 331.40 +/- 98.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 331      |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1296  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 20832 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=388.80 +/- 86.86\n",
      "Episode length: 388.80 +/- 86.86\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 389          |\n",
      "|    mean_reward          | 389          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087361615 |\n",
      "|    clip_fraction        | 0.26         |\n",
      "|    clip_range           | 0.128        |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.481        |\n",
      "|    learning_rate        | 0.00657      |\n",
      "|    loss                 | 8.44         |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.0166      |\n",
      "|    value_loss           | 23.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=406.40 +/- 79.96\n",
      "Episode length: 406.40 +/- 79.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 406      |\n",
      "|    mean_reward     | 406      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=429.40 +/- 60.81\n",
      "Episode length: 429.40 +/- 60.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 429      |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1209  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 26040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=437.00 +/- 88.07\n",
      "Episode length: 437.00 +/- 88.07\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 437         |\n",
      "|    mean_reward          | 437         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009518238 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.128       |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.458       |\n",
      "|    learning_rate        | 0.00657     |\n",
      "|    loss                 | 6.35        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=407.40 +/- 73.10\n",
      "Episode length: 407.40 +/- 73.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 407      |\n",
      "|    mean_reward     | 407      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1111  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 31248 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=373.60 +/- 94.91\n",
      "Episode length: 373.60 +/- 94.91\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 374         |\n",
      "|    mean_reward          | 374         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008567818 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.128       |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.00657     |\n",
      "|    loss                 | 4.9         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    value_loss           | 7.6         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=469.40 +/- 39.08\n",
      "Episode length: 469.40 +/- 39.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 469      |\n",
      "|    mean_reward     | 469      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=36000, episode_reward=463.00 +/- 38.24\n",
      "Episode length: 463.00 +/- 38.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 463      |\n",
      "|    mean_reward     | 463      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 987   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 36456 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=420.80 +/- 87.70\n",
      "Episode length: 420.80 +/- 87.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 421         |\n",
      "|    mean_reward          | 421         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004831293 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.128       |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.00657     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.00426    |\n",
      "|    value_loss           | 4.98        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=364.60 +/- 56.72\n",
      "Episode length: 364.60 +/- 56.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 365      |\n",
      "|    mean_reward     | 365      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 994   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 41664 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=418.60 +/- 86.28\n",
      "Episode length: 418.60 +/- 86.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 419         |\n",
      "|    mean_reward          | 419         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003877341 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.128       |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.00657     |\n",
      "|    loss                 | 0.197       |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | -0.00053    |\n",
      "|    value_loss           | 0.415       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=457.00 +/- 86.00\n",
      "Episode length: 457.00 +/- 86.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 457      |\n",
      "|    mean_reward     | 457      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=476.60 +/- 33.20\n",
      "Episode length: 476.60 +/- 33.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 477      |\n",
      "|    mean_reward     | 477      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 979   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 47    |\n",
      "|    total_timesteps | 46872 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=470.80 +/- 58.40\n",
      "Episode length: 470.80 +/- 58.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 471          |\n",
      "|    mean_reward          | 471          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 48000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057553477 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.128        |\n",
      "|    entropy_loss         | -0.525       |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.00657      |\n",
      "|    loss                 | 0.125        |\n",
      "|    n_updates            | 72           |\n",
      "|    policy_gradient_loss | -0.000247    |\n",
      "|    value_loss           | 0.736        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=475.60 +/- 48.80\n",
      "Episode length: 475.60 +/- 48.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 476      |\n",
      "|    mean_reward     | 476      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=473.00 +/- 36.25\n",
      "Episode length: 473.00 +/- 36.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 473      |\n",
      "|    mean_reward     | 473      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 959   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 52080 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=496.60 +/- 6.80\n",
      "Episode length: 496.60 +/- 6.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 497          |\n",
      "|    mean_reward          | 497          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 54000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026822605 |\n",
      "|    clip_fraction        | 0.0845       |\n",
      "|    clip_range           | 0.128        |\n",
      "|    entropy_loss         | -0.52        |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.00657      |\n",
      "|    loss                 | 0.0719       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.00189      |\n",
      "|    value_loss           | 0.124        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56000, episode_reward=452.40 +/- 70.61\n",
      "Episode length: 452.40 +/- 70.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 452      |\n",
      "|    mean_reward     | 452      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 963   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 57288 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=459.60 +/- 70.04\n",
      "Episode length: 459.60 +/- 70.04\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 460          |\n",
      "|    mean_reward          | 460          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045937556 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.128        |\n",
      "|    entropy_loss         | -0.534       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00657      |\n",
      "|    loss                 | 0.099        |\n",
      "|    n_updates            | 88           |\n",
      "|    policy_gradient_loss | -0.00198     |\n",
      "|    value_loss           | 0.434        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=471.20 +/- 36.70\n",
      "Episode length: 471.20 +/- 36.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 471      |\n",
      "|    mean_reward     | 471      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=462.60 +/- 50.21\n",
      "Episode length: 462.60 +/- 50.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 463      |\n",
      "|    mean_reward     | 463      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 62000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 924   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 62496 |\n",
      "------------------------------\n",
      "Multi-environment training took 79.43 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>433.8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-sweep-9</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ve275hep' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ve275hep</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_143926-ve275hep/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g9fxmo60 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.11318092032959608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.008298592588749622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9351013122112336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.925946884007824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.002454851484432049\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 4.494691393776664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 45031\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144059-g9fxmo60</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/g9fxmo60' target=\"_blank\">peachy-sweep-10</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/g9fxmo60' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/g9fxmo60</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3588`, after every 56 untruncated mini-batches, there will be a truncated mini-batch of size 4\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=897 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1422cc580> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x1422cc670>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=9.60 +/- 0.49\n",
      "Episode length: 9.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2865 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 3588 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=178.40 +/- 13.50\n",
      "Episode length: 178.40 +/- 13.50\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 178          |\n",
      "|    mean_reward          | 178          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052151084 |\n",
      "|    clip_fraction        | 0.281        |\n",
      "|    clip_range           | 0.113        |\n",
      "|    entropy_loss         | -0.689       |\n",
      "|    explained_variance   | -0.0845      |\n",
      "|    learning_rate        | 0.00245      |\n",
      "|    loss                 | 0.183        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0156      |\n",
      "|    value_loss           | 2.18         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=186.80 +/- 43.30\n",
      "Episode length: 186.80 +/- 43.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 187      |\n",
      "|    mean_reward     | 187      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1312 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 7176 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=274.60 +/- 141.90\n",
      "Episode length: 274.60 +/- 141.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 275         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006252119 |\n",
      "|    clip_fraction        | 0.425       |\n",
      "|    clip_range           | 0.113       |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | 1.03        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    value_loss           | 1.79        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=213.60 +/- 41.41\n",
      "Episode length: 213.60 +/- 41.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 214      |\n",
      "|    mean_reward     | 214      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 973   |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 10764 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=454.20 +/- 65.18\n",
      "Episode length: 454.20 +/- 65.18\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 454          |\n",
      "|    mean_reward          | 454          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074756416 |\n",
      "|    clip_fraction        | 0.401        |\n",
      "|    clip_range           | 0.113        |\n",
      "|    entropy_loss         | -0.643       |\n",
      "|    explained_variance   | 0.682        |\n",
      "|    learning_rate        | 0.00245      |\n",
      "|    loss                 | 4.18         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0306      |\n",
      "|    value_loss           | 2.41         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=400.80 +/- 109.82\n",
      "Episode length: 400.80 +/- 109.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 401      |\n",
      "|    mean_reward     | 401      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 896   |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 14352 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=458.20 +/- 73.10\n",
      "Episode length: 458.20 +/- 73.10\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 458         |\n",
      "|    mean_reward          | 458         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010663756 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.113       |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.027      |\n",
      "|    value_loss           | 1.97        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 918   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 17940 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=424.20 +/- 64.76\n",
      "Episode length: 424.20 +/- 64.76\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 424         |\n",
      "|    mean_reward          | 424         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008856066 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.113       |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=429.00 +/- 55.36\n",
      "Episode length: 429.00 +/- 55.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 429      |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 906   |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 21528 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067687263 |\n",
      "|    clip_fraction        | 0.191        |\n",
      "|    clip_range           | 0.113        |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.701        |\n",
      "|    learning_rate        | 0.00245      |\n",
      "|    loss                 | 0.0541       |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00774     |\n",
      "|    value_loss           | 0.767        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=449.40 +/- 63.58\n",
      "Episode length: 449.40 +/- 63.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 449      |\n",
      "|    mean_reward     | 449      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 830   |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 25116 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=486.60 +/- 26.80\n",
      "Episode length: 486.60 +/- 26.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 487          |\n",
      "|    mean_reward          | 487          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 26000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044332473 |\n",
      "|    clip_fraction        | 0.17         |\n",
      "|    clip_range           | 0.113        |\n",
      "|    entropy_loss         | -0.55        |\n",
      "|    explained_variance   | 0.841        |\n",
      "|    learning_rate        | 0.00245      |\n",
      "|    loss                 | -0.0166      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00663     |\n",
      "|    value_loss           | 0.366        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 811   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 28704 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013366609 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.113       |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | -0.0141     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00334    |\n",
      "|    value_loss           | 0.284       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=494.60 +/- 10.80\n",
      "Episode length: 494.60 +/- 10.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 495      |\n",
      "|    mean_reward     | 495      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 789   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 32292 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=489.40 +/- 21.20\n",
      "Episode length: 489.40 +/- 21.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 489          |\n",
      "|    mean_reward          | 489          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038561325 |\n",
      "|    clip_fraction        | 0.204        |\n",
      "|    clip_range           | 0.113        |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.00245      |\n",
      "|    loss                 | 0.0729       |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.000874     |\n",
      "|    value_loss           | 0.0969       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 787   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 35880 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004143982 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.113       |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.00245     |\n",
      "|    loss                 | -0.0285     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00115     |\n",
      "|    value_loss           | 0.0931      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 778   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 39468 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028474303 |\n",
      "|    clip_fraction        | 0.147        |\n",
      "|    clip_range           | 0.113        |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.691        |\n",
      "|    learning_rate        | 0.00245      |\n",
      "|    loss                 | 4.23         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00293     |\n",
      "|    value_loss           | 0.357        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=491.00 +/- 18.00\n",
      "Episode length: 491.00 +/- 18.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 491      |\n",
      "|    mean_reward     | 491      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 766   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 43056 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=485.60 +/- 28.80\n",
      "Episode length: 485.60 +/- 28.80\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 486          |\n",
      "|    mean_reward          | 486          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 44000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059140963 |\n",
      "|    clip_fraction        | 0.171        |\n",
      "|    clip_range           | 0.113        |\n",
      "|    entropy_loss         | -0.523       |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.00245      |\n",
      "|    loss                 | -0.0082      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | 0.000156     |\n",
      "|    value_loss           | 0.161        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 756   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 46644 |\n",
      "------------------------------\n",
      "Multi-environment training took 68.43 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-sweep-10</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/g9fxmo60' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/g9fxmo60</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144059-g9fxmo60/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aptx9k77 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.18942248420702665\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.007734372299321099\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9985021406700644\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.983130716988123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.006468469170415138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 7.488949179445153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 941\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 65228\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144222-aptx9k77</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/aptx9k77' target=\"_blank\">desert-sweep-11</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/aptx9k77' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/aptx9k77</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3764`, after every 58 untruncated mini-batches, there will be a truncated mini-batch of size 52\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=941 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x1421ff3a0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x14223ba90>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=9.60 +/- 0.49\n",
      "Episode length: 9.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3385 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 3764 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=294.60 +/- 123.28\n",
      "Episode length: 294.60 +/- 123.28\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 295         |\n",
      "|    mean_reward          | 295         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015596774 |\n",
      "|    clip_fraction        | 0.332       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | 3.1e-05     |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 44.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=291.40 +/- 74.14\n",
      "Episode length: 291.40 +/- 74.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 291      |\n",
      "|    mean_reward     | 291      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1587 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 7528 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=210.40 +/- 69.25\n",
      "Episode length: 210.40 +/- 69.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 210         |\n",
      "|    mean_reward          | 210         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015885442 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 0.278       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 104         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=160.20 +/- 18.20\n",
      "Episode length: 160.20 +/- 18.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 160      |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1412  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 11292 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=332.20 +/- 139.01\n",
      "Episode length: 332.20 +/- 139.01\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 332         |\n",
      "|    mean_reward          | 332         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010666879 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 72.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=280.00 +/- 44.83\n",
      "Episode length: 280.00 +/- 44.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 280      |\n",
      "|    mean_reward     | 280      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1292  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 15056 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=444.00 +/- 73.10\n",
      "Episode length: 444.00 +/- 73.10\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 444          |\n",
      "|    mean_reward          | 444          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 16000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094805565 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.189        |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.365        |\n",
      "|    learning_rate        | 0.00647      |\n",
      "|    loss                 | 66.9         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0117      |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=425.60 +/- 91.68\n",
      "Episode length: 425.60 +/- 91.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 426      |\n",
      "|    mean_reward     | 426      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1187  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 15    |\n",
      "|    total_timesteps | 18820 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=474.00 +/- 52.00\n",
      "Episode length: 474.00 +/- 52.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 474         |\n",
      "|    mean_reward          | 474         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011905442 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.333       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 42          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    value_loss           | 96.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=22000, episode_reward=455.20 +/- 56.61\n",
      "Episode length: 455.20 +/- 56.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 455      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 22000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1124  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 22584 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017221184 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 11.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.000394    |\n",
      "|    value_loss           | 17.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=26000, episode_reward=413.80 +/- 61.36\n",
      "Episode length: 413.80 +/- 61.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 414      |\n",
      "|    mean_reward     | 414      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1028  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 26348 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=413.40 +/- 74.25\n",
      "Episode length: 413.40 +/- 74.25\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 413         |\n",
      "|    mean_reward          | 413         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009654836 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 0.781       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.000216   |\n",
      "|    value_loss           | 3.4         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=432.40 +/- 56.07\n",
      "Episode length: 432.40 +/- 56.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 432      |\n",
      "|    mean_reward     | 432      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 900   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 30112 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=495.40 +/- 9.20\n",
      "Episode length: 495.40 +/- 9.20\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 495        |\n",
      "|    mean_reward          | 495        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04364061 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.189      |\n",
      "|    entropy_loss         | -0.53      |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 0.00647    |\n",
      "|    loss                 | 2.67       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00287   |\n",
      "|    value_loss           | 21.4       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 864   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 33876 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=495.40 +/- 9.20\n",
      "Episode length: 495.40 +/- 9.20\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 495          |\n",
      "|    mean_reward          | 495          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 34000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058137183 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.189        |\n",
      "|    entropy_loss         | -0.529       |\n",
      "|    explained_variance   | 0.943        |\n",
      "|    learning_rate        | 0.00647      |\n",
      "|    loss                 | 0.196        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000774    |\n",
      "|    value_loss           | 0.652        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 805   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 37640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052141463 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.189        |\n",
      "|    entropy_loss         | -0.514       |\n",
      "|    explained_variance   | 0.945        |\n",
      "|    learning_rate        | 0.00647      |\n",
      "|    loss                 | -0.00126     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 0.000328     |\n",
      "|    value_loss           | 0.0553       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=484.40 +/- 31.20\n",
      "Episode length: 484.40 +/- 31.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 484      |\n",
      "|    mean_reward     | 484      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 805   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 41404 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012452899 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 0.212       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    value_loss           | 1.2         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 797   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 45168 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006321361 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 0.028       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    value_loss           | 0.178       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 769   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 63    |\n",
      "|    total_timesteps | 48932 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009742853 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | -12.2       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.000328   |\n",
      "|    value_loss           | 0.0157      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 742   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 52696 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009671588 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.0233      |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 35.8        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000311   |\n",
      "|    value_loss           | 18.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 56000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 730   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 56460 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 58000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01090487 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.189      |\n",
      "|    entropy_loss         | -0.556     |\n",
      "|    explained_variance   | -0.0138    |\n",
      "|    learning_rate        | 0.00647    |\n",
      "|    loss                 | 0.656      |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -4.39e-05  |\n",
      "|    value_loss           | 17.7       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 736   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 81    |\n",
      "|    total_timesteps | 60224 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008808429 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.189       |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | -27.3       |\n",
      "|    learning_rate        | 0.00647     |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.000534    |\n",
      "|    value_loss           | 0.084       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 726   |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 88    |\n",
      "|    total_timesteps | 63988 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052101323 |\n",
      "|    clip_fraction        | 0.0909       |\n",
      "|    clip_range           | 0.189        |\n",
      "|    entropy_loss         | -0.553       |\n",
      "|    explained_variance   | 0.00557      |\n",
      "|    learning_rate        | 0.00647      |\n",
      "|    loss                 | 9.56         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000492    |\n",
      "|    value_loss           | 18.1         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 66000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 707   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 95    |\n",
      "|    total_timesteps | 67752 |\n",
      "------------------------------\n",
      "Multi-environment training took 100.42 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">desert-sweep-11</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/aptx9k77' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/aptx9k77</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144222-aptx9k77/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jyopb7jo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.18257293913711908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.004567995142655914\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9234485169818264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9269724036588955\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009243973278178472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 4.043145996792058\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 13981\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144417-jyopb7jo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jyopb7jo' target=\"_blank\">true-sweep-12</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jyopb7jo' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jyopb7jo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4312`, after every 67 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1078 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x104468280> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x14140e2b0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=10.00 +/- 0.89\n",
      "Episode length: 10.00 +/- 0.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 10       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=9.60 +/- 0.49\n",
      "Episode length: 9.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3353 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 4312 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=148.00 +/- 46.54\n",
      "Episode length: 148.00 +/- 46.54\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 148          |\n",
      "|    mean_reward          | 148          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0134493485 |\n",
      "|    clip_fraction        | 0.305        |\n",
      "|    clip_range           | 0.183        |\n",
      "|    entropy_loss         | -0.681       |\n",
      "|    explained_variance   | -0.0109      |\n",
      "|    learning_rate        | 0.00924      |\n",
      "|    loss                 | 0.506        |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.0239      |\n",
      "|    value_loss           | 1.1          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=117.40 +/- 13.22\n",
      "Episode length: 117.40 +/- 13.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 117      |\n",
      "|    mean_reward     | 117      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1940 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 8624 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=235.60 +/- 36.72\n",
      "Episode length: 235.60 +/- 36.72\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 236         |\n",
      "|    mean_reward          | 236         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017915152 |\n",
      "|    clip_fraction        | 0.424       |\n",
      "|    clip_range           | 0.183       |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.00924     |\n",
      "|    loss                 | 1.12        |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    value_loss           | 1.53        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=197.00 +/- 13.91\n",
      "Episode length: 197.00 +/- 13.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 197      |\n",
      "|    mean_reward     | 197      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1618  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 12936 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=292.40 +/- 16.69\n",
      "Episode length: 292.40 +/- 16.69\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 292         |\n",
      "|    mean_reward          | 292         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026008371 |\n",
      "|    clip_fraction        | 0.367       |\n",
      "|    clip_range           | 0.183       |\n",
      "|    entropy_loss         | -0.564      |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.00924     |\n",
      "|    loss                 | 0.688       |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=289.40 +/- 41.96\n",
      "Episode length: 289.40 +/- 41.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 289      |\n",
      "|    mean_reward     | 289      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1479  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 17248 |\n",
      "------------------------------\n",
      "Multi-environment training took 17.13 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>427.9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sweep-12</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jyopb7jo' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/jyopb7jo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144417-jyopb7jo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: abqbvt0e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2629417284083698\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0080704071807796\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9807192781563856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9665813556395996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00923101118782979\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 5.4812598804832815\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 81263\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144447-abqbvt0e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/abqbvt0e' target=\"_blank\">zesty-sweep-13</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/abqbvt0e' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/abqbvt0e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2168`, after every 33 untruncated mini-batches, there will be a truncated mini-batch of size 56\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=542 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x141e00fa0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x10448d730>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=44.40 +/- 15.24\n",
      "Episode length: 44.40 +/- 15.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 44.4     |\n",
      "|    mean_reward     | 44.4     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3080 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2168 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=111.60 +/- 21.15\n",
      "Episode length: 111.60 +/- 21.15\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 112         |\n",
      "|    mean_reward          | 112         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030720886 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | -0.00859    |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 3.16        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2088 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 4336 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=193.00 +/- 36.20\n",
      "Episode length: 193.00 +/- 36.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 193         |\n",
      "|    mean_reward          | 193         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035661407 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 6.94        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0366     |\n",
      "|    value_loss           | 21.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1805 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 6504 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=325.40 +/- 53.23\n",
      "Episode length: 325.40 +/- 53.23\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 325        |\n",
      "|    mean_reward          | 325        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04769302 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.541     |\n",
      "|    explained_variance   | 0.672      |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | 5.99       |\n",
      "|    n_updates            | 18         |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    value_loss           | 10.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1634 |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 8672 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=320.40 +/- 95.52\n",
      "Episode length: 320.40 +/- 95.52\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 320         |\n",
      "|    mean_reward          | 320         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033283897 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00665    |\n",
      "|    value_loss           | 7.32        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1541  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 10840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02427987 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.538     |\n",
      "|    explained_variance   | 0.774      |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | 0.759      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.000531  |\n",
      "|    value_loss           | 2.62       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1435  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 13008 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=335.80 +/- 23.98\n",
      "Episode length: 335.80 +/- 23.98\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 336        |\n",
      "|    mean_reward          | 336        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06366049 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.52      |\n",
      "|    explained_variance   | 0.682      |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | 0.511      |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | 0.00425    |\n",
      "|    value_loss           | 2.17       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1391  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 15176 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025530402 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 42          |\n",
      "|    policy_gradient_loss | 0.00176     |\n",
      "|    value_loss           | 8.1         |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1338  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 17344 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027524099 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | -1.83       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.0814      |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    value_loss           | 0.713       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1299  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 15    |\n",
      "|    total_timesteps | 19512 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 500       |\n",
      "|    mean_reward          | 500       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 20000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0250327 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.263     |\n",
      "|    entropy_loss         | -0.506    |\n",
      "|    explained_variance   | 0.0198    |\n",
      "|    learning_rate        | 0.00923   |\n",
      "|    loss                 | 5.82      |\n",
      "|    n_updates            | 54        |\n",
      "|    policy_gradient_loss | -0.00825  |\n",
      "|    value_loss           | 17        |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1274  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 21680 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025550734 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.523      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 5.02        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00211    |\n",
      "|    value_loss           | 8.29        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1254  |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 23848 |\n",
      "------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 24000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021745253 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.52       |\n",
      "|    explained_variance   | -1.28       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.179       |\n",
      "|    n_updates            | 66          |\n",
      "|    policy_gradient_loss | 0.00352     |\n",
      "|    value_loss           | 0.625       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 26000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1186  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 26016 |\n",
      "------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019545391 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | -1.73       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.0405      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    value_loss           | 0.0566      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1179  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 28184 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020216271 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.552      |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.237       |\n",
      "|    n_updates            | 78          |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 1.12        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1168  |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 30352 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018993493 |\n",
      "|    clip_fraction        | 0.0996      |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.000109   |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1161  |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 32520 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=395.80 +/- 7.47\n",
      "Episode length: 395.80 +/- 7.47\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 396         |\n",
      "|    mean_reward          | 396         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020210477 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | -0.00592    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00404     |\n",
      "|    value_loss           | 0.0842      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1162  |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 34688 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=227.80 +/- 9.50\n",
      "Episode length: 227.80 +/- 9.50\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 228         |\n",
      "|    mean_reward          | 228         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031990614 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.564      |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | 0.000349    |\n",
      "|    value_loss           | 1.95        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1167  |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 36856 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=294.40 +/- 18.13\n",
      "Episode length: 294.40 +/- 18.13\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 294         |\n",
      "|    mean_reward          | 294         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020399226 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.482       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 4.52        |\n",
      "|    n_updates            | 102         |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    value_loss           | 7.15        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1172  |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 39024 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=434.60 +/- 65.74\n",
      "Episode length: 434.60 +/- 65.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 435         |\n",
      "|    mean_reward          | 435         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 40000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041709304 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.88        |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.172       |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | 0.00109     |\n",
      "|    value_loss           | 1.35        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1170  |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 41192 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 42000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08176616 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.435     |\n",
      "|    explained_variance   | -0.143     |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | 0.0692     |\n",
      "|    n_updates            | 114        |\n",
      "|    policy_gradient_loss | 0.0184     |\n",
      "|    value_loss           | 0.303      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1163  |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 43360 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 44000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02983778 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.462     |\n",
      "|    explained_variance   | 0.935      |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.0196     |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1159  |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 39    |\n",
      "|    total_timesteps | 45528 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023511892 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | -3.68       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.0391      |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    value_loss           | 0.0016      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1155  |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 47696 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016339432 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.505      |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | -0.000137   |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | 0.00403     |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1151  |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 49864 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025669124 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | -29.4       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.131       |\n",
      "|    n_updates            | 138         |\n",
      "|    policy_gradient_loss | 0.00216     |\n",
      "|    value_loss           | 0.000533    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 52000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1127  |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 52032 |\n",
      "------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 54000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01787783 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.498     |\n",
      "|    explained_variance   | -8.09      |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 144        |\n",
      "|    policy_gradient_loss | -0.00562   |\n",
      "|    value_loss           | 3.26e-05   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1126  |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 54200 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031185813 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | -11         |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | -0.00896    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | 0.00139     |\n",
      "|    value_loss           | 8.88e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1124  |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 50    |\n",
      "|    total_timesteps | 56368 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 58000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017421534 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | -0.0416     |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 156         |\n",
      "|    policy_gradient_loss | 0.00202     |\n",
      "|    value_loss           | 3.96e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1122  |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 58536 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025987845 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | -1.48       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 162         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 1.11e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1120  |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 60704 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027131509 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | -5.78       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.032       |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    value_loss           | 6.85e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1118  |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 62872 |\n",
      "------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 64000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017194979 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.456      |\n",
      "|    explained_variance   | -4.25       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | -0.00877    |\n",
      "|    n_updates            | 174         |\n",
      "|    policy_gradient_loss | -0.00401    |\n",
      "|    value_loss           | 1.03e-08    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1116  |\n",
      "|    iterations      | 30    |\n",
      "|    time_elapsed    | 58    |\n",
      "|    total_timesteps | 65040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 66000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018776605 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | -7.95       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | -0.0284     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 5.82e-10    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1115  |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 67208 |\n",
      "------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 68000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08227015 |\n",
      "|    clip_fraction        | 0.398      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.48      |\n",
      "|    explained_variance   | -1.28      |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | -0.0751    |\n",
      "|    n_updates            | 186        |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    value_loss           | 3.83e-11   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1113  |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 69376 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11923323 |\n",
      "|    clip_fraction        | 0.37       |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.441     |\n",
      "|    explained_variance   | 0.0427     |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | 0.36       |\n",
      "|    n_updates            | 192        |\n",
      "|    policy_gradient_loss | 0.0166     |\n",
      "|    value_loss           | 5.17       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1112  |\n",
      "|    iterations      | 33    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 71544 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 72000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01456859 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.528     |\n",
      "|    explained_variance   | -12.3      |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | -0.00697   |\n",
      "|    n_updates            | 198        |\n",
      "|    policy_gradient_loss | 0.00352    |\n",
      "|    value_loss           | 0.0221     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1111  |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 73712 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036540683 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | -0.617      |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 0.0628      |\n",
      "|    n_updates            | 204         |\n",
      "|    policy_gradient_loss | 0.00563     |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1110  |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 68    |\n",
      "|    total_timesteps | 75880 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 76000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01815112 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.263      |\n",
      "|    entropy_loss         | -0.484     |\n",
      "|    explained_variance   | -0.108     |\n",
      "|    learning_rate        | 0.00923    |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.000612  |\n",
      "|    value_loss           | 0.000391   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1093  |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 78048 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=200.80 +/- 24.29\n",
      "Episode length: 200.80 +/- 24.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 201         |\n",
      "|    mean_reward          | 201         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035204865 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.000202    |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.00358    |\n",
      "|    value_loss           | 5.86        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1098  |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 73    |\n",
      "|    total_timesteps | 80216 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=69.60 +/- 2.94\n",
      "Episode length: 69.60 +/- 2.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 69.6        |\n",
      "|    mean_reward          | 69.6        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 82000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032003846 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.263       |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.00923     |\n",
      "|    loss                 | 7.16        |\n",
      "|    n_updates            | 222         |\n",
      "|    policy_gradient_loss | 0.00629     |\n",
      "|    value_loss           | 8.31        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1098  |\n",
      "|    iterations      | 38    |\n",
      "|    time_elapsed    | 74    |\n",
      "|    total_timesteps | 82384 |\n",
      "------------------------------\n",
      "Multi-environment training took 78.79 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>151.8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-13</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/abqbvt0e' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/abqbvt0e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144447-abqbvt0e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u69qp0wd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.26596196603759936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.004627668821370098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9123813606025336\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.988776311014806\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00961163052151704\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 6.731477170539769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 18049\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144621-u69qp0wd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/u69qp0wd' target=\"_blank\">legendary-sweep-14</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/u69qp0wd' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/u69qp0wd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1552`, after every 24 untruncated mini-batches, there will be a truncated mini-batch of size 16\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=388 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x142191310> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x1421858e0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3731 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1552 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=264.00 +/- 120.79\n",
      "Episode length: 264.00 +/- 120.79\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 264         |\n",
      "|    mean_reward          | 264         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023725554 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.266       |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.00217     |\n",
      "|    learning_rate        | 0.00961     |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 2           |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 12.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2152 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 3104 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=119.20 +/- 12.35\n",
      "Episode length: 119.20 +/- 12.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 119        |\n",
      "|    mean_reward          | 119        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03750565 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.266      |\n",
      "|    entropy_loss         | -0.593     |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.00961    |\n",
      "|    loss                 | 5.62       |\n",
      "|    n_updates            | 4          |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    value_loss           | 18.1       |\n",
      "----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2114 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 4656 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=338.60 +/- 154.26\n",
      "Episode length: 338.60 +/- 154.26\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 339        |\n",
      "|    mean_reward          | 339        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03417741 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.266      |\n",
      "|    entropy_loss         | -0.576     |\n",
      "|    explained_variance   | 0.228      |\n",
      "|    learning_rate        | 0.00961    |\n",
      "|    loss                 | 1.02       |\n",
      "|    n_updates            | 6          |\n",
      "|    policy_gradient_loss | -0.00784   |\n",
      "|    value_loss           | 20.1       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1811 |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 6208 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1964        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 7760        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027645646 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.266       |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.00961     |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=475.80 +/- 48.40\n",
      "Episode length: 475.80 +/- 48.40\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 476        |\n",
      "|    mean_reward          | 476        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02763915 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.266      |\n",
      "|    entropy_loss         | -0.543     |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.00961    |\n",
      "|    loss                 | 0.356      |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00347   |\n",
      "|    value_loss           | 10.4       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1757 |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 9312 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02430273 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.266      |\n",
      "|    entropy_loss         | -0.546     |\n",
      "|    explained_variance   | 0.876      |\n",
      "|    learning_rate        | 0.00961    |\n",
      "|    loss                 | 1.03       |\n",
      "|    n_updates            | 12         |\n",
      "|    policy_gradient_loss | 0.000269   |\n",
      "|    value_loss           | 9.34       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1603  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 10864 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02731485 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.266      |\n",
      "|    entropy_loss         | -0.5       |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.00961    |\n",
      "|    loss                 | 0.0462     |\n",
      "|    n_updates            | 14         |\n",
      "|    policy_gradient_loss | 0.000957   |\n",
      "|    value_loss           | 4.21       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1055  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 12416 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1006        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 13968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051781405 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.266       |\n",
      "|    entropy_loss         | -0.407      |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.00961     |\n",
      "|    loss                 | 0.0881      |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    value_loss           | 3.44        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018129967 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.266       |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.252       |\n",
      "|    learning_rate        | 0.00961     |\n",
      "|    loss                 | 0.0736      |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | 0.00486     |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 870   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 15520 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014346604 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.266       |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.00961     |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00233     |\n",
      "|    value_loss           | 1.71        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 883   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 19    |\n",
      "|    total_timesteps | 17072 |\n",
      "------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036564566 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.266       |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.0551      |\n",
      "|    learning_rate        | 0.00961     |\n",
      "|    loss                 | 0.234       |\n",
      "|    n_updates            | 22          |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    value_loss           | 7.58        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 843   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 18624 |\n",
      "------------------------------\n",
      "Multi-environment training took 25.32 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-sweep-14</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/u69qp0wd' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/u69qp0wd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144621-u69qp0wd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vzmg1gup with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.10153188144231572\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.007510599381233399\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9761938505946344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9678233537586322\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009191032051214965\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 4.123543122282363\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 78798\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144703-vzmg1gup</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vzmg1gup' target=\"_blank\">dazzling-sweep-15</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vzmg1gup' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vzmg1gup</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1252`, after every 19 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=313 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x14265f7f0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x141ef49d0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2610 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1252 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=150.60 +/- 34.27\n",
      "Episode length: 150.60 +/- 34.27\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 151          |\n",
      "|    mean_reward          | 151          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042414083 |\n",
      "|    clip_fraction        | 0.296        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.69        |\n",
      "|    explained_variance   | 0.00478      |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 3.33         |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 16.3         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1487 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2504 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1367        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 3756        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006192654 |\n",
      "|    clip_fraction        | 0.41        |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 9.27        |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=343.80 +/- 63.30\n",
      "Episode length: 343.80 +/- 63.30\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 344          |\n",
      "|    mean_reward          | 344          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068658493 |\n",
      "|    clip_fraction        | 0.422        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.663       |\n",
      "|    explained_variance   | 0.428        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    value_loss           | 25.2         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1058 |\n",
      "|    iterations      | 4    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 5008 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=298.00 +/- 73.68\n",
      "Episode length: 298.00 +/- 73.68\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 298          |\n",
      "|    mean_reward          | 298          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6000         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053581498 |\n",
      "|    clip_fraction        | 0.289        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.642       |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 9.27         |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    value_loss           | 21.9         |\n",
      "------------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 906  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 6260 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 954         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 7512        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007949058 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 25.6        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=288.80 +/- 112.42\n",
      "Episode length: 288.80 +/- 112.42\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 289         |\n",
      "|    mean_reward          | 289         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005793428 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 8.3         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    value_loss           | 18.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 934  |\n",
      "|    iterations      | 7    |\n",
      "|    time_elapsed    | 9    |\n",
      "|    total_timesteps | 8764 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=290.20 +/- 20.23\n",
      "Episode length: 290.20 +/- 20.23\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 290         |\n",
      "|    mean_reward          | 290         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029785674 |\n",
      "|    clip_fraction        | 0.366       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 3.33        |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | 0.00375     |\n",
      "|    value_loss           | 5.42        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 915   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 10016 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 952        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 11268      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00978419 |\n",
      "|    clip_fraction        | 0.309      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.514     |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 0.00919    |\n",
      "|    loss                 | 1.38       |\n",
      "|    n_updates            | 64         |\n",
      "|    policy_gradient_loss | 0.0149     |\n",
      "|    value_loss           | 1.92       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=313.40 +/- 104.29\n",
      "Episode length: 313.40 +/- 104.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 313         |\n",
      "|    mean_reward          | 313         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007981721 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.0995      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 0.00376     |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 915   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 13    |\n",
      "|    total_timesteps | 12520 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 954         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 13772       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033353675 |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.497      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.242       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.0256      |\n",
      "|    value_loss           | 0.888       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=328.00 +/- 159.78\n",
      "Episode length: 328.00 +/- 159.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 328         |\n",
      "|    mean_reward          | 328         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004995997 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | 0.87        |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.162       |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | 0.00537     |\n",
      "|    value_loss           | 2.52        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 928   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 15024 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=380.80 +/- 125.20\n",
      "Episode length: 380.80 +/- 125.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 381         |\n",
      "|    mean_reward          | 381         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006163123 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.288       |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | 0.00257     |\n",
      "|    value_loss           | 0.456       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 898   |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 16276 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 17528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061109914 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0284       |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | 0.00269      |\n",
      "|    value_loss           | 0.16         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=387.60 +/- 90.06\n",
      "Episode length: 387.60 +/- 90.06\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 388          |\n",
      "|    mean_reward          | 388          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 18000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069204024 |\n",
      "|    clip_fraction        | 0.184        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0138       |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | 0.00401      |\n",
      "|    value_loss           | 0.145        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 907   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 18780 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=429.20 +/- 86.73\n",
      "Episode length: 429.20 +/- 86.73\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 429          |\n",
      "|    mean_reward          | 429          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050339266 |\n",
      "|    clip_fraction        | 0.203        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.00137     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000539    |\n",
      "|    value_loss           | 0.0686       |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 886   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 20032 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 909        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 21284      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00466637 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.486     |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00919    |\n",
      "|    loss                 | 0.0207     |\n",
      "|    n_updates            | 128        |\n",
      "|    policy_gradient_loss | -0.00199   |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=335.80 +/- 84.54\n",
      "Episode length: 335.80 +/- 84.54\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 336        |\n",
      "|    mean_reward          | 336        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00836822 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.504     |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.00919    |\n",
      "|    loss                 | 0.0115     |\n",
      "|    n_updates            | 136        |\n",
      "|    policy_gradient_loss | 0.000233   |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 894   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 22536 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 912         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 23788       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013742939 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.442       |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | 0.00273     |\n",
      "|    value_loss           | 3.12        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=304.60 +/- 91.15\n",
      "Episode length: 304.60 +/- 91.15\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 305          |\n",
      "|    mean_reward          | 305          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 24000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074239434 |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0272       |\n",
      "|    n_updates            | 152          |\n",
      "|    policy_gradient_loss | 0.00117      |\n",
      "|    value_loss           | 0.22         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 872   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 28    |\n",
      "|    total_timesteps | 25040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=323.80 +/- 56.59\n",
      "Episode length: 323.80 +/- 56.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 324         |\n",
      "|    mean_reward          | 324         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005304533 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 0.0488      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 860   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 26292 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 27544       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008391837 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.00924     |\n",
      "|    n_updates            | 168         |\n",
      "|    policy_gradient_loss | -0.00562    |\n",
      "|    value_loss           | 0.0442      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=377.80 +/- 101.97\n",
      "Episode length: 377.80 +/- 101.97\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 378         |\n",
      "|    mean_reward          | 378         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 28000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004912279 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 176         |\n",
      "|    policy_gradient_loss | 0.00604     |\n",
      "|    value_loss           | 0.0732      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 869   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 28796 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=474.00 +/- 31.90\n",
      "Episode length: 474.00 +/- 31.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 474         |\n",
      "|    mean_reward          | 474         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005248002 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.0522      |\n",
      "|    n_updates            | 184         |\n",
      "|    policy_gradient_loss | 0.00174     |\n",
      "|    value_loss           | 0.193       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 858   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 30048 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 31300        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058573755 |\n",
      "|    clip_fraction        | 0.166        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.48        |\n",
      "|    explained_variance   | 0.951        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0196       |\n",
      "|    n_updates            | 192          |\n",
      "|    policy_gradient_loss | 0.00225      |\n",
      "|    value_loss           | 0.156        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=283.20 +/- 10.68\n",
      "Episode length: 283.20 +/- 10.68\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 283        |\n",
      "|    mean_reward          | 283        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00370915 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.506     |\n",
      "|    explained_variance   | 0.874      |\n",
      "|    learning_rate        | 0.00919    |\n",
      "|    loss                 | 0.00665    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.00031    |\n",
      "|    value_loss           | 0.0196     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 871   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 32552 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 33804       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005474628 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.00328     |\n",
      "|    n_updates            | 208         |\n",
      "|    policy_gradient_loss | 0.00653     |\n",
      "|    value_loss           | 0.187       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 500       |\n",
      "|    mean_reward          | 500       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 34000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5211925 |\n",
      "|    clip_fraction        | 0.475     |\n",
      "|    clip_range           | 0.102     |\n",
      "|    entropy_loss         | -0.421    |\n",
      "|    explained_variance   | 0.742     |\n",
      "|    learning_rate        | 0.00919   |\n",
      "|    loss                 | 3.96      |\n",
      "|    n_updates            | 216       |\n",
      "|    policy_gradient_loss | 0.019     |\n",
      "|    value_loss           | 3.09      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 836   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 41    |\n",
      "|    total_timesteps | 35056 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=432.60 +/- 29.65\n",
      "Episode length: 432.60 +/- 29.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 433         |\n",
      "|    mean_reward          | 433         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007381308 |\n",
      "|    clip_fraction        | 0.357       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.385       |\n",
      "|    n_updates            | 224         |\n",
      "|    policy_gradient_loss | 0.0246      |\n",
      "|    value_loss           | 0.889       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 823   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 36308 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 835          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 37560        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057627223 |\n",
      "|    clip_fraction        | 0.211        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.487       |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0866       |\n",
      "|    n_updates            | 232          |\n",
      "|    policy_gradient_loss | 0.00428      |\n",
      "|    value_loss           | 0.294        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063466257 |\n",
      "|    clip_fraction        | 0.189        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.486       |\n",
      "|    explained_variance   | 0.924        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.352        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | 0.00537      |\n",
      "|    value_loss           | 0.637        |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 828   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 38812 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104391165 |\n",
      "|    clip_fraction        | 0.237        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.47        |\n",
      "|    explained_variance   | 0.946        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.00939      |\n",
      "|    n_updates            | 248          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    value_loss           | 0.0204       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 825   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 40064 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 836          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 41316        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038472079 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.415       |\n",
      "|    explained_variance   | -4.72        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 256          |\n",
      "|    policy_gradient_loss | -1.39e-05    |\n",
      "|    value_loss           | 0.0048       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 42000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072057294 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | 0.899        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0215       |\n",
      "|    n_updates            | 264          |\n",
      "|    policy_gradient_loss | 0.00271      |\n",
      "|    value_loss           | 0.00224      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 830   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 42568 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 841          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 43820        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062583648 |\n",
      "|    clip_fraction        | 0.2          |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | -2.59        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.00287     |\n",
      "|    n_updates            | 272          |\n",
      "|    policy_gradient_loss | 0.000354     |\n",
      "|    value_loss           | 9.44e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009124857 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | 0.0203      |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | -0.00807    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 4.19e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 830   |\n",
      "|    iterations      | 36    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 45072 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 46000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057282913 |\n",
      "|    clip_fraction        | 0.193        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0073       |\n",
      "|    n_updates            | 288          |\n",
      "|    policy_gradient_loss | 5.12e-05     |\n",
      "|    value_loss           | 1.48e-05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 822   |\n",
      "|    iterations      | 37    |\n",
      "|    time_elapsed    | 56    |\n",
      "|    total_timesteps | 46324 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 829         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 47576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004319783 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | -0.0636     |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 296         |\n",
      "|    policy_gradient_loss | -0.000786   |\n",
      "|    value_loss           | 1.31e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005302133 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | -5.68       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 304         |\n",
      "|    policy_gradient_loss | -0.00206    |\n",
      "|    value_loss           | 2.85e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 818   |\n",
      "|    iterations      | 39    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 48828 |\n",
      "------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 50000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040857536 |\n",
      "|    clip_fraction        | 0.26         |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.489       |\n",
      "|    explained_variance   | 0.319        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.00681     |\n",
      "|    n_updates            | 312          |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 1.23e-08     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 811   |\n",
      "|    iterations      | 40    |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 50080 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 813          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 51332        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054771136 |\n",
      "|    clip_fraction        | 0.237        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.492       |\n",
      "|    explained_variance   | -2.95        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.022       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 1.76e-08     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 52000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0096235555 |\n",
      "|    clip_fraction        | 0.271        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.51        |\n",
      "|    explained_variance   | -12.1        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.00361      |\n",
      "|    n_updates            | 328          |\n",
      "|    policy_gradient_loss | -0.00264     |\n",
      "|    value_loss           | 5.09e-08     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 787   |\n",
      "|    iterations      | 42    |\n",
      "|    time_elapsed    | 66    |\n",
      "|    total_timesteps | 52584 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 53836       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030263832 |\n",
      "|    clip_fraction        | 0.32        |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | -0.000915   |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 336         |\n",
      "|    policy_gradient_loss | 0.00479     |\n",
      "|    value_loss           | 4.73        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 54000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009199267 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | -4.38       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.00342     |\n",
      "|    n_updates            | 344         |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    value_loss           | 0.0592      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 780   |\n",
      "|    iterations      | 44    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 55088 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 56000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03858926 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.453     |\n",
      "|    explained_variance   | -3.16      |\n",
      "|    learning_rate        | 0.00919    |\n",
      "|    loss                 | -0.0149    |\n",
      "|    n_updates            | 352        |\n",
      "|    policy_gradient_loss | 0.00962    |\n",
      "|    value_loss           | 0.000473   |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 774   |\n",
      "|    iterations      | 45    |\n",
      "|    time_elapsed    | 72    |\n",
      "|    total_timesteps | 56340 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 780          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 57592        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062071076 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.45        |\n",
      "|    explained_variance   | -0.295       |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.0055      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | 0.00469      |\n",
      "|    value_loss           | 0.00374      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077209375 |\n",
      "|    clip_fraction        | 0.222        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.461       |\n",
      "|    explained_variance   | -0.00259     |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.479        |\n",
      "|    n_updates            | 368          |\n",
      "|    policy_gradient_loss | 0.00778      |\n",
      "|    value_loss           | 4.36         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 774   |\n",
      "|    iterations      | 47    |\n",
      "|    time_elapsed    | 76    |\n",
      "|    total_timesteps | 58844 |\n",
      "------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014806603 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.00421     |\n",
      "|    n_updates            | 376         |\n",
      "|    policy_gradient_loss | 0.0066      |\n",
      "|    value_loss           | 0.468       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 772   |\n",
      "|    iterations      | 48    |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 60096 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 78         |\n",
      "|    total_timesteps      | 61348      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09689215 |\n",
      "|    clip_fraction        | 0.531      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.339     |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.00919    |\n",
      "|    loss                 | 0.0186     |\n",
      "|    n_updates            | 384        |\n",
      "|    policy_gradient_loss | 0.0433     |\n",
      "|    value_loss           | 0.0381     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022970427 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.37       |\n",
      "|    explained_variance   | 0.00298     |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 0.0223      |\n",
      "|    n_updates            | 392         |\n",
      "|    policy_gradient_loss | 0.0237      |\n",
      "|    value_loss           | 2.56        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 780   |\n",
      "|    iterations      | 50    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 62600 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 788         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 63852       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019691002 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | 0.000244    |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | 2.47        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.018       |\n",
      "|    value_loss           | 12.8        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 64000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071315602 |\n",
      "|    clip_fraction        | 0.212        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.418       |\n",
      "|    explained_variance   | -1.36        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0174       |\n",
      "|    n_updates            | 408          |\n",
      "|    policy_gradient_loss | 0.00755      |\n",
      "|    value_loss           | 0.0323       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 787   |\n",
      "|    iterations      | 52    |\n",
      "|    time_elapsed    | 82    |\n",
      "|    total_timesteps | 65104 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 66000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055321287 |\n",
      "|    clip_fraction        | 0.207        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.424       |\n",
      "|    explained_variance   | 0.000499     |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.0177       |\n",
      "|    n_updates            | 416          |\n",
      "|    policy_gradient_loss | 0.00424      |\n",
      "|    value_loss           | 0.00847      |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 787   |\n",
      "|    iterations      | 53    |\n",
      "|    time_elapsed    | 84    |\n",
      "|    total_timesteps | 66356 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 795          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 67608        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025350181 |\n",
      "|    clip_fraction        | 0.172        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | -0.0019      |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.00114      |\n",
      "|    n_updates            | 424          |\n",
      "|    policy_gradient_loss | 0.00508      |\n",
      "|    value_loss           | 0.00187      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 68000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053702286 |\n",
      "|    clip_fraction        | 0.141        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.000851     |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | 0.00776      |\n",
      "|    n_updates            | 432          |\n",
      "|    policy_gradient_loss | 0.00104      |\n",
      "|    value_loss           | 0.000347     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 794   |\n",
      "|    iterations      | 55    |\n",
      "|    time_elapsed    | 86    |\n",
      "|    total_timesteps | 68860 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 70000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027995238 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.379       |\n",
      "|    explained_variance   | -0.00426     |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.00574     |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.00242      |\n",
      "|    value_loss           | 8.73e-05     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 793   |\n",
      "|    iterations      | 56    |\n",
      "|    time_elapsed    | 88    |\n",
      "|    total_timesteps | 70112 |\n",
      "------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 801          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 71364        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041801315 |\n",
      "|    clip_fraction        | 0.149        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.412       |\n",
      "|    explained_variance   | -0.00932     |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.00334     |\n",
      "|    n_updates            | 448          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 1.72e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 72000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032534706 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.391       |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.0113      |\n",
      "|    n_updates            | 456          |\n",
      "|    policy_gradient_loss | 5.79e-05     |\n",
      "|    value_loss           | 4.73e-06     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 800   |\n",
      "|    iterations      | 58    |\n",
      "|    time_elapsed    | 90    |\n",
      "|    total_timesteps | 72616 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 73868       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004698445 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | -6.89       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 464         |\n",
      "|    policy_gradient_loss | -0.00411    |\n",
      "|    value_loss           | 6.63e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005541797 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.102       |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.00919     |\n",
      "|    loss                 | -0.00891    |\n",
      "|    n_updates            | 472         |\n",
      "|    policy_gradient_loss | 0.00277     |\n",
      "|    value_loss           | 4.33e-07    |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 807   |\n",
      "|    iterations      | 60    |\n",
      "|    time_elapsed    | 93    |\n",
      "|    total_timesteps | 75120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 76000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033648298 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.412       |\n",
      "|    explained_variance   | -5.51        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.00276     |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | 0.0021       |\n",
      "|    value_loss           | 0.000153     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 806   |\n",
      "|    iterations      | 61    |\n",
      "|    time_elapsed    | 94    |\n",
      "|    total_timesteps | 76372 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 95         |\n",
      "|    total_timesteps      | 77624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00495772 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.102      |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | -5.67      |\n",
      "|    learning_rate        | 0.00919    |\n",
      "|    loss                 | 0.00182    |\n",
      "|    n_updates            | 488        |\n",
      "|    policy_gradient_loss | -0.000932  |\n",
      "|    value_loss           | 3.21e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 78000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035818978 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.102        |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | -4.09        |\n",
      "|    learning_rate        | 0.00919      |\n",
      "|    loss                 | -0.00755     |\n",
      "|    n_updates            | 496          |\n",
      "|    policy_gradient_loss | 0.000632     |\n",
      "|    value_loss           | 2.26e-08     |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 812   |\n",
      "|    iterations      | 63    |\n",
      "|    time_elapsed    | 97    |\n",
      "|    total_timesteps | 78876 |\n",
      "------------------------------\n",
      "Multi-environment training took 101.23 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-sweep-15</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vzmg1gup' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/vzmg1gup</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144703-vzmg1gup/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6ovf68u4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.24883790651029108\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.004494595353133154\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9999088872395436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9178702401180036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.008270560254136024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 9.786493599771026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 15858\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144856-6ovf68u4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6ovf68u4' target=\"_blank\">hardy-sweep-16</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6ovf68u4' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6ovf68u4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5148`, after every 80 untruncated mini-batches, there will be a truncated mini-batch of size 28\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1287 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x14270db80> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x14270dca0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=10.20 +/- 0.40\n",
      "Episode length: 10.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10.2     |\n",
      "|    mean_reward     | 10.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=10.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 10       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3317 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 5148 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=327.00 +/- 108.17\n",
      "Episode length: 327.00 +/- 108.17\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 327         |\n",
      "|    mean_reward          | 327         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026757145 |\n",
      "|    clip_fraction        | 0.321       |\n",
      "|    clip_range           | 0.249       |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | -0.00708    |\n",
      "|    learning_rate        | 0.00827     |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=294.60 +/- 70.77\n",
      "Episode length: 294.60 +/- 70.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 295      |\n",
      "|    mean_reward     | 295      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=219.60 +/- 79.93\n",
      "Episode length: 219.60 +/- 79.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 220      |\n",
      "|    mean_reward     | 220      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1585  |\n",
      "|    iterations      | 2     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 10296 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=312.00 +/- 123.29\n",
      "Episode length: 312.00 +/- 123.29\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 312         |\n",
      "|    mean_reward          | 312         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031852953 |\n",
      "|    clip_fraction        | 0.345       |\n",
      "|    clip_range           | 0.249       |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.00827     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0544     |\n",
      "|    value_loss           | 3.59        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=340.80 +/- 69.44\n",
      "Episode length: 340.80 +/- 69.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 341      |\n",
      "|    mean_reward     | 341      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1380  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 15444 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=327.80 +/- 107.35\n",
      "Episode length: 327.80 +/- 107.35\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 328        |\n",
      "|    mean_reward          | 328        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03115206 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.249      |\n",
      "|    entropy_loss         | -0.526     |\n",
      "|    explained_variance   | 0.557      |\n",
      "|    learning_rate        | 0.00827    |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 2.04       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=422.60 +/- 52.99\n",
      "Episode length: 422.60 +/- 52.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 423      |\n",
      "|    mean_reward     | 423      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=339.20 +/- 83.50\n",
      "Episode length: 339.20 +/- 83.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 339      |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1238  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 20592 |\n",
      "------------------------------\n",
      "Multi-environment training took 21.81 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-sweep-16</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6ovf68u4' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6ovf68u4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144856-6ovf68u4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9lvhih7g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2339620347449421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.007951593204867432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.96011773201551\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9301124364054616\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0023176078361614753\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 7.335907288409155\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 774\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 21308\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_144932-9lvhih7g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/9lvhih7g' target=\"_blank\">unique-sweep-17</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/9lvhih7g' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/9lvhih7g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3096`, after every 48 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=774 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x14262ebb0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x141f867c0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=8.60 +/- 0.49\n",
      "Episode length: 8.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.6      |\n",
      "|    mean_reward     | 8.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3534 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 3096 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=143.40 +/- 44.71\n",
      "Episode length: 143.40 +/- 44.71\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 143         |\n",
      "|    mean_reward          | 143         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020733824 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.234       |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | -0.0104     |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | 1.38        |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.0254     |\n",
      "|    value_loss           | 6.99        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=119.40 +/- 43.98\n",
      "Episode length: 119.40 +/- 43.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 119      |\n",
      "|    mean_reward     | 119      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2343 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 6192 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=295.00 +/- 83.78\n",
      "Episode length: 295.00 +/- 83.78\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 295         |\n",
      "|    mean_reward          | 295         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029645007 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.234       |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.487       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | 0.982       |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2011 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 9288 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=10000, episode_reward=410.00 +/- 106.64\n",
      "Episode length: 410.00 +/- 106.64\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 410         |\n",
      "|    mean_reward          | 410         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040569887 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.234       |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | 0.313       |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0319     |\n",
      "|    value_loss           | 2.27        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=241.60 +/- 65.87\n",
      "Episode length: 241.60 +/- 65.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 242      |\n",
      "|    mean_reward     | 242      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1741  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 12384 |\n",
      "------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=379.40 +/- 79.86\n",
      "Episode length: 379.40 +/- 79.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 379         |\n",
      "|    mean_reward          | 379         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 14000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017941171 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.234       |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.708       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | 0.354       |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    value_loss           | 1.21        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1690  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 9     |\n",
      "|    total_timesteps | 15480 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=358.20 +/- 44.59\n",
      "Episode length: 358.20 +/- 44.59\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 358         |\n",
      "|    mean_reward          | 358         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012429671 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.234       |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | 0.123       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.782       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=478.60 +/- 18.37\n",
      "Episode length: 478.60 +/- 18.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 479      |\n",
      "|    mean_reward     | 479      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1539  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 18576 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=426.60 +/- 71.24\n",
      "Episode length: 426.60 +/- 71.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 427         |\n",
      "|    mean_reward          | 427         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021270704 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.234       |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.00232     |\n",
      "|    loss                 | -0.0442     |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00859    |\n",
      "|    value_loss           | 0.698       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1529  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 14    |\n",
      "|    total_timesteps | 21672 |\n",
      "------------------------------\n",
      "Multi-environment training took 17.64 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>494.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-17</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/9lvhih7g' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/9lvhih7g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_144932-9lvhih7g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6cmd6ony with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.27991576072267776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.0078585395978037\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9485517953171548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9601266208563708\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.009867358841191297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 7.429725433343909\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 1261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 98898\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_145003-6cmd6ony</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6cmd6ony' target=\"_blank\">avid-sweep-18</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6cmd6ony' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6cmd6ony</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 5044`, after every 78 untruncated mini-batches, there will be a truncated mini-batch of size 52\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1261 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x142732670> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x141f37eb0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=8.80 +/- 0.98\n",
      "Episode length: 8.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 8.8      |\n",
      "|    mean_reward     | 8.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=9.40 +/- 0.49\n",
      "Episode length: 9.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.4      |\n",
      "|    mean_reward     | 9.4      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3447 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 5044 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=6000, episode_reward=82.80 +/- 14.62\n",
      "Episode length: 82.80 +/- 14.62\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 82.8       |\n",
      "|    mean_reward          | 82.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03305888 |\n",
      "|    clip_fraction        | 0.32       |\n",
      "|    clip_range           | 0.28       |\n",
      "|    entropy_loss         | -0.663     |\n",
      "|    explained_variance   | -0.00387   |\n",
      "|    learning_rate        | 0.00987    |\n",
      "|    loss                 | 1.04       |\n",
      "|    n_updates            | 2          |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    value_loss           | 5.93       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=82.20 +/- 10.83\n",
      "Episode length: 82.20 +/- 10.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 82.2     |\n",
      "|    mean_reward     | 82.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=90.80 +/- 20.58\n",
      "Episode length: 90.80 +/- 20.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 90.8     |\n",
      "|    mean_reward     | 90.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2756  |\n",
      "|    iterations      | 2     |\n",
      "|    time_elapsed    | 3     |\n",
      "|    total_timesteps | 10088 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=175.80 +/- 59.20\n",
      "Episode length: 175.80 +/- 59.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 176         |\n",
      "|    mean_reward          | 176         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040409636 |\n",
      "|    clip_fraction        | 0.307       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | 3.19        |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 7.92        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=138.20 +/- 23.65\n",
      "Episode length: 138.20 +/- 23.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 138      |\n",
      "|    mean_reward     | 138      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 2499  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 15132 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=458.60 +/- 40.24\n",
      "Episode length: 458.60 +/- 40.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 459         |\n",
      "|    mean_reward          | 459         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039086323 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.628       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | 4.94        |\n",
      "|    n_updates            | 6           |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    value_loss           | 6.19        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=429.40 +/- 62.88\n",
      "Episode length: 429.40 +/- 62.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 429      |\n",
      "|    mean_reward     | 429      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=408.80 +/- 16.65\n",
      "Episode length: 408.80 +/- 16.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 409      |\n",
      "|    mean_reward     | 409      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1972  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 10    |\n",
      "|    total_timesteps | 20176 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043963145 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | 0.705       |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=24000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1822  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 13    |\n",
      "|    total_timesteps | 25220 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063153766 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | 0.196       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.00922     |\n",
      "|    value_loss           | 1.91        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1641  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 30264 |\n",
      "------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019748475 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | 0.101       |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | 0.0036      |\n",
      "|    value_loss           | 0.708       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 34000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1600  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 35308 |\n",
      "------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 36000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022811055 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | 0.43        |\n",
      "|    n_updates            | 14          |\n",
      "|    policy_gradient_loss | 0.00675     |\n",
      "|    value_loss           | 0.335       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 38000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1513  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 40352 |\n",
      "------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027642347 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | 0.0775      |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    value_loss           | 0.228       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 44000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1500  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 30    |\n",
      "|    total_timesteps | 45396 |\n",
      "------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 46000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02248175 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.28       |\n",
      "|    entropy_loss         | -0.514     |\n",
      "|    explained_variance   | 0.379      |\n",
      "|    learning_rate        | 0.00987    |\n",
      "|    loss                 | -0.00503   |\n",
      "|    n_updates            | 18         |\n",
      "|    policy_gradient_loss | -0.00274   |\n",
      "|    value_loss           | 0.00837    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 48000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1449  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 50440 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017182278 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | -0.0289     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    value_loss           | 0.0025      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1443  |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 55484 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 56000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022766383 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | -0.0722     |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | -0.00136    |\n",
      "|    n_updates            | 22          |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 0.000405    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 58000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1407  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 43    |\n",
      "|    total_timesteps | 60528 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 62000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01893714 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.28       |\n",
      "|    entropy_loss         | -0.487     |\n",
      "|    explained_variance   | 0.156      |\n",
      "|    learning_rate        | 0.00987    |\n",
      "|    loss                 | 0.000524   |\n",
      "|    n_updates            | 24         |\n",
      "|    policy_gradient_loss | -0.00106   |\n",
      "|    value_loss           | 9.68e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1405  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 46    |\n",
      "|    total_timesteps | 65572 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 66000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01990272 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.28       |\n",
      "|    entropy_loss         | -0.471     |\n",
      "|    explained_variance   | 0.319      |\n",
      "|    learning_rate        | 0.00987    |\n",
      "|    loss                 | 0.00906    |\n",
      "|    n_updates            | 26         |\n",
      "|    policy_gradient_loss | 0.00261    |\n",
      "|    value_loss           | 2.51e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1379  |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 51    |\n",
      "|    total_timesteps | 70616 |\n",
      "------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 72000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013925406 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.432      |\n",
      "|    explained_variance   | -0.22       |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 6.18e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 74000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1380  |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 54    |\n",
      "|    total_timesteps | 75660 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 76000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01788772 |\n",
      "|    clip_fraction        | 0.0959     |\n",
      "|    clip_range           | 0.28       |\n",
      "|    entropy_loss         | -0.428     |\n",
      "|    explained_variance   | -0.365     |\n",
      "|    learning_rate        | 0.00987    |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.000582  |\n",
      "|    value_loss           | 2.56e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1358  |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 59    |\n",
      "|    total_timesteps | 80704 |\n",
      "------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 82000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01730038 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.28       |\n",
      "|    entropy_loss         | -0.415     |\n",
      "|    explained_variance   | 0.0444     |\n",
      "|    learning_rate        | 0.00987    |\n",
      "|    loss                 | -0.000577  |\n",
      "|    n_updates            | 32         |\n",
      "|    policy_gradient_loss | -0.0021    |\n",
      "|    value_loss           | 7.09e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 84000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1361  |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 62    |\n",
      "|    total_timesteps | 85748 |\n",
      "------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 86000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01975497 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.28       |\n",
      "|    entropy_loss         | -0.383     |\n",
      "|    explained_variance   | 0.09       |\n",
      "|    learning_rate        | 0.00987    |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 34         |\n",
      "|    policy_gradient_loss | -0.00464   |\n",
      "|    value_loss           | 2.82e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 88000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1348  |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 67    |\n",
      "|    total_timesteps | 90792 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011431553 |\n",
      "|    clip_fraction        | 0.0765      |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | -0.0241     |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | -0.0283     |\n",
      "|    n_updates            | 36          |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 1.02e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 94000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1353  |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 70    |\n",
      "|    total_timesteps | 95836 |\n",
      "------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 96000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018207133 |\n",
      "|    clip_fraction        | 0.0893      |\n",
      "|    clip_range           | 0.28        |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | -0.175      |\n",
      "|    learning_rate        | 0.00987     |\n",
      "|    loss                 | -0.00749    |\n",
      "|    n_updates            | 38          |\n",
      "|    policy_gradient_loss | 0.00297     |\n",
      "|    value_loss           | 1.69e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 98000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "-------------------------------\n",
      "| time/              |        |\n",
      "|    fps             | 1342   |\n",
      "|    iterations      | 20     |\n",
      "|    time_elapsed    | 75     |\n",
      "|    total_timesteps | 100880 |\n",
      "-------------------------------\n",
      "Multi-environment training took 78.57 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>500.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">avid-sweep-18</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6cmd6ony' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/6cmd6ony</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_145003-6cmd6ony/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: olbe60x1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.20951198176323985\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.00041650238399878374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.9000154154766176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9631464580727594\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001321431661525495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 2.8364356798447186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 902\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 96329\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_145136-olbe60x1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/olbe60x1' target=\"_blank\">logical-sweep-19</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/olbe60x1' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/olbe60x1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 3608`, after every 56 untruncated mini-batches, there will be a truncated mini-batch of size 24\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=902 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x141fe3640> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x141fe3610>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=35.20 +/- 11.20\n",
      "Episode length: 35.20 +/- 11.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 35.2     |\n",
      "|    mean_reward     | 35.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3278 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 3608 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=372.20 +/- 105.32\n",
      "Episode length: 372.20 +/- 105.32\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 372         |\n",
      "|    mean_reward          | 372         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015421468 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | -0.00165    |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 9           |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=363.20 +/- 90.66\n",
      "Episode length: 363.20 +/- 90.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 363      |\n",
      "|    mean_reward     | 363      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1499 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 7216 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=355.60 +/- 126.90\n",
      "Episode length: 355.60 +/- 126.90\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 356         |\n",
      "|    mean_reward          | 356         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025084097 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 18          |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 3.3         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=370.40 +/- 94.10\n",
      "Episode length: 370.40 +/- 94.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 370      |\n",
      "|    mean_reward     | 370      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1276  |\n",
      "|    iterations      | 3     |\n",
      "|    time_elapsed    | 8     |\n",
      "|    total_timesteps | 10824 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=410.60 +/- 75.06\n",
      "Episode length: 410.60 +/- 75.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 411         |\n",
      "|    mean_reward          | 411         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031115264 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.507       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 27          |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14000, episode_reward=360.00 +/- 84.78\n",
      "Episode length: 360.00 +/- 84.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 360      |\n",
      "|    mean_reward     | 360      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 14000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1184  |\n",
      "|    iterations      | 4     |\n",
      "|    time_elapsed    | 12    |\n",
      "|    total_timesteps | 14432 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=457.20 +/- 55.57\n",
      "Episode length: 457.20 +/- 55.57\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 457        |\n",
      "|    mean_reward          | 457        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04526703 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.21       |\n",
      "|    entropy_loss         | -0.464     |\n",
      "|    explained_variance   | 0.779      |\n",
      "|    learning_rate        | 0.00132    |\n",
      "|    loss                 | 0.464      |\n",
      "|    n_updates            | 36         |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    value_loss           | 1.3        |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=299.00 +/- 101.48\n",
      "Episode length: 299.00 +/- 101.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 299      |\n",
      "|    mean_reward     | 299      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 18000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1140  |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 15    |\n",
      "|    total_timesteps | 18040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=423.40 +/- 100.06\n",
      "Episode length: 423.40 +/- 100.06\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 423         |\n",
      "|    mean_reward          | 423         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012888519 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.837       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 0.672       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1148  |\n",
      "|    iterations      | 6     |\n",
      "|    time_elapsed    | 18    |\n",
      "|    total_timesteps | 21648 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=314.60 +/- 45.74\n",
      "Episode length: 314.60 +/- 45.74\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 315         |\n",
      "|    mean_reward          | 315         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 22000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010586835 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.452      |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.274       |\n",
      "|    n_updates            | 54          |\n",
      "|    policy_gradient_loss | -0.00622    |\n",
      "|    value_loss           | 0.397       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=379.00 +/- 72.04\n",
      "Episode length: 379.00 +/- 72.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 379      |\n",
      "|    mean_reward     | 379      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 24000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1124  |\n",
      "|    iterations      | 7     |\n",
      "|    time_elapsed    | 22    |\n",
      "|    total_timesteps | 25256 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=366.00 +/- 54.16\n",
      "Episode length: 366.00 +/- 54.16\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 366         |\n",
      "|    mean_reward          | 366         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 26000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008766067 |\n",
      "|    clip_fraction        | 0.0952      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.0554      |\n",
      "|    n_updates            | 63          |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    value_loss           | 0.295       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=436.00 +/- 92.95\n",
      "Episode length: 436.00 +/- 92.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 436      |\n",
      "|    mean_reward     | 436      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 28000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1099  |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 28864 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=393.80 +/- 104.70\n",
      "Episode length: 393.80 +/- 104.70\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 394         |\n",
      "|    mean_reward          | 394         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009031636 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.407      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.0651      |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | 5.91e-05    |\n",
      "|    value_loss           | 0.0956      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=385.60 +/- 68.79\n",
      "Episode length: 385.60 +/- 68.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 386      |\n",
      "|    mean_reward     | 386      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1083  |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 32472 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=415.80 +/- 103.34\n",
      "Episode length: 415.80 +/- 103.34\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 416         |\n",
      "|    mean_reward          | 416         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003567539 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 81          |\n",
      "|    policy_gradient_loss | -0.000157   |\n",
      "|    value_loss           | 0.0656      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=461.60 +/- 31.94\n",
      "Episode length: 461.60 +/- 31.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 462      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 36000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1063  |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 33    |\n",
      "|    total_timesteps | 36080 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=449.60 +/- 63.28\n",
      "Episode length: 449.60 +/- 63.28\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 450          |\n",
      "|    mean_reward          | 450          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 38000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065889615 |\n",
      "|    clip_fraction        | 0.0494       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | -0.0144      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 0.0537       |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1072  |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 37    |\n",
      "|    total_timesteps | 39688 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=453.80 +/- 92.40\n",
      "Episode length: 453.80 +/- 92.40\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 454          |\n",
      "|    mean_reward          | 454          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071888007 |\n",
      "|    clip_fraction        | 0.0597       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.405       |\n",
      "|    explained_variance   | 0.996        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | 0.0443       |\n",
      "|    n_updates            | 99           |\n",
      "|    policy_gradient_loss | 0.000273     |\n",
      "|    value_loss           | 0.0687       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=427.20 +/- 95.94\n",
      "Episode length: 427.20 +/- 95.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 427      |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 42000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1059  |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 43296 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=470.20 +/- 59.60\n",
      "Episode length: 470.20 +/- 59.60\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 470         |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 44000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008278173 |\n",
      "|    clip_fraction        | 0.0848      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.398      |\n",
      "|    explained_variance   | 0.997       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | -0.00153    |\n",
      "|    value_loss           | 0.0578      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=46000, episode_reward=450.80 +/- 60.59\n",
      "Episode length: 450.80 +/- 60.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 451      |\n",
      "|    mean_reward     | 451      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 46000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1045  |\n",
      "|    iterations      | 13    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 46904 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=414.20 +/- 79.94\n",
      "Episode length: 414.20 +/- 79.94\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 414         |\n",
      "|    mean_reward          | 414         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 48000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011269024 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.404      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.0159      |\n",
      "|    n_updates            | 117         |\n",
      "|    policy_gradient_loss | -0.00644    |\n",
      "|    value_loss           | 0.0266      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=456.80 +/- 85.40\n",
      "Episode length: 456.80 +/- 85.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 457      |\n",
      "|    mean_reward     | 457      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1036  |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 50512 |\n",
      "------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=425.80 +/- 90.65\n",
      "Episode length: 425.80 +/- 90.65\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 426         |\n",
      "|    mean_reward          | 426         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 52000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021998085 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 126         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 0.0294      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=471.80 +/- 43.38\n",
      "Episode length: 471.80 +/- 43.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 472      |\n",
      "|    mean_reward     | 472      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 54000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1027  |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 52    |\n",
      "|    total_timesteps | 54120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 56000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072745807 |\n",
      "|    clip_fraction        | 0.076        |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.467       |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | 0.0556       |\n",
      "|    n_updates            | 135          |\n",
      "|    policy_gradient_loss | -0.000998    |\n",
      "|    value_loss           | 0.297        |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1033  |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 55    |\n",
      "|    total_timesteps | 57728 |\n",
      "------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 500          |\n",
      "|    mean_reward          | 500          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 58000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072792713 |\n",
      "|    clip_fraction        | 0.0855       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.452       |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | 0.102        |\n",
      "|    n_updates            | 144          |\n",
      "|    policy_gradient_loss | -0.00373     |\n",
      "|    value_loss           | 0.175        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=489.80 +/- 20.40\n",
      "Episode length: 489.80 +/- 20.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 490      |\n",
      "|    mean_reward     | 490      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1022  |\n",
      "|    iterations      | 17    |\n",
      "|    time_elapsed    | 60    |\n",
      "|    total_timesteps | 61336 |\n",
      "------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 62000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008850978 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 153         |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    value_loss           | 0.352       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 1009  |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 64    |\n",
      "|    total_timesteps | 64944 |\n",
      "------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=461.20 +/- 48.23\n",
      "Episode length: 461.20 +/- 48.23\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 461          |\n",
      "|    mean_reward          | 461          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 66000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061189495 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.21         |\n",
      "|    entropy_loss         | -0.455       |\n",
      "|    explained_variance   | 0.954        |\n",
      "|    learning_rate        | 0.00132      |\n",
      "|    loss                 | 0.0148       |\n",
      "|    n_updates            | 162          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 0.169        |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 68000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 955   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 71    |\n",
      "|    total_timesteps | 68552 |\n",
      "------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=450.80 +/- 42.84\n",
      "Episode length: 450.80 +/- 42.84\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 451         |\n",
      "|    mean_reward          | 451         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012218435 |\n",
      "|    clip_fraction        | 0.0834      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.00316    |\n",
      "|    n_updates            | 171         |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    value_loss           | 0.0386      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=465.60 +/- 39.30\n",
      "Episode length: 465.60 +/- 39.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 466      |\n",
      "|    mean_reward     | 466      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 72000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 936   |\n",
      "|    iterations      | 20    |\n",
      "|    time_elapsed    | 77    |\n",
      "|    total_timesteps | 72160 |\n",
      "------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 74000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021290744 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0275     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00594    |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 936   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 80    |\n",
      "|    total_timesteps | 75768 |\n",
      "------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 76000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00912162 |\n",
      "|    clip_fraction        | 0.081      |\n",
      "|    clip_range           | 0.21       |\n",
      "|    entropy_loss         | -0.485     |\n",
      "|    explained_variance   | 0.656      |\n",
      "|    learning_rate        | 0.00132    |\n",
      "|    loss                 | 0.079      |\n",
      "|    n_updates            | 189        |\n",
      "|    policy_gradient_loss | -0.00777   |\n",
      "|    value_loss           | 0.762      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 78000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 916   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 86    |\n",
      "|    total_timesteps | 79376 |\n",
      "------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019415699 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.344       |\n",
      "|    n_updates            | 198         |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 1           |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 500      |\n",
      "|    mean_reward     | 500      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 82000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 908   |\n",
      "|    iterations      | 23    |\n",
      "|    time_elapsed    | 91    |\n",
      "|    total_timesteps | 82984 |\n",
      "------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=408.80 +/- 26.21\n",
      "Episode length: 408.80 +/- 26.21\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 409         |\n",
      "|    mean_reward          | 409         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 84000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033606127 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 207         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 0.965       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=408.00 +/- 82.86\n",
      "Episode length: 408.00 +/- 82.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 408      |\n",
      "|    mean_reward     | 408      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 86000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 883   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 98    |\n",
      "|    total_timesteps | 86592 |\n",
      "------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=396.60 +/- 8.11\n",
      "Episode length: 396.60 +/- 8.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 397         |\n",
      "|    mean_reward          | 397         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 88000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023089807 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | 0.0654      |\n",
      "|    n_updates            | 216         |\n",
      "|    policy_gradient_loss | -0.00796    |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=381.00 +/- 20.47\n",
      "Episode length: 381.00 +/- 20.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 381      |\n",
      "|    mean_reward     | 381      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 874   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 103   |\n",
      "|    total_timesteps | 90200 |\n",
      "------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=274.60 +/- 9.18\n",
      "Episode length: 274.60 +/- 9.18\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 275         |\n",
      "|    mean_reward          | 275         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 92000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011609191 |\n",
      "|    clip_fraction        | 0.0915      |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.329      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.00561    |\n",
      "|    n_updates            | 225         |\n",
      "|    policy_gradient_loss | -0.00401    |\n",
      "|    value_loss           | 0.00042     |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 879   |\n",
      "|    iterations      | 26    |\n",
      "|    time_elapsed    | 106   |\n",
      "|    total_timesteps | 93808 |\n",
      "------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=223.60 +/- 11.36\n",
      "Episode length: 223.60 +/- 11.36\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 224         |\n",
      "|    mean_reward          | 224         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 94000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020219838 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.21        |\n",
      "|    entropy_loss         | -0.343      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.00132     |\n",
      "|    loss                 | -0.0336     |\n",
      "|    n_updates            | 234         |\n",
      "|    policy_gradient_loss | 0.000967    |\n",
      "|    value_loss           | 0.623       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=214.80 +/- 4.49\n",
      "Episode length: 214.80 +/- 4.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 215      |\n",
      "|    mean_reward     | 215      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 871   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 111   |\n",
      "|    total_timesteps | 97416 |\n",
      "------------------------------\n",
      "Multi-environment training took 116.48 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>489.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-19</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/olbe60x1' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/olbe60x1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_145136-olbe60x1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ilaj49sp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_range: 0.2894012100094645\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.005047333190850075\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.947050244754861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.9236209779100624\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00786776114966432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_grad_norm: 1.945041517355801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttotal_timesteps: 49451\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/cschmidl/phd/repos/wandb-playground/notebooks/hyperparameter-tuning/wandb/run-20230327_145347-ilaj49sp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ilaj49sp' target=\"_blank\">glad-sweep-20</a></strong> to <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/sweeps/b30md251</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ilaj49sp' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ilaj49sp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:145: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 1380`, after every 21 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=345 and n_envs=4)\n",
      "  warnings.warn(\n",
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py:399: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x142742460> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x142742e50>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3301 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 1380 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cschmidl/phd/repos/wandb-playground/venv/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=340.00 +/- 111.86\n",
      "Episode length: 340.00 +/- 111.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 340         |\n",
      "|    mean_reward          | 340         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034938835 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.573       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    value_loss           | 2.89        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1316 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2760 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=4000, episode_reward=363.80 +/- 115.79\n",
      "Episode length: 363.80 +/- 115.79\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 364        |\n",
      "|    mean_reward          | 364        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04467712 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.56      |\n",
      "|    explained_variance   | 0.638      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.848      |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0537    |\n",
      "|    value_loss           | 2.19       |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1057 |\n",
      "|    iterations      | 3    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 4140 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1119       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 5520       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11819129 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.475     |\n",
      "|    explained_variance   | 0.299      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.113      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    value_loss           | 0.87       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=416.20 +/- 49.28\n",
      "Episode length: 416.20 +/- 49.28\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 416        |\n",
      "|    mean_reward          | 416        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02534348 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.436     |\n",
      "|    explained_variance   | 0.559      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.0789     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.000328  |\n",
      "|    value_loss           | 0.335      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 995  |\n",
      "|    iterations      | 5    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 6900 |\n",
      "-----------------------------\n",
      "Eval num_timesteps=8000, episode_reward=319.00 +/- 60.11\n",
      "Episode length: 319.00 +/- 60.11\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 319         |\n",
      "|    mean_reward          | 319         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034376025 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | -0.00787    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 0.00168     |\n",
      "|    value_loss           | 0.199       |\n",
      "-----------------------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 958  |\n",
      "|    iterations      | 6    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 8280 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1007        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 9660        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030574871 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00547     |\n",
      "|    value_loss           | 0.233       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=458.20 +/- 47.41\n",
      "Episode length: 458.20 +/- 47.41\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 458        |\n",
      "|    mean_reward          | 458        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04882244 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.363     |\n",
      "|    explained_variance   | 0.779      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.0218     |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | 0.0144     |\n",
      "|    value_loss           | 0.277      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 966   |\n",
      "|    iterations      | 8     |\n",
      "|    time_elapsed    | 11    |\n",
      "|    total_timesteps | 11040 |\n",
      "------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=488.40 +/- 23.20\n",
      "Episode length: 488.40 +/- 23.20\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 488         |\n",
      "|    mean_reward          | 488         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017228069 |\n",
      "|    clip_fraction        | 0.0907      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.0734      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00498     |\n",
      "|    value_loss           | 0.0826      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 938   |\n",
      "|    iterations      | 9     |\n",
      "|    time_elapsed    | 13    |\n",
      "|    total_timesteps | 12420 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 965        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 13800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04292686 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.381     |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.0412     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | 0.0135     |\n",
      "|    value_loss           | 0.0167     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=431.40 +/- 84.92\n",
      "Episode length: 431.40 +/- 84.92\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 431       |\n",
      "|    mean_reward          | 431       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 14000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0197263 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.289     |\n",
      "|    entropy_loss         | -0.452    |\n",
      "|    explained_variance   | 0.885     |\n",
      "|    learning_rate        | 0.00787   |\n",
      "|    loss                 | 0.0512    |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | 0.00312   |\n",
      "|    value_loss           | 0.16      |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 937   |\n",
      "|    iterations      | 11    |\n",
      "|    time_elapsed    | 16    |\n",
      "|    total_timesteps | 15180 |\n",
      "------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=185.00 +/- 11.58\n",
      "Episode length: 185.00 +/- 11.58\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 185         |\n",
      "|    mean_reward          | 185         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 16000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027045878 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.443      |\n",
      "|    explained_variance   | 0.996       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | -0.0234     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00504     |\n",
      "|    value_loss           | 0.0172      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 946   |\n",
      "|    iterations      | 12    |\n",
      "|    time_elapsed    | 17    |\n",
      "|    total_timesteps | 16560 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 974        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 17940      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08757576 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.361     |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | 0.00803    |\n",
      "|    value_loss           | 0.379      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=469.80 +/- 38.39\n",
      "Episode length: 469.80 +/- 38.39\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 470         |\n",
      "|    mean_reward          | 470         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 18000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018765494 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | -0.000286   |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    value_loss           | 0.0129      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 952   |\n",
      "|    iterations      | 14    |\n",
      "|    time_elapsed    | 20    |\n",
      "|    total_timesteps | 19320 |\n",
      "------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=217.20 +/- 24.33\n",
      "Episode length: 217.20 +/- 24.33\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 217         |\n",
      "|    mean_reward          | 217         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035314146 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.155       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    value_loss           | 0.0489      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 958   |\n",
      "|    iterations      | 15    |\n",
      "|    time_elapsed    | 21    |\n",
      "|    total_timesteps | 20700 |\n",
      "------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=295.40 +/- 173.53\n",
      "Episode length: 295.40 +/- 173.53\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 295        |\n",
      "|    mean_reward          | 295        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05175837 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.324     |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | -0.0196    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.000582   |\n",
      "|    value_loss           | 0.00799    |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 953   |\n",
      "|    iterations      | 16    |\n",
      "|    time_elapsed    | 23    |\n",
      "|    total_timesteps | 22080 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 971         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 23460       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049046036 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.841       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | -0.0381     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 0.0713      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=146.40 +/- 6.34\n",
      "Episode length: 146.40 +/- 6.34\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 146        |\n",
      "|    mean_reward          | 146        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16096763 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.301     |\n",
      "|    explained_variance   | 0.701      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | 0.00757    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 979   |\n",
      "|    iterations      | 18    |\n",
      "|    time_elapsed    | 25    |\n",
      "|    total_timesteps | 24840 |\n",
      "------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=262.00 +/- 32.38\n",
      "Episode length: 262.00 +/- 32.38\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 262        |\n",
      "|    mean_reward          | 262        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09535666 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.28      |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | -0.0602    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | 0.00181    |\n",
      "|    value_loss           | 0.0224     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 976   |\n",
      "|    iterations      | 19    |\n",
      "|    time_elapsed    | 26    |\n",
      "|    total_timesteps | 26220 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 992         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 27600       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111281306 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.417      |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.0418      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    value_loss           | 0.041       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=424.00 +/- 152.00\n",
      "Episode length: 424.00 +/- 152.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 424        |\n",
      "|    mean_reward          | 424        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06661746 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | 0.586      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.205      |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | 0.0248     |\n",
      "|    value_loss           | 1.25       |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 977   |\n",
      "|    iterations      | 21    |\n",
      "|    time_elapsed    | 29    |\n",
      "|    total_timesteps | 28980 |\n",
      "------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=473.20 +/- 49.22\n",
      "Episode length: 473.20 +/- 49.22\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 473       |\n",
      "|    mean_reward          | 473       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 30000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1250844 |\n",
      "|    clip_fraction        | 0.355     |\n",
      "|    clip_range           | 0.289     |\n",
      "|    entropy_loss         | -0.337    |\n",
      "|    explained_variance   | 0.831     |\n",
      "|    learning_rate        | 0.00787   |\n",
      "|    loss                 | 0.161     |\n",
      "|    n_updates            | 210       |\n",
      "|    policy_gradient_loss | 0.00646   |\n",
      "|    value_loss           | 0.434     |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 964   |\n",
      "|    iterations      | 22    |\n",
      "|    time_elapsed    | 31    |\n",
      "|    total_timesteps | 30360 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 979        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 32         |\n",
      "|    total_timesteps      | 31740      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07129296 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.367     |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | -0.00945   |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.00391   |\n",
      "|    value_loss           | 0.0302     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=475.60 +/- 31.55\n",
      "Episode length: 475.60 +/- 31.55\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 476         |\n",
      "|    mean_reward          | 476         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 32000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033557355 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.367      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 0.0246      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 967   |\n",
      "|    iterations      | 24    |\n",
      "|    time_elapsed    | 34    |\n",
      "|    total_timesteps | 33120 |\n",
      "------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 34000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052319966 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.065       |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | 0.029       |\n",
      "|    value_loss           | 0.0112      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 956   |\n",
      "|    iterations      | 25    |\n",
      "|    time_elapsed    | 36    |\n",
      "|    total_timesteps | 34500 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 970        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 35880      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04334601 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.285     |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.00435    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | 0.00769    |\n",
      "|    value_loss           | 0.075      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 36000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22956564 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.238     |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | -0.00311   |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | 0.00579    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 958   |\n",
      "|    iterations      | 27    |\n",
      "|    time_elapsed    | 38    |\n",
      "|    total_timesteps | 37260 |\n",
      "------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 500         |\n",
      "|    mean_reward          | 500         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 38000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115895875 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.187      |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.0481      |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.0317      |\n",
      "|    value_loss           | 0.214       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 948   |\n",
      "|    iterations      | 28    |\n",
      "|    time_elapsed    | 40    |\n",
      "|    total_timesteps | 38640 |\n",
      "------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=500.00 +/- 0.00\n",
      "Episode length: 500.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 500        |\n",
      "|    mean_reward          | 500        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05487204 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.264     |\n",
      "|    explained_variance   | -1.89      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 938   |\n",
      "|    iterations      | 29    |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 40020 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 950         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 41400       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079577655 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.377      |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.0245      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.00627     |\n",
      "|    value_loss           | 0.147       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=137.40 +/- 4.22\n",
      "Episode length: 137.40 +/- 4.22\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 137         |\n",
      "|    mean_reward          | 137         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 42000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060768906 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.868       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | 0.0254      |\n",
      "|    value_loss           | 0.098       |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 956   |\n",
      "|    iterations      | 31    |\n",
      "|    time_elapsed    | 44    |\n",
      "|    total_timesteps | 42780 |\n",
      "------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=158.40 +/- 14.07\n",
      "Episode length: 158.40 +/- 14.07\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 158       |\n",
      "|    mean_reward          | 158       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 44000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1695051 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.289     |\n",
      "|    entropy_loss         | -0.328    |\n",
      "|    explained_variance   | 0.776     |\n",
      "|    learning_rate        | 0.00787   |\n",
      "|    loss                 | 0.0573    |\n",
      "|    n_updates            | 310       |\n",
      "|    policy_gradient_loss | -0.00219  |\n",
      "|    value_loss           | 0.722     |\n",
      "---------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 960   |\n",
      "|    iterations      | 32    |\n",
      "|    time_elapsed    | 45    |\n",
      "|    total_timesteps | 44160 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 971         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 45540       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098619156 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.037       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    value_loss           | 0.041       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=252.60 +/- 42.86\n",
      "Episode length: 252.60 +/- 42.86\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 253         |\n",
      "|    mean_reward          | 253         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 46000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070554346 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.289       |\n",
      "|    entropy_loss         | -0.326      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.00787     |\n",
      "|    loss                 | 0.0513      |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    value_loss           | 0.0182      |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 970   |\n",
      "|    iterations      | 34    |\n",
      "|    time_elapsed    | 48    |\n",
      "|    total_timesteps | 46920 |\n",
      "------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=153.20 +/- 4.71\n",
      "Episode length: 153.20 +/- 4.71\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 153        |\n",
      "|    mean_reward          | 153        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 48000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03472363 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.405     |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.00659   |\n",
      "|    value_loss           | 0.0685     |\n",
      "----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 973   |\n",
      "|    iterations      | 35    |\n",
      "|    time_elapsed    | 49    |\n",
      "|    total_timesteps | 48300 |\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 983        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 50         |\n",
      "|    total_timesteps      | 49680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13429937 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.289      |\n",
      "|    entropy_loss         | -0.315     |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.00787    |\n",
      "|    loss                 | 0.705      |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | 0.00638    |\n",
      "|    value_loss           | 0.9        |\n",
      "----------------------------------------\n",
      "Multi-environment training took 55.13 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/mean_reward</td><td>240.6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glad-sweep-20</strong> at: <a href='https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ilaj49sp' target=\"_blank\">https://wandb.ai/cschmidl/ppo_hyperparamter_tuning/runs/ilaj49sp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230327_145347-ilaj49sp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(multi_sweep_id, function=sweep_agent_multi, count=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
